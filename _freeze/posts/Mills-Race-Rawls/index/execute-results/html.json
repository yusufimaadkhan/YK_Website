{
  "hash": "10bf0895894d5471e429c4d90c42788c",
  "result": {
    "markdown": "---\ntitle: \"Mills, Race, and Rawls\"\nauthor: \"Yusuf Imaad Khan\"\ncategories: [Political Philosophy]\ndate: \"2023-11-14\"\nformat: html\neditor: visual\ntoc: false\nself-contained: true\ncode-tools: true\ncode-fold: true \nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n*28/08/2024 - This blog post is based on some charts I made for my Contemporary Political Theory students while we were covering critics of Rawls, including Mills. I was trying to keep it interesting for them. I'm retrospectively posting it here, tagged with the original date, because why not...*\n\n------------------------------------------------------------------------\n\nIn \"Black Rights/White Wrongs: The Critique of Racial Liberalism\", Charles Mills writes the following on Rawls:\n\n> \"*The person seen as the most important twentieth-century American political philosopher and theorist of social justice, and a fortiori the most important American contract theorist, had nothing to say about the remediation of racial injustice, so central to American society and history. His five major books (excluding the two lecture collections on the history of ethics and political philosophy)---A Theory of Justice, Political Liberalism, Collected Papers, The Law of Peoples, and Justice as Fairness: A Restatement---together total over 2,000 pages. **If one were to add together all their sentences on race and racism, one might get half a dozen pages,** if that much*\"\n>\n> Charles Mills, 2017, 35\n\nNow I thought \"why not attempt this?\". Seems straightforward, and we have tools that make it pretty easy.\n\nBut first, we have to conduct a *thought experiment*. Suppose one legally obtained PDFs of:\n\n-   A Theory of Justice\n\n-   Political Liberalism\n\n-   Rawls' Collected Papers\n\n-   The Law of Peoples\n\n-   Justice as Fairness: A Restatement\n\nIn a manner similar to the *purely hypothetical* nature of the original position, how might one add together all the sentences on race and racism?\n\nWell first you might load up some packages:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(tidytext)\nlibrary(tokenizers)\n```\n:::\n\n\nThen you might read your hypothetical PDFs in as a list:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfiles <- list.files(pattern = \"pdf$\")\n\nrawls_books <- lapply(files, pdf_text)\n```\n:::\n\n\nAfter that, you could extract the sentences and words from all the books and turn them into dataframes:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Use map to extract sentences from each PDF and combine into a data frame\ntext_df_sentences <- purrr::map_dfr(files, function(file) {\n  pdf_text <- pdf_text(file)\n  sentences <- unlist(tokenizers::tokenize_sentences(pdf_text))\n  data.frame(\n    sentence = sentences,\n    document = rep(file, length(sentences))\n  )\n})\n\ntext_df_words <- purrr::map_dfr(files, function(file) {\n  pdf_text <- pdf_text(file)\n  words <- unlist(tokenizers::tokenize_words(pdf_text))\n  data.frame(\n    word = words,\n    document = rep(file, length(words))\n  )\n})\n```\n:::\n\n\nThen you might wish to do some basic analysis where you count most frequent words.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntext_df_words <- text_df_words %>%\n  anti_join(stop_words)\n\nword_freq <- text_df_words %>%\n  count(word, sort = TRUE)\n\nmost_freq <- head(word_freq, 10)\n\nglimpse(most_freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 10\nColumns: 2\n$ word <chr> \"justice\", \"political\", \"principles\", \"conception\", \"society\", \"s…\n$ n    <int> 7484, 6358, 4413, 3913, 3476, 3094, 3055, 2649, 2615, 2540\n```\n:::\n:::\n\n\nCould turn that into a nice chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- most_freq %>% \n  mutate(word = fct_reorder(word, n)) %>% \n  ggplot(aes(word, n)) +\n  geom_bar(stat=\"identity\", fill=\"#268bd2\") +\n  coord_flip() +\n  labs(title = \"10 most frequent words across Rawls' big 5 books\",\n       caption = \"Source: Yusuf Khan - original analysis of entirely legal PDFs of COLL, JAF, LOP, PL, TOJ\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") +\n  ylab(\"\") +\n  xlab(\"\") +\n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\"),\n        axis.text.y = element_text(face = \"bold\")\n        ) +\n  geom_text(aes(label = n), hjust = 1.2, size = 3, colour = \"white\", fontface = \"bold\")\n\n# ggsave(\"rawls_count.png\", plot = p1, width = 6, height = 4, units = \"in\", bg = \"#fdf6e3\")\n\np1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWhat about counting the sentences mentioning race? Well...you could interpret Mills very crudely and just flag race/racism/racist/racial. This approach isn't very careful. Other terms such as \"black\", \"white\", \"indigenous\", \"Jim Crow\", \"native\", \"skin\", \"colour/color\", \"segregation\" and so on could all be used to discuss race.\n\nSo perhaps a more sophisticated approach is called for? Ah but even when you've looked for these terms...it isn't looking good. So to keep things simple, you resolve to look for race/racism/racist/racial. This could crudely support Mills' point but its not very complete.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncount_race <- text_df_sentences %>% \n  filter(str_detect(sentence, \"\\\\b(?:race|racism|racist|racial)\\\\b\")) %>% \n  mutate(book = case_match(\n    document,\n    \"RawlsCOLL.pdf\" ~ \"Collected Papers\",\n    \"RawlsJAF.pdf\" ~ \"Justice as Fairness\",\n    \"RawlsLOP.pdf\" ~ \"Law of Peoples\",\n    \"RawlsPL.pdf\" ~ \"Political Liberalism\",\n    \"RawlsTOJ.pdf\" ~ \"Theory of Justice\",\n    .default = document\n  ),\n  count = 1\n  ) %>% \n  group_by(book) %>%\n  summarise(sum = sum(count)) %>% \n  ungroup() %>% \n  arrange(sum)\n\nglimpse(count_race)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 5\nColumns: 2\n$ book <chr> \"Law of Peoples\", \"Theory of Justice\", \"Political Liberalism\", \"C…\n$ sum  <dbl> 3, 8, 9, 10, 12\n```\n:::\n:::\n\n\nYou could also turn this into a chart and report the findings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np2 <- count_race %>% \n  mutate(book = fct_reorder(book, sum)) %>% \n  ggplot(aes(book, sum)) +\n  geom_bar(stat=\"identity\", fill = \"#d33682\") +\n  coord_flip() +\n  theme_minimal() +\n  ylab(\"\") +\n  xlab(\"\") +\n  labs(title = \"5 books, over 2000 pages, and about 1 million words...but Rawls only \\nhas 42 sentences containing 'race', 'racism', 'racist', or 'racial'\",\n       subtitle = \"Sentences containing race/racism/racist/racial across 5 of Rawls' books\",\n       caption = \"Source: Yusuf Khan - original analysis of entirely legal PDFs\") +\n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\"),\n        axis.text.y = element_text(face = \"bold\")) +\n  geom_text(aes(label = sum), hjust = 1.8, size = 3, colour = \"white\", fontface = \"bold\") +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 4))\n\n\nggsave(\"rawls_race.png\", plot = p2, width = 6.5, height = 4, units = \"in\", bg = \"#fdf6e3\")\n\np2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nLet's return to Mills' claim:\n\n> *If one were to add together all their sentences on race and racism, one might get half a dozen pages, if that much*\n\nHow does Rawls fare? First, let's check the average sentence length across all his major works:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Calculate number of words per sentence\ntext_df_sentences <- text_df_sentences %>%\n  rowwise() %>%\n  mutate(word_count = str_count(sentence, \"\\\\S+\"))\n\n# Calculate average sentence length across all sentences\naverage_sentence_length <- mean(text_df_sentences$word_count)\n\naverage_sentence_length\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 23.83138\n```\n:::\n:::\n\n\nNext, let's assume a page has 450 words and calculate the number of sentences per page (sorry I am not bothered to break up the files by page):\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nwords_per_page <- 450\n\n# Estimate sentences per page\nsentences_per_page <- words_per_page / average_sentence_length\n\nsentences_per_page\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18.88267\n```\n:::\n:::\n\n\nIf Rawls' work only has 42 sentences containing \"race\", \"racism\", \"racist\", or \"racial\"...this roughly comes to \\[*rapidly taps racism calculator*\\]:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nrawls_race_pages <- 42/sentences_per_page\n\nrawls_race_pages\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.224262\n```\n:::\n:::\n\n\n2.2 pages. I am shocked. SHOCKED I SAY.\n\n*But remember, this is all a thought experiment.* Suppose one *legally* obtained those PDFs. *So*, *all of this is hypothetical*, of course...\n\n![](rawrls.jpg){fig-align=\"center\"}\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}