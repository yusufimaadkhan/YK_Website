[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yusuf Imaad Khan",
    "section": "",
    "text": "I am an MRes/PhD student in the Department of Government at the London School of Economics. My research sits between political philosophy and social epistemology.\nI completed my MSc in the Philosophy of the Social Sciences at the LSE in the Department of Philosophy, Logic, and Scientific Method. Before that, I completed my BA in Philosophy and Economics at Cardiff University.\nIn between my studies, I worked in the Government Economic Service in a few areas including international climate finance and local policy analysis.\nWelcome to my website! Here you can find my blog - Rogue Analysisüìà, and a page of resources I‚Äôve found handy.\n\n\n\n\n\n\nDisambiguation\n\n\n\n\n\nYusuf Khan is a pretty common name so I tend to use my middle name too. To clear things up further:\n\nI am not Kamala‚Äôs father\nI am not this character from the famous Pashtun amorous folktale\nI am not the murderous doctor from Eastenders\nI am not this actor\nI am not Dilip Kumar from before he changed his name\nI am not this WSJ journalist\netc."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Rogue Analysis üìà (Blog)",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nIn our thousands, in our millions‚Ä¶üáµüá∏\n\n\n\n\n\n\n\nPalestine\n\n\nGenerative Art\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\n\n\n\n\n  \n\n\n\n\nThe sycophantic stooge of US imperialism\n\n\n\n\n\n\n\nPalestine\n\n\nUK\n\n\nUS\n\n\nUN\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nYusuf Imaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nMaking the Hockey Sticks of Prosperity and Doom üèí\n\n\n\n\n\n\n\nClimate\n\n\nGlobalisation\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nYusuf Imaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nThe Vices of the Vice-Chancellors\n\n\n\n\n\n\n\nLSE\n\n\nUnions\n\n\nStrikes\n\n\nEmployment\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nYusuf Imaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nAcademic casualisation across the UK\n\n\n\n\n\n\n\nLSE\n\n\nUnions\n\n\nStrikes\n\n\nEmployment\n\n\nPolitics\n\n\nEconomics\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nYusuf Imaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nMake your very own fiscal black hole! üßë‚Äçüç≥\n\n\n\n\n\n\n\nCooking\n\n\nPolitics\n\n\nEconomics\n\n\nMoney\n\n\nBudget\n\n\nSatire\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nYusuf Imaad Khan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)\n\n\n\n\n\n\n\nNetworks\n\n\nCorruption\n\n\nPolitics\n\n\nMoney\n\n\nBudget\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2022\n\n\nYusuf Imaad Khan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Rogue Analysis\n\n\n\n\n\n\n\nIntroductions\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nYusuf Imaad Khan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to Rogue Analysis",
    "section": "",
    "text": "Hi. I‚Äôm Yusuf and this is my blog - ‚ÄúRogue Analysis‚Äù. I‚Äôd like to join the community of philosophy blogs, practice my writing, and clarify a few thoughts."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here are a bunch of resources that might be handy for somebody looking to get into philosophy/political theory, or generally interested in the content I‚Äôve put on Rogue Analysis (okay plan to put on Rogue Analysis). It isn‚Äôt meant to be exhaustive, just stuff I‚Äôve personally found useful.\nCAVEATS:\n\nI have yet to use some of these resources fully, but they seem useful to share\nThis page may be a bit of a ‚Äúliving document‚Äù, so potentially there will be changes\nPossibly not even worth saying, but none/some/all of this may or may not be helpful\n\nIn any case:"
  },
  {
    "objectID": "posts/mario/index.html",
    "href": "posts/mario/index.html",
    "title": "Analysis of Mario Kart",
    "section": "",
    "text": "USE THE LINE UP THING FROM OBSERVABLE"
  },
  {
    "objectID": "posts/welcome/index.html#why-is-your-blog-called-that",
    "href": "posts/welcome/index.html#why-is-your-blog-called-that",
    "title": "Welcome to Rogue Analysis",
    "section": "Why is your blog called that?",
    "text": "Why is your blog called that?\nSo why the edgy blog name? Well beyond it being a fun name, and feeling like I missed out on my chance to be embarrassingly edgy on the internet in my formative years:\n\nI‚Äôve just resigned from being an analyst in the civil service to go and do some research in political philosophy.\nThe name came from a colleague remarking it would be great if there was some unit called ‚ÄúRogue Analysis‚Äù that undertook analytical work with little oversight and a wide remit\nIt seemed like a suitably capacious name to cover things I‚Äôd like to write about:\n\n\nPolitical Philosophy\nSocial Epistemology\nPhilosophy of Social Science\nData Visualisation\nMario Kart\nWhy the government are:\n\n\nimport {checkbox} from \"@jashkenas/inputs\"\nviewof ch = checkbox([\"Stupid\", \"Cruel\", \"Insane\"])"
  },
  {
    "objectID": "posts/welcome/index.html#deeper-motivations",
    "href": "posts/welcome/index.html#deeper-motivations",
    "title": "Welcome to Rogue Analysis",
    "section": "Deeper Motivations",
    "text": "Deeper Motivations"
  },
  {
    "objectID": "posts/welcome/index.html#why-is-my-blog-called-that",
    "href": "posts/welcome/index.html#why-is-my-blog-called-that",
    "title": "Welcome to Rogue Analysis",
    "section": "Why is my blog called that?",
    "text": "Why is my blog called that?\nSo why the edgy blog name? Well beyond it being a fun name, and feeling like I wasn‚Äôt embarrassingly edgy enough on the internet in my teenage years:\n\nI‚Äôve just resigned from being an analyst in the civil service to go and do some research in political philosophy\nThe name came from a colleague remarking it would be great if there was some unit called ‚ÄúRogue Analysis‚Äù that undertook analytical work with little oversight and a wide remit\nIt seemed like a suitably capacious name to cover things I‚Äôd like to write about:\n\n\nPolitical Philosophy\nSocial Epistemology\nPhilosophy of Social Science\nPublic Policy\nData Visualisation\nMario Kart (okay I‚Äôm hoping to put some lighthearted stuff here too)"
  },
  {
    "objectID": "posts/welcome/index.html#some-further-motivations",
    "href": "posts/welcome/index.html#some-further-motivations",
    "title": "Welcome to Rogue Analysis",
    "section": "Some further motivations",
    "text": "Some further motivations\n\nRuthless criticism\nOn further reflection there are a few more strands of thought that tie in to what I hope to weave here. For one, Marx‚Äôs well known call for ‚Äúruthless criticism‚Äù:\n\n‚ÄúIf we have no business with the construction of the future or with organizing it for all time, there can still be no doubt about the task confronting us at present: the ruthless criticism of the existing order, ruthless in that it will shrink neither from its own discoveries, nor from conflict with the powers that be.‚Äù\n(Letter from Marx to Ruge - September 1843 - Accessible here.)\n\nI like the spirit here, and of particular interest to me is ‚Äúconflict with the powers that be‚Äù. I am moving from a role that required political impartiality, to work that should be about far-reaching social and political criticism. Its a bit of a jarring transition, but its what I want to do. Reflecting on Marx‚Äôs words reminds me to resist slipping back into a weird political existence where policy is happening close by, but the range of acceptable criticism is minuscule.\n\n\nPhilosophy and public policy\nAnother strand of thought is the role of philosophy in public policy. I hope to write on matters of public policy. But here I agree with Chomsky that professional training in philosophy does not confer any ‚Äúspecific competence‚Äù to critique public policy, but it is a concern for philosophy:\n\n‚ÄúThere is no profession that can claim with greater authenticity that its concern is the intellectual culture of the society or that it possesses the tools for the analysis of ideology and the critique of social knowledge and its use.‚Äù\n\nThat said.\n\n\nThe open source community and its tools\nA final strand of thought is the open source community and its tools. I am quite taken"
  },
  {
    "objectID": "posts/welcome/index.html#a-note-on-form",
    "href": "posts/welcome/index.html#a-note-on-form",
    "title": "Welcome to Rogue Analysis",
    "section": "A note on form",
    "text": "A note on form\nI‚Äôd like to use this space to experiment with combining visuals/text/literate programming in a way that supports openness and clarity1. To achieve this, my website is built with an open source publishing system called Quarto. With Quarto you can do stuff like easily chuck this interactive visual of Schelling‚Äôs model of segregation built by Graham McNeil2 right into your blog3:\n\n\nCode\nviewof seed = Inputs.range([0.01, 0.99], {value: 0.01, step: 0.01, label: \"random seed\"})\n\nviewof squaresPerSide = Inputs.range([10, 120], {value: 80, step: 1, label: \"squares per side\"});\n\nviewof fractionEmpty = Inputs.range([0.01, 0.1], {value: 0.02, step: 0.01, label: \"fraction empty\"});\n\nviewof moveThreshold = Inputs.range([0, 1], {value: 0.3, step: 0.01, label: \"move threshold\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nAA = import('https://cdn.skypack.dev/@gjmcn/atomic-agents@0.1.6?min');\n\nAV = import('https://cdn.skypack.dev/@gjmcn/atomic-agents-vis@0.4.1?min')\n\n{\n  \n  // simulation\n  AA.random.seed(seed);\n  const sim = new AA.Simulation({\n    width: squaresPerSide * 5,\n    height: squaresPerSide * 5,\n    gridStep: 5\n  });\n  invalidation.then(() => sim.end());\n  \n  // initialise each square to land type 0, 1 or 2 (empty)\n  const nEmpty = Math.round(sim.squares.size * fractionEmpty);\n  for (let [i, sq] of AA.shuffle([...sim.squares]).entries()) {\n    sq.label('land', i < nEmpty ?  2 : i % 2);\n  }\n  \n  // get unhappy\n  const unhappy = () => sim.squares.filter(sq => {\n    const land = sq.label('land');\n    if (land === 2) return false;\n    const layer = sq.layer();\n    return layer.reduce((count, neb) => count + (neb.label('land') === land), 0) / \n           layer.reduce((count, neb) => count + (neb.label('land') !== 2), 0)\n             < moveThreshold;\n  }, true);\n\n  // each tick, move unhappy to random empty\n  const rand = AA.random.int(nEmpty);\n  sim.beforeTick = () => {\n    for (let sq of AA.shuffle(unhappy())) {\n      [...sim.withLabel('land', 2)][rand()]  // clunky - copying xset to array each step\n        .label('land', sq.label('land'));\n      sq.label('land', 2);\n    }\n  };\n  \n  // vis\n  const tints = [AV.colors.blue, AV.colors.orange, 0xbbbbbb];\n  sim.squares.forEach(sq => {\n    sq.zIndex = -Infinity\n    sq.vis({tint: sq => tints[sq.label('land')]});\n  });\n  return AV.visObs(sim, {\n    stats: true,\n    backParticles: true,\n  });\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code button even lets you see the underlying code used to make it! I‚Äôm hoping to experiment a bit further with this stuff, but I don‚Äôt want to get carried away with shiny things.\nAnyway I think that‚Äôs long enough for an intro post. Here‚Äôs to hoping it will go okay!"
  },
  {
    "objectID": "resources.html#workflow",
    "href": "resources.html#workflow",
    "title": "Resources",
    "section": "Workflow",
    "text": "Workflow\nI am trying to sort all my tools out now so I can just focus on reading/writing without unnecessary headaches. I vacillate between thinking I should just return to pen + paper or use fancy tools because they might have cool epistemic benefits. You can skip this part if you‚Äôre happy with your set up.\n\nPlain Text?\nI hadn‚Äôt really given my workflow much thought until recently. After a bit of browsing I came across ‚ÄúThe Plain Person‚Äôs Guide to Plain Text Social Science.‚Äù by Kieran Healy. This is addressed at new grad students deciding what software to use for their work. Healy makes the case for using tools that ‚Äúgive you more control over the process of data analysis and writing‚Äù. This seems like a good idea.\nNow I‚Äôm not following this guide to the letter, but its convinced me to implement some decent version control, use open source tools where possible, and write papers in markdown for easier formatting. Worth a look, but no use upending your workflow if you have a system that works for you.\n\n\nTaking Notes\nOkay a confession - I do not have a good set up for digital note taking. I want to experiment a bit and see what works. Maybe it seems ridiculous that I haven‚Äôt sorted this out yet, but there are umpteen note taking apps that do cool things and could help me out.\nGenerally I want to move to some kind of plain text set up because it enables a focus on content rather than format, would enable easy conversion to other formats, and allow for easy version control. See here for some benefits of plain text notes. Currently I‚Äôm messing with some horrible mix of:\n\nOneNote (not plain text)\nSimplenote\nObsidian\nGitJournal"
  },
  {
    "objectID": "resources.html#notes",
    "href": "resources.html#notes",
    "title": "Resources",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "resources.html#data-visualisation",
    "href": "resources.html#data-visualisation",
    "title": "Resources",
    "section": "Data Visualisation",
    "text": "Data Visualisation\n\nWebsites\nSome great sources for R visuals:\n\nR Graph Gallery - great examples and ideas. Absolutely brilliant site that taught me a lot\nData to Viz- even more great ideas and examples\nhtmlwidgets for R - cool interactivey stuff\n\nand D3/Observable:\n\nD3 Graph Gallery\nObservable (how do more people not use this?)\n\n\n\nBooks\n\nR for Data Science\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practical Introduction\nPublic Policy Analytics: Code & Context for Data Science in Government\nScott Murray‚Äôs Interactive Data Visualization for the Web\n\n\n\nVideos\n\nA channel with some great videos on ggplot"
  },
  {
    "objectID": "resources.html#post-grad-applications",
    "href": "resources.html#post-grad-applications",
    "title": "Resources",
    "section": "Post-grad applications",
    "text": "Post-grad applications\n\nMentoring\nThe Minorities and Philosophy UK branch offer a mentoring scheme for marginalised people.\n\n\nProposals\nIt can be really tough to find good examples of what a PhD proposal is meant to look like. The Postgrad Application Library maintained by Orlando Lazar and Alena Davis was insanely helpful for me."
  },
  {
    "objectID": "resources.html#website",
    "href": "resources.html#website",
    "title": "Resources",
    "section": "Website",
    "text": "Website\nI made this website using Quarto, Github, and Netlify. You can fork the code for this website HERE. I found these resources useful to get going:\n\nQuarto Documentation\nCreating a blog with Quarto in 10 steps - Beatriz Milz\nThe ultimate guide to starting a Quarto blog - Albert Rapp"
  },
  {
    "objectID": "resources.html#phd-tips",
    "href": "resources.html#phd-tips",
    "title": "Resources",
    "section": "PhD Tips",
    "text": "PhD Tips\nWe‚Äôll see how well this pans out, but I came across these two pieces that gave me suggestions to consider and a bit of reassurance:\n\n12 Tips for Success in Philosophy Graduate School - Liz Jackson\n6 Tips for Graduate Political Theory Students - Bruno Leipold"
  },
  {
    "objectID": "resources.html#thesis-stuff",
    "href": "resources.html#thesis-stuff",
    "title": "Resources",
    "section": "Thesis stuff?",
    "text": "Thesis stuff?"
  },
  {
    "objectID": "resources.html#writing-a-thesis",
    "href": "resources.html#writing-a-thesis",
    "title": "Resources",
    "section": "Writing a thesis",
    "text": "Writing a thesis\nAgain - no idea how this will pan out either"
  },
  {
    "objectID": "resources.html#taking-notes",
    "href": "resources.html#taking-notes",
    "title": "Resources",
    "section": "Taking notes",
    "text": "Taking notes"
  },
  {
    "objectID": "resources.html#networks",
    "href": "resources.html#networks",
    "title": "Resources",
    "section": "Networks",
    "text": "Networks\nIt can be helpful to join some networks for\nAlong with keeping an eye on philosophy twitter, I‚Äôve found"
  },
  {
    "objectID": "resources.html#grad-school",
    "href": "resources.html#grad-school",
    "title": "Resources",
    "section": "Grad School",
    "text": "Grad School\n\nMentoring\nThe Minorities and Philosophy UK branch offer a mentoring scheme for marginalised people.\n\n\nWriting PhD Proposals in the UK\nIt can be really tough to find good examples of what a PhD proposal is meant to look like. The Postgrad Application Library maintained by Orlando Lazar and Alena Davis was insanely helpful for me.\n\n\nNetworks\nIt can be helpful to join some networks for\nAlong with keeping an eye on philosophy twitter, I‚Äôve found\n\n\nPhilosophy Grad School Tips\nWe‚Äôll see how well this pans out, but I came across these two pieces that gave me suggestions to consider and a bit of reassurance:\n\n12 Tips for Success in Philosophy Graduate School - Liz Jackson\n6 Tips for Graduate Political Theory Students - Bruno Leipold\n\n\n\nWriting a thesis\nAgain - no idea how this will pan out either"
  },
  {
    "objectID": "resources.html#grad-school-in-philosophypolitical-theory",
    "href": "resources.html#grad-school-in-philosophypolitical-theory",
    "title": "Resources",
    "section": "Grad School in Philosophy/Political Theory",
    "text": "Grad School in Philosophy/Political Theory\n\nMentoring\nThe Minorities and Philosophy UK branch offer a mentoring scheme for marginalised people in academic philosophy. This is for any UK philosophy student or researcher who consider themselves to be members of a marginalised group.\n\n\nNetworks\nIt can be helpful to join some philosophy networks to stay in the loop for papers/discussions/developments/conferences. I‚Äôve found these networks to be useful:\n\nPhilosophy twitter\nPhilPeople\nThe community being created by the wonderful people at Philosophy Exchange\nLiverpool List email list\n\n\n\nPhilosophy Grad School Tips\nWe‚Äôll see how well this pans out, but I came across these two pieces that gave me suggestions to consider and a bit of reassurance:\n\n12 Tips for Success in Philosophy Graduate School - Liz Jackson\n6 Tips for Graduate Political Theory Students - Bruno Leipold"
  },
  {
    "objectID": "resources.html#data-visualisationdata-science-stuff",
    "href": "resources.html#data-visualisationdata-science-stuff",
    "title": "Resources",
    "section": "Data visualisation/‚ÄúData Science‚Äù Stuff",
    "text": "Data visualisation/‚ÄúData Science‚Äù Stuff\nLike many people during the peaks of COVID, I got quite into data visualisation and used it a fair bit at work. Here are some useful resources I came across.\n\nWebsites\nSome great sources for R visuals:\n\nhttps://www.r-graph-gallery.com/index.html - great examples and ideas. Absolutely brilliant site that taught me a lot\nhttps://www.data-to-viz.com/ - even more great ideas and examples\nhttp://gallery.htmlwidgets.org/ - cool interactivey stuff\n\nand D3/Observable:\n\nhttps://d3-graph-gallery.com/\nhttps://observablehq.com/explore\n\n\n\nBooks\n\nR for Data Science - https://r4ds.had.co.nz/\nggplot2: Elegant Graphics for Data Analysis - https://ggplot2-book.org/\nData Visualization: A Practical Introduction - https://socviz.co/\nPublic Policy Analytics: Code & Context for Data Science in Government - https://urbanspatial.github.io/PublicPolicyAnalytics/index.html#\n\n\n\nVideos\n\nA channel with some great videos on ggplot - https://www.youtube.com/watch?v=HPJn1CMvtmI"
  },
  {
    "objectID": "resources.html#data-visualisationdata-science",
    "href": "resources.html#data-visualisationdata-science",
    "title": "Resources",
    "section": "Data Visualisation/‚ÄúData Science‚Äù",
    "text": "Data Visualisation/‚ÄúData Science‚Äù\nLike many people during the peaks of COVID, I got quite into data visualisation and used it a fair bit at work. Here are some useful resources I came across.\n\nWebsites\nSome great sources for R visuals:\n\nhttps://www.r-graph-gallery.com/index.html - great examples and ideas. Absolutely brilliant site that taught me a lot\nhttps://www.data-to-viz.com/ - even more great ideas and examples\nhttp://gallery.htmlwidgets.org/ - cool interactivey stuff\n\nand D3/Observable:\n\nhttps://d3-graph-gallery.com/\nhttps://observablehq.com/explore\n\n\n\nBooks\n\nR for Data Science - https://r4ds.had.co.nz/\nggplot2: Elegant Graphics for Data Analysis - https://ggplot2-book.org/\nData Visualization: A Practical Introduction - https://socviz.co/\nPublic Policy Analytics: Code & Context for Data Science in Government - https://urbanspatial.github.io/PublicPolicyAnalytics/index.html#\n\n\n\nVideos\n\nA channel with some great videos on ggplot - https://www.youtube.com/watch?v=HPJn1CMvtmI"
  },
  {
    "objectID": "resources.html#making-a-website",
    "href": "resources.html#making-a-website",
    "title": "Resources",
    "section": "Making a Website",
    "text": "Making a Website\nI made this website using Quarto, Github, and Netlify. I found these resources useful to get going:\n\nQuarto Documentation\nCreating a blog with Quarto in 10 steps - Beatriz Milz\nThe ultimate guide to starting a Quarto blog - Albert Rapp"
  },
  {
    "objectID": "posts/gradrank/index.html",
    "href": "posts/gradrank/index.html",
    "title": "Subjectively Ranking Philosophy Grad Programmes",
    "section": "",
    "text": "It‚Äôs that time of the year again. People are applying for grad school. If that‚Äôs you, I wish you all the best! Anyway, there‚Äôs all sorts of advice flying around and one of the things you might come across are rankings of grad programmes. In this post I want to consider a potentially helpful tool for subjectively ranking grad programmes.\nWhen I was looking into grad school for philosophy/political theory, I was recommended the Philosophical Gourmet Report (can you imagine - a ranking for philosophy programmes). Initially, it seemed helpful because I had no idea what to do, and it had breakdowns for subject specialism. But, there are a number of important criticisms of this ranking, and indeed rankings in general. These criticisms center around:\nNow maybe you can construct a counter-argument based on judgment aggregation to the effect of:\n‚Äúthese rankings pool the independent opinions of specialists in the field - so are likely to track well with quality of grad programmes and stuff‚Äù\nI have at least two concerns here."
  },
  {
    "objectID": "posts/gradrank/index.html#what-to-do-about-rankings",
    "href": "posts/gradrank/index.html#what-to-do-about-rankings",
    "title": "Subjectively Ranking Philosophy Grad Programmes",
    "section": "What to do about rankings?",
    "text": "What to do about rankings?\nSo there are limitations and problems with rankings of programmes. But it can be helpful for prospective grad students to consider rankings based on a bunch of attributes as a way to focus their applications. But I think these rankings should:\n\nContain information that matters to grad students (e.g.¬†going wider than just quality of faculty to stuff like cost of living in the area)\nHave that information be weighted according to their priorities\n\nTo some extent these concerns are addressed by the APDA project. It is great to see them consider student judgements but then also xyx"
  },
  {
    "objectID": "posts/gradrank/index.html#caveats-to-ranking",
    "href": "posts/gradrank/index.html#caveats-to-ranking",
    "title": "Subjectively Ranking Philosophy Grad Programmes",
    "section": "Caveats to ranking",
    "text": "Caveats to ranking\nNow that‚Äôs all good, but we need to be careful. No ranking will ever perfectly capture all the relevant information, and combine it in a non-controversial way. Some stuff can‚Äôt be quantified and made commensurable (e.g.¬†are you really going to weigh up proximity to family/friends with faculty citation count? Any utilitarians thinking about answering that can fight me). Even so, it doesn‚Äôt need to be perfect, just better than what we have now."
  },
  {
    "objectID": "posts/gradrank/index.html#just-make-your-own-ranking---lineupjs",
    "href": "posts/gradrank/index.html#just-make-your-own-ranking---lineupjs",
    "title": "Subjectively Ranking Philosophy Grad Programmes",
    "section": "Just make your own ranking - LineupJS",
    "text": "Just make your own ranking - LineupJS"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html",
    "href": "posts/TrussDodgyNetwork/index.html",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "",
    "text": "The recent ¬£45bn ‚Äúmini-budget‚Äù1 was a disaster in both content and consequence. This is widely understood. To believe it was about growth is beyond a joke. The government claimed that tax cuts for the rich will somehow lead to growth that will boost wages and support public services. This is either unsupported by the evidence, or the opposite is true. Shockingly, it turns out that tax cuts for the rich (who spend less and save more as a proportion of their income) will just benefit the rich.\nThe government then U-turned on plans to abolish the 45p additional rate of tax. For now, this still leaves roughly ¬£43bn (by 2026-27) of tax cuts such as the cancelled increase in corporation tax from 19% to 25%, the reversal of the temporary increase in national insurance, and the cancellation of the Health and Social Care Levy2.\nSo why are the government doing this? Simple. Truss and her cabinet are sock puppets for corporate power to further their own interests - not a novel feature of politics. Perhaps they genuinely believe, against the evidence, that tax cuts will create growth and prosperity for all. If so, we should pity them and their ideological delusions - not partake in them.\nA recent article in the Byline Times highlights the influence of corporate power by investigating Truss‚Äôs donors who collectively gave ¬£424,349 to her campaign. The Guardian updated this figure with new information taking the donation amounts to over ¬£500,000. These articles draw on the MP‚Äôs register of interests and reveal a dodgy network of billionaires, hedge fund managers, investment bankers, and others that will use the government to enrich themselves at the expense of everyone else.\nIf the remaining tax cuts in the ‚Äúmini-budget‚Äù pass, this will concentrate the wealth of this dodgy network further, and give them even more power to buy off the government.\nIn this post, I wanted to explore a way of visualising this network of dodgy donors, and explore the landscape of UK political donations more generally."
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#test",
    "href": "posts/TrussDodgyNetwork/index.html#test",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network",
    "section": "test",
    "text": "test\n\n\nCode\nchart = ForceGraph(miserables, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeTitle: d => `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l => Math.sqrt(l.value),\n  width,\n  height: 600,\n  invalidation // a promise to stop the simulation when the cell is re-run\n});\n\n\n\nmiserables = FileAttachment(\"miserables.json\").json()\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, ‚Ä¶])\n  links // an iterable of link objects (typically [{source, target}, ‚Ä¶])\n}, {\n  nodeId = d => d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) => source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) => target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) => N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) => ({id: N[i]}));\n  links = d3.map(links, (_, i) => ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) => N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) => W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) => L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) => color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) => T[i]);\n  if (invalidation != null) invalidation.then(() => simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y);\n\n    node\n      .attr(\"cx\", d => d.x)\n      .attr(\"cy\", d => d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet."
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#real",
    "href": "posts/TrussDodgyNetwork/index.html#real",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network",
    "section": "real",
    "text": "real\n\n\nCode\ngraph = FileAttachment(\"trussj.json\").json()\n\n//byline\n\nchart = ForceGraph(graph, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeText: d => d.info,\n  nodeTitle: d => `${d.id} (${d.group}) ${d.info}`,\n  linkStrokeWidth: l => Math.sqrt(l.value)/20,\n  nodeRadius: l => Math.sqrt(l.value)/10,\n  nodeStrength: -1500, // n => (Math.sqrt(n.value, 1))*-1,\n  width,\n  height: 500,\n  invalidation // a promise to stop the simulation when the cell is re-run\n});\n\n// Fixing it\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/disjoint-force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, ‚Ä¶])\n  links // an iterable of link objects (typically [{source, target}, ‚Ä¶])\n}, {\n  nodeId = d => d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeText,\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 0, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 100, // node radius, in pixels\n  nodeStrength, // = -900,\n  linkSource = ({source}) => source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) => target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) => N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const NT = nodeText == null ? null : d3.map(nodes, nodeText).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const R = typeof nodeRadius !== \"function\" ? null : d3.map(links, nodeRadius);\n  const NS = typeof nodeStrength !== \"function\" ? null : d3.map(links, nodeStrength);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) => ({id: N[i]}));\n  links = d3.map(links, (_, i) => ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) => N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", linkStroke)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) => W[i]);\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      //.attr(\"r\", nodeRadius)\n      .attr(\"r\", typeof nodeRadius !== \"function\" ? nodeRadius : null)\n      .call(drag(simulation));\n\n  if (G) node.attr(\"fill\", ({index: i}) => color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) => T[i]);\n  if (R) node.attr(\"r\", ({index: i}) => R[i]);\n  if (NS) node.attr(\"node-strength\", ({index: i}) => NS[i]);\n\n  // Handle invalidation.\n  if (invalidation != null) invalidation.then(() => simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y)\n      .attr(\"r\", d => d.value);\n\n    node\n      .attr(\"cx\", d => d.x)\n      .attr(\"cy\", d => d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet."
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#further-analysis",
    "href": "posts/TrussDodgyNetwork/index.html#further-analysis",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Further analysis",
    "text": "Further analysis"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#sankey",
    "href": "posts/TrussDodgyNetwork/index.html#sankey",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Sankey",
    "text": "Sankey"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#donations-to-party-by-donor-type",
    "href": "posts/TrussDodgyNetwork/index.html#donations-to-party-by-donor-type",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Donations to Party by Donor Type",
    "text": "Donations to Party by Donor Type\n\nMost of the Conservative Party‚Äôs funding comes from individuals and companies\n\n\n\n\n\n\nRoughly ¬£14m of the ¬£17m in donations received by the Conservative Parties so far this year has been from individuals and businesses. Not hugely surprising, but interesting to compare to the donation sources of the other parties. Its worth mentioning that opposition parties receive the bulk of public funding through the ‚ÄúShort Money‚Äù payment to fund their activities.\nSo that‚Äôs a broad look at the types of donors, quantums involved, and parties receiving donations. As they‚Äôre proving so lucrative for the Conservative Party, lets zoom in a bit on individual and company donations.\n\n\n10 Individuals have donated over ¬£250k to the Conservative Party so far this year\n\n\n\n\n\n\nhttps://www.parliament.uk/site-information/foi/transparency-publications/hoc-transparency-publications/financial-information/financial-assistance-to-opposition-parties/\nPUT MP REGISTER DATE\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet.\nAdd a bit about Andrew Law and Kwarteng\nThe break between markets and hedge funds\nNote somewhere about how you‚Äôve taken non-cash values\nhttps://www.theguardian.com/politics/2022/oct/05/liz-truss-raised-500000-for-bid-to-be-leader-register-of-interests-reveals UPDATE THE QUANTUMS"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#bars",
    "href": "posts/TrussDodgyNetwork/index.html#bars",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Bars",
    "text": "Bars\n\n\nCode\np <- don_by_party%>% \n  ggplot(aes(Value,DonorStatus,fill=Party_Name)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~Party_Name,ncol=1,scales = \"free_y\") +\n  scale_x_continuous(label=comma) +\n  theme_minimal()+\n  theme(legend.position=\"none\")\n\np"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#full-rain-cloud",
    "href": "posts/TrussDodgyNetwork/index.html#full-rain-cloud",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Full rain cloud",
    "text": "Full rain cloud\n\n\nCode\np <- don_by_party_indiv %>% \n  ggplot(aes(Value,DonorStatus,label=DonorName,colour=Party_Name)) +\n  geom_jitter(alpha=0.7,size=2.5) +\n  facet_wrap(~Party_Name,scales=\"free\",ncol = 1) +\n  scale_x_continuous(label=comma) +\n  labs(title = \"XYZ\")+\n  theme_minimal()+\n  theme(plot.title.position = \"plot\",\n        legend.position = \"none\")\n\np"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#raincloud-or-something",
    "href": "posts/TrussDodgyNetwork/index.html#raincloud-or-something",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Raincloud or something",
    "text": "Raincloud or something\n\n\nCode\nlibrary(ggbeeswarm)\nlibrary(hrbrthemes)\n\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n\n\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n\n\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n\nCode\np <- don_by_party_indiv %>% \n  filter( #Party_Name == \"Conservative\",\n         grepl(\"Individual\",DonorStatus)) %>% \n  ggplot(aes(Party_Name,Value,label=DonorName,fill=Party_Name)) +\n  geom_jitter(alpha=0.4,size=2.5,grouponX=TRUE) +\n  scale_y_continuous(label=comma) +\n  labs(title = \"XYZ\")+\n  #theme_minimal()+\n  theme_ft_rc() +\n  theme(plot.title.position = \"plot\",\n        legend.position = \"none\") +\n  coord_flip()\n\n\nWarning: Ignoring unknown parameters: grouponX\n\n\nCode\np\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nhttps://www.parliament.uk/site-information/foi/transparency-publications/hoc-transparency-publications/financial-information/financial-assistance-to-opposition-parties/\nPUT MP REGISTER DATE\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet.\nAdd a bit about Andrew Law and Kwarteng\nThe break between markets and hedge funds\nNote somewhere about how you‚Äôve taken non-cash values"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#trusss-dodgy-network",
    "href": "posts/TrussDodgyNetwork/index.html#trusss-dodgy-network",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Truss‚Äôs Dodgy Network",
    "text": "Truss‚Äôs Dodgy Network\n\n\nCode\nkey = Swatches(chart.scales.color)\n\ngraph = FileAttachment(\"trussj.json\").json() // scraped and cleaned from byline times article\n\n//byline\n\nchart = ForceGraph(graph, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeText: d => d.info,\n  nodeTitle: d => `${d.id} (${d.group}) ${d.info}`,\n  linkStrokeWidth: l => Math.sqrt(l.value)/20,\n  nodeRadius: l => Math.sqrt(l.value)/10,\n  nodeStrength: -1500, // n => (Math.sqrt(n.value, 1))*-1,\n  width,\n  height: 400,\n  invalidation // a promise to stop the simulation when the cell is re-run\n});\n\n// Fixing it\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/disjoint-force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, ‚Ä¶])\n  links // an iterable of link objects (typically [{source, target}, ‚Ä¶])\n}, {\n  nodeId = d => d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeText,\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 0, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 100, // node radius, in pixels\n  nodeStrength, // = -900,\n  linkSource = ({source}) => source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) => target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) => N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const NT = nodeText == null ? null : d3.map(nodes, nodeText).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const R = typeof nodeRadius !== \"function\" ? null : d3.map(links, nodeRadius);\n  const NS = typeof nodeStrength !== \"function\" ? null : d3.map(links, nodeStrength);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) => ({id: N[i]}));\n  links = d3.map(links, (_, i) => ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) => N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", linkStroke)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) => W[i]);\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      //.attr(\"r\", nodeRadius)\n      .attr(\"r\", typeof nodeRadius !== \"function\" ? nodeRadius : null)\n      .call(drag(simulation));\n\n  if (G) node.attr(\"fill\", ({index: i}) => color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) => T[i]);\n  if (R) node.attr(\"r\", ({index: i}) => R[i]);\n  if (NS) node.attr(\"node-strength\", ({index: i}) => NS[i]);\n\n  // Handle invalidation.\n  if (invalidation != null) invalidation.then(() => simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y)\n      .attr(\"r\", d => d.value);\n\n    node\n      .attr(\"cx\", d => d.x)\n      .attr(\"cy\", d => d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow there is much to be improved on here2, but in essence\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet.\nAdd a bit about Andrew Law and Kwarteng\nThe break between markets and hedge funds"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#trusss-dodgy-network-of-donors",
    "href": "posts/TrussDodgyNetwork/index.html#trusss-dodgy-network-of-donors",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Truss‚Äôs Dodgy Network of Donors",
    "text": "Truss‚Äôs Dodgy Network of Donors\n\nTwo individuals were able to donate 2/3rds of Truss‚Äôs campaign spending limit3 of ¬£300k\nHover over circles for information.\n\n\nCode\nkey = Swatches(chart.scales.color)\n\ngraph = FileAttachment(\"trussj.json\").json() // scraped and cleaned from byline times article\n\n//byline\n\nchart = ForceGraph(graph, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeText: d => d.info,\n  nodeTitle: d => `${d.id} (${d.group}) ${d.info}`,\n  linkStrokeWidth: l => Math.sqrt(l.value)/20,\n  nodeRadius: l => Math.sqrt(l.value)/10,\n  nodeStrength: -1500, // n => (Math.sqrt(n.value, 1))*-1,\n  width,\n  height: 400,\n  invalidation // a promise to stop the simulation when the cell is re-run\n});\n\n// Fixing it\n\n// Copyright 2021 Observable, Inc. - some edits from YK\n// Released under the ISC license.\n// https://observablehq.com/@d3/disjoint-force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, ‚Ä¶])\n  links // an iterable of link objects (typically [{source, target}, ‚Ä¶])\n}, {\n  nodeId = d => d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeText,\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 0, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 100, // node radius, in pixels\n  nodeStrength, // = -900,\n  linkSource = ({source}) => source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) => target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) => N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const NT = nodeText == null ? null : d3.map(nodes, nodeText).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const R = typeof nodeRadius !== \"function\" ? null : d3.map(links, nodeRadius);\n  const NS = typeof nodeStrength !== \"function\" ? null : d3.map(links, nodeStrength);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) => ({id: N[i]}));\n  links = d3.map(links, (_, i) => ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) => N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", linkStroke)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) => W[i]);\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      //.attr(\"r\", nodeRadius)\n      .attr(\"r\", typeof nodeRadius !== \"function\" ? nodeRadius : null)\n      .call(drag(simulation));\n\n  if (G) node.attr(\"fill\", ({index: i}) => color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) => T[i]);\n  if (R) node.attr(\"r\", ({index: i}) => R[i]);\n  if (NS) node.attr(\"node-strength\", ({index: i}) => NS[i]);\n\n  // Handle invalidation.\n  if (invalidation != null) invalidation.then(() => simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y)\n      .attr(\"r\", d => d.value);\n\n    node\n      .attr(\"cx\", d => d.x)\n      .attr(\"cy\", d => d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere‚Äôs what I‚Äôve got. I scraped the Byline Times article and just visualised the information (full credit to them). The orange circle in the center is Truss. The blue circles in orbit are each of the donors. The circles and links are scaled by the ¬£ amount of their donation. In the interest of time, I haven‚Äôt got information on the additional donors mentioned in the Guardian here. So this covers ¬£424,349 of donations.\nThe two largest blue circles each represent ¬£100k of donations from Fitriani Hay and Natasha Barnaba. You can hover over to see more information about them. Also of note is Andrew Law (smaller circle - southeast) - who donated ¬£5,127. Law hosted a champagne reception after the mini budget with Kwasi Kwarteng in attendance.\nAnyway there is much to be improved here4, but I think it serves to illustrate the dodgy financing network surrounding Truss and co. In the future, I hope to return to this sort of representation after some improvement. Obvious props to Sophie E. Hill‚Äôs ‚ÄúMy Little Crony‚Äù for inspiration. Lets take it a bit further."
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#further-analysis---a-brief-look-at-uk-political-donations",
    "href": "posts/TrussDodgyNetwork/index.html#further-analysis---a-brief-look-at-uk-political-donations",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Further Analysis - A Brief Look at UK Political Donations",
    "text": "Further Analysis - A Brief Look at UK Political Donations\nSo we‚Äôve had a quick look at the recent donors surrounding Truss. But how do things look beyond just Truss‚Äôs donors? What does the landscape of political donations in the UK look like?\nTo answer this, I‚Äôve taken data from the Electoral Commission website for political donations in 2022 to date (all available quarters). I am focusing on Con/Lab/Lib/SNP, and I‚Äôve tidied it up like so:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(scales)\nlibrary(plotly)\n\n# Get list of MPs and donations from 2022\n\nmps <- read_csv(\"mps.csv\")\n\ndon <- read_csv(\"2022-electoral-commission-donations.csv\")\n\n# Cleaning donation names\n\ndon <- don %>% \n  mutate(RegulatedEntityName = str_remove_all(RegulatedEntityName, \"Dr |Ms |Mrs |Mr |The Rt Hon |Rt Hon | MP| MP |Dame |Sir \")) %>% \n  select(DonorName,DonorStatus,RegulatedEntityName,RegulatedDoneeType,Value)\n\n# Merge first and last name in MPs so we can make a lookup to party\n\nmps <- mps %>%\n  select(`First name`,`Last name`, Party) %>% \n  mutate(`Full name` = paste(`First name`,`Last name`, sep = \" \"))\n\n# Join donation data and MP/party lookup\n\ndon_by_party <- don %>% \n  full_join(mps, by=c(\"RegulatedEntityName\"=\"Full name\")) %>% \n  filter(!is.na(Value)) %>% # get rid of no donations\n\n# Classify the donations that go to parties rather than individuals. Grepl should catch this  \n  \n  mutate(Party_Name = case_when(\n    \n    # Most individual donations\n    \n    RegulatedDoneeType==\"MP - Member of Parliament\" ~ Party,\n    RegulatedDoneeType==\"Leadership Candidate\" ~ Party,\n    \n    # Party donations\n    \n    grepl(\"Liberal Democrat\",RegulatedEntityName) ~ \"Liberal Democrat\",\n    grepl(\"Conservative\",RegulatedEntityName) ~ \"Conservative\",\n    grepl(\"Labour\",RegulatedEntityName) ~ \"Labour\",\n    grepl(\"Green Party\",RegulatedEntityName) ~ \"Green\",\n    grepl(\"Scottish National Party\",RegulatedEntityName) ~ \"Scottish National Party\",\n    grepl(\"Sinn F√©in\",RegulatedEntityName) ~ \"Sinn F√©in\",\n    grepl(\"Plaid\",RegulatedEntityName) ~ \"Plaid Cymru\",\n    grepl(\"Democratic Unionist Party\",RegulatedEntityName) ~ \"Democratic Unionist Party\",\n    \n    # I am making no distinction between labour and the cooperative party because xyz\n    \n    grepl(\"Co-operative Party\",RegulatedEntityName) ~ \"Labour\")) %>% \n    \n    # There are a few MPs who evade classification due to missing data in donation register or because of name spelling and lookup (e.g. Tom vs Thomas). Fix this manually :(\n  \n  mutate(\n      \n      Party_Name = if_else(grepl(\"David Lammy|Jamie Driscoll|Stephen McCabe\",\n                                 RegulatedEntityName), \"Labour\", Party_Name),\n      \n      Party_Name = if_else(grepl(\"Tom Tugendhat|Andy Street|Steve Baker|Crispin Jeremy Rupert Blunt|Christopher Grayling\",\n                                 RegulatedEntityName), \"Conservative\", Party_Name),\n      \n    # Fix that issue where one donation has been classified as labour and coop\n      \n      Party_Name = if_else(grepl(\"Labour/Co-operative\",Party_Name),\"Labour\",Party_Name)\n        )\n\n# tbh I was only going to look at Con/Lab/Lib/SNP so...\n\ndon_by_party_indiv <- don_by_party %>% \n  select(Party_Name,DonorName,DonorStatus,Value) %>% \n  mutate(Value = str_remove_all(Value, \"¬£|,\"),\n         Value = as.numeric(Value)) %>% \n  group_by(Party_Name,DonorName,DonorStatus) %>% \n  summarise(Value = sum(Value)) %>% ungroup() %>% \n  filter(grepl(\"Labour|Conservative|Liberal Democrat|Scottish National Party\",Party_Name))\n\ndon_by_party <- don_by_party_indiv %>% # be careful here - you mean to say non-agg, not the individual donor category\n  group_by(Party_Name,DonorStatus) %>% \n  summarise(Value = sum(Value)) %>% ungroup() \n\nglimpse(don_by_party_indiv)\n\n\nRows: 954\nColumns: 4\n$ Party_Name  <chr> \"Conservative\", \"Conservative\", \"Conservative\", \"Conservat‚Ä¶\n$ DonorName   <chr> \"(AQ) Networks Ltd\", \"8hwe Ltd\", \"Abraham H Busby\", \"Acces‚Ä¶\n$ DonorStatus <chr> \"Company\", \"Company\", \"Individual\", \"Company\", \"Company\", ‚Ä¶\n$ Value       <dbl> 28000.00, 3000.00, 35068.88, 10000.00, 2400.00, 10000.00, ‚Ä¶\n\n\nNote - I have taken donations to individual MPs within parties and donations to parties themselves. I sum these and aggregate to get the total donations going towards the party. I have checked, and I don‚Äôt think this double counts. Do let me know if you reckon otherwise.\n\nMost of the Conservative Party‚Äôs funding comes from individuals and companies\n\n\n\n\n\n\nRoughly ¬£14m of the ¬£17m in donations received by the Conservative Parties so far this year has been from individuals and businesses. Not hugely surprising, but interesting to compare to the donation sources of the other parties. Its worth mentioning that opposition parties receive the bulk of public funding through the ‚ÄúShort Money‚Äù payment to fund their activities.\nSo that‚Äôs a broad look at the types of donors, quantums involved, and parties receiving donations. As they‚Äôre proving so lucrative for the Conservative Party, lets zoom in a bit on individual and company donations.\n\n\nThe top 10% of Conservative donors who are individuals make up 38% of total party donations in 2022 so far‚Ä¶\n\nJitterOrdered dots\n\n\nMedian donation from individual towards party: ¬£10,000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI have used a log scale here to better view the distribution and ordering. Also I don‚Äôt like how the ordered points obscure the density/overplotting. The jitter is probably better here.\n\n\n\n\n\n\n\n\n\n\n\nhttps://www.parliament.uk/site-information/foi/transparency-publications/hoc-transparency-publications/financial-information/financial-assistance-to-opposition-parties/\nPUT MP REGISTER DATE\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet.\nAdd a bit about Andrew Law and Kwarteng\nThe break between markets and hedge funds\nNote somewhere about how you‚Äôve taken non-cash values\nhttps://www.theguardian.com/politics/2022/oct/05/liz-truss-raised-500000-for-bid-to-be-leader-register-of-interests-reveals UPDATE THE QUANTUMS"
  },
  {
    "objectID": "posts/test/index.html",
    "href": "posts/test/index.html",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "",
    "text": "blah blah\nThe recent ¬£45bn ‚Äúmini-budget‚Äù1 was a disaster in both content and consequence. This is widely understood. To believe it was about growth is beyond a joke. The government claimed that tax cuts for the rich will somehow lead to growth that will boost wages and support public services. This is either unsupported by the evidence, or the opposite is true. Shockingly, it turns out that tax cuts for the rich (who spend less and save more as a proportion of their income) will just benefit the rich.\nThe government has just U-turned on plans to abolish the 45p additional rate of tax. For now, this still leaves roughly ¬£43bn (by 2026-27) of tax cuts such as the cancelled increase in corporation tax from 19% to 25%, the reversal of the temporary increase in national insurance, and the cancellation of the Health and Social Care Levy2.\nSo why are the government doing this? Simple. Truss and her cabinet are sock puppets for corporate power to further their own interests - not a novel feature of politics. A recent article in the Byline Times looks at this by investigating Truss‚Äôs donors who collectively gave ¬£424,349 to her campaign. The article draws on the MP‚Äôs register of interests and reveals a dodgy network of billionaires, hedge fund managers, investment bankers, and others that will use the government to enrich themselves at the expense of everyone else.\nIf the remaining tax cuts in the ‚Äúmini-budget‚Äù pass, this will concentrate the wealth of this dodgy network further, and give them even more power to buy off the government.\nIn this post, I wanted to explore a way of visualising this network of dodgy donors, and explore the landscape of UK political donations more generally."
  },
  {
    "objectID": "posts/test/index.html#trusss-dodgy-network-of-donors",
    "href": "posts/test/index.html#trusss-dodgy-network-of-donors",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Truss‚Äôs Dodgy Network of Donors",
    "text": "Truss‚Äôs Dodgy Network of Donors\nHover over circles for information.\n\nkey = Swatches(chart.scales.color)\n\ngraph = FileAttachment(\"trussj.json\").json() // scraped and cleaned from byline times article\n\n//byline\n\nchart = ForceGraph(graph, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeText: d => d.info,\n  nodeTitle: d => `${d.id} (${d.group}) ${d.info}`,\n  linkStrokeWidth: l => Math.sqrt(l.value)/20,\n  nodeRadius: l => Math.sqrt(l.value)/10,\n  nodeStrength: -1500, // n => (Math.sqrt(n.value, 1))*-1,\n  width,\n  height: 400,\n  invalidation // a promise to stop the simulation when the cell is re-run\n});\n\n// Fixing it\n\n// Copyright 2021 Observable, Inc. - some edits from YK\n// Released under the ISC license.\n// https://observablehq.com/@d3/disjoint-force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, ‚Ä¶])\n  links // an iterable of link objects (typically [{source, target}, ‚Ä¶])\n}, {\n  nodeId = d => d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeText,\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 0, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 100, // node radius, in pixels\n  nodeStrength, // = -900,\n  linkSource = ({source}) => source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) => target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) => N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const NT = nodeText == null ? null : d3.map(nodes, nodeText).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const R = typeof nodeRadius !== \"function\" ? null : d3.map(links, nodeRadius);\n  const NS = typeof nodeStrength !== \"function\" ? null : d3.map(links, nodeStrength);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) => ({id: N[i]}));\n  links = d3.map(links, (_, i) => ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) => N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", linkStroke)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) => W[i]);\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      //.attr(\"r\", nodeRadius)\n      .attr(\"r\", typeof nodeRadius !== \"function\" ? nodeRadius : null)\n      .call(drag(simulation));\n\n  if (G) node.attr(\"fill\", ({index: i}) => color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) => T[i]);\n  if (R) node.attr(\"r\", ({index: i}) => R[i]);\n  if (NS) node.attr(\"node-strength\", ({index: i}) => NS[i]);\n\n  // Handle invalidation.\n  if (invalidation != null) invalidation.then(() => simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y)\n      .attr(\"r\", d => d.value);\n\n    node\n      .attr(\"cx\", d => d.x)\n      .attr(\"cy\", d => d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOkay so here‚Äôs what I‚Äôve got. I scraped the Byline Times article and just visualised the information (full credit to them). The orange circle in the center is Truss. The blue circles in orbit are each of the donors. The circles and links are scaled by the ¬£ amount of their donation.\nIf we take a look at the circles xyz\nNow there is much to be improved here3, but I think it begins to illustrate the dodgy financing network surrounding Truss and co. Lets take it a bit further."
  },
  {
    "objectID": "posts/test/index.html#further-analysis---a-brief-look-at-uk-political-donations",
    "href": "posts/test/index.html#further-analysis---a-brief-look-at-uk-political-donations",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Further Analysis - A Brief Look at UK Political Donations",
    "text": "Further Analysis - A Brief Look at UK Political Donations\nSo we‚Äôve had a quick look at the recent donors surrounding Truss. But how do things look beyond just Truss‚Äôs donors? What does the landscape of political donations in the UK look like?\nTo answer this, I‚Äôve taken data from the Electoral Commission website for political donations in 2022 to date (all available quarters). I am focusing on Con/Lab/Lib/SNP, and I‚Äôve tidied it up like so:\n\n# Get list of MPs and donations from 2022\n\nmps <- read_csv(\"mps.csv\")\n\ndon <- read_csv(\"2022-electoral-commission-donations.csv\")\n\n# Cleaning donation names\n\ndon <- don %>% \n  mutate(RegulatedEntityName = str_remove_all(RegulatedEntityName, \"Dr |Ms |Mrs |Mr |The Rt Hon |Rt Hon | MP| MP |Dame |Sir \")) %>% \n  select(DonorName,DonorStatus,RegulatedEntityName,RegulatedDoneeType,Value)\n\n# Merge first and last name in MPs so we can make a lookup to party\n\nmps <- mps %>%\n  select(`First name`,`Last name`, Party) %>% \n  mutate(`Full name` = paste(`First name`,`Last name`, sep = \" \"))\n\n# Join donation data and MP/party lookup\n\ndon_by_party <- don %>% \n  full_join(mps, by=c(\"RegulatedEntityName\"=\"Full name\")) %>% \n  filter(!is.na(Value)) %>% # get rid of no donations\n\n# Classify the donations that go to parties rather than individuals. Grepl should catch this  \n  \n  mutate(Party_Name = case_when(\n    \n    # Most individual donations\n    \n    RegulatedDoneeType==\"MP - Member of Parliament\" ~ Party,\n    RegulatedDoneeType==\"Leadership Candidate\" ~ Party,\n    \n    # Party donations\n    \n    grepl(\"Liberal Democrat\",RegulatedEntityName) ~ \"Liberal Democrat\",\n    grepl(\"Conservative\",RegulatedEntityName) ~ \"Conservative\",\n    grepl(\"Labour\",RegulatedEntityName) ~ \"Labour\",\n    grepl(\"Green Party\",RegulatedEntityName) ~ \"Green\",\n    grepl(\"Scottish National Party\",RegulatedEntityName) ~ \"Scottish National Party\",\n    grepl(\"Sinn F√©in\",RegulatedEntityName) ~ \"Sinn F√©in\",\n    grepl(\"Plaid\",RegulatedEntityName) ~ \"Plaid Cymru\",\n    grepl(\"Democratic Unionist Party\",RegulatedEntityName) ~ \"Democratic Unionist Party\",\n    \n    # I am making no distinction between labour and the cooperative party because xyz\n    \n    grepl(\"Co-operative Party\",RegulatedEntityName) ~ \"Labour\")) %>% \n    \n    # There are a few MPs who evade classification due to missing data in donation register or because of name spelling and lookup (e.g. Tom vs Thomas). Fix this manually :(\n  \n  mutate(\n      \n      Party_Name = if_else(grepl(\"David Lammy|Jamie Driscoll|Stephen McCabe\",\n                                 RegulatedEntityName), \"Labour\", Party_Name),\n      \n      Party_Name = if_else(grepl(\"Tom Tugendhat|Andy Street|Steve Baker|Crispin Jeremy Rupert Blunt|Christopher Grayling\",\n                                 RegulatedEntityName), \"Conservative\", Party_Name),\n      \n    # Fix that issue where one donation has been classified as labour and coop\n      \n      Party_Name = if_else(grepl(\"Labour/Co-operative\",Party_Name),\"Labour\",Party_Name)\n        )\n\n# tbh I was only going to look at Con/Lab/Lib/SNP so...\n\ndon_by_party_indiv <- don_by_party %>% \n  select(Party_Name,DonorName,DonorStatus,Value) %>% \n  mutate(Value = str_remove_all(Value, \"¬£|,\"),\n         Value = as.numeric(Value)) %>% \n  group_by(Party_Name,DonorName,DonorStatus) %>% \n  summarise(Value = sum(Value)) %>% ungroup() %>% \n  filter(grepl(\"Labour|Conservative|Liberal Democrat|Scottish National Party\",Party_Name))\n\ndon_by_party <- don_by_party_indiv %>% \n  group_by(Party_Name,DonorStatus) %>% \n  summarise(Value = sum(Value)) %>% ungroup() \n\nglimpse(don_by_party_indiv)\n\nRows: 954\nColumns: 4\n$ Party_Name  <chr> \"Conservative\", \"Conservative\", \"Conservative\", \"Conservat‚Ä¶\n$ DonorName   <chr> \"(AQ) Networks Ltd\", \"8hwe Ltd\", \"Abraham H Busby\", \"Acces‚Ä¶\n$ DonorStatus <chr> \"Company\", \"Company\", \"Individual\", \"Company\", \"Company\", ‚Ä¶\n$ Value       <dbl> 28000.00, 3000.00, 35068.88, 10000.00, 2400.00, 10000.00, ‚Ä¶"
  },
  {
    "objectID": "posts/test/index.html#donations-to-party-by-donor-type",
    "href": "posts/test/index.html#donations-to-party-by-donor-type",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Donations to Party by Donor Type",
    "text": "Donations to Party by Donor Type\n\nMost of the Conservative Party‚Äôs funding comes from individuals and companies\n\n\n\n\n\n\nRoughly ¬£14m of the ¬£17m in donations received by the Conservative Parties this year has been from individuals and businesses. This shouldn‚Äôt be too surprising - public funding is mainly available for the opposition party through the ‚ÄúShort Money‚Äù system."
  },
  {
    "objectID": "posts/test/index.html#raincloud-or-something",
    "href": "posts/test/index.html#raincloud-or-something",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Raincloud or something",
    "text": "Raincloud or something\n\np <- don_by_party_indiv %>% \n  filter(Party_Name == \"Conservative\",\n         grepl(\"Individual|Company\",DonorStatus)) %>% \n  ggplot(aes(Value,DonorStatus,label=DonorName,fill=DonorStatus)) +\n  geom_jitter(alpha=0.7,size=2.5) +\n  facet_wrap(~DonorStatus,scales=\"free\",nrow = 2) +\n  scale_x_continuous(label=comma) +\n  labs(title = \"XYZ\")+\n  theme_minimal()+\n  theme(plot.title.position = \"plot\",\n        legend.position = \"none\")\n\n  \nggplotly(p)\n\n\n\n\n\nhttps://www.parliament.uk/site-information/foi/transparency-publications/hoc-transparency-publications/financial-information/financial-assistance-to-opposition-parties/\nPUT MP REGISTER DATE\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet.\nAdd a bit about Andrew Law and Kwarteng\nThe break between markets and hedge funds\nNote somewhere about how you‚Äôve taken non-cash values"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#a-brief-look-at-uk-political-donations",
    "href": "posts/TrussDodgyNetwork/index.html#a-brief-look-at-uk-political-donations",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "A Brief Look at UK Political Donations",
    "text": "A Brief Look at UK Political Donations\nSo we‚Äôve seen the recent donors surrounding Truss. But how do things look beyond just Truss‚Äôs donors? What does the landscape of political donations in the UK look like? For some good general breakdowns - see the Electoral Commissions visuals. For a general report on the influence of big finance on democracy in the UK, see this report published by Positive Money over the summer. This is an incredibly comprehensive report, and they suggest a number of recommendations that you‚Äôd think would already be implemented as common sense.\nAnyway, to have my own look at the landscape, I‚Äôve taken data from the Electoral Commission website5 for political donations reported6 in 2022 to date. I am focusing on Con/Lab/Lib/SNP, and I‚Äôve tidied it up like so:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(scales)\n\n\n# Get list of MPs and donations from 2022\n\nmps <- read_csv(\"mps.csv\")\n\ndon <- read_csv(\"2022-electoral-commission-donations.csv\")\n\n# Cleaning donation names\n\ndon <- don %>% \n  mutate(RegulatedEntityName = str_remove_all(RegulatedEntityName, \"Dr |Ms |Mrs |Mr |The Rt Hon |Rt Hon | MP| MP |Dame |Sir \")) %>% \n  select(DonorName,DonorStatus,RegulatedEntityName,RegulatedDoneeType,Value)\n\n\n# Merge first and last name in MPs so we can make a lookup to party\n\nmps <- mps %>%\n  select(`First name`,`Last name`, Party) %>% \n  mutate(`Full name` = paste(`First name`,`Last name`, sep = \" \"))\n\n# Join donation data and MP/party lookup\n\ndon_by_party <- don %>% \n  full_join(mps, by=c(\"RegulatedEntityName\"=\"Full name\")) %>% \n  filter(!is.na(Value)) %>% # get rid of no donations\n\n# Classify the donations that go to parties rather than individuals. Grepl should catch this. But you need to be careful here as it could just catch stuff like the \"Labour\" in SDLP - Social Democratic and Labour Party and then classify it as Labour. In fact, filter them here:\n  \n  filter(!grepl(\"SDLP\",RegulatedEntityName)) %>% \n  \n  mutate(Party_Name = case_when(\n    \n    # Most individual donations\n    \n    RegulatedDoneeType==\"MP - Member of Parliament\" ~ Party,\n    RegulatedDoneeType==\"Leadership Candidate\" ~ Party,\n    \n    # Party donations\n    \n    grepl(\"Liberal Democrat\",RegulatedEntityName) ~ \"Liberal Democrats\",\n    grepl(\"Conservative\",RegulatedEntityName) ~ \"Conservative\",\n    grepl(\"Labour\",RegulatedEntityName) ~ \"Labour\",\n    grepl(\"Green Party\",RegulatedEntityName) ~ \"Green\",\n    grepl(\"Scottish National Party\",RegulatedEntityName) ~ \"Scottish National Party\",\n    grepl(\"Sinn F√©in\",RegulatedEntityName) ~ \"Sinn F√©in\",\n    grepl(\"Plaid\",RegulatedEntityName) ~ \"Plaid Cymru\",\n    grepl(\"Democratic Unionist Party\",RegulatedEntityName) ~ \"Democratic Unionist Party\",\n    \n    # I am making no distinction between labour and the cooperative party because due tp their pact and organisation, I reckon its functionally the same \n    \n    grepl(\"Co-operative Party\",RegulatedEntityName) ~ \"Labour\")) %>% \n    \n    # There are a few MPs who evade classification due to missing data in donation register or because of name spelling and lookup (e.g. Tom vs Thomas). Fix this manually :(\n  \n  mutate(\n      \n      Party_Name = if_else(grepl(\"David Lammy|Jamie Driscoll|Stephen McCabe\",\n                                 RegulatedEntityName), \"Labour\", Party_Name),\n      \n      Party_Name = if_else(grepl(\"Tom Tugendhat|Andy Street|Steve Baker|Crispin Jeremy Rupert Blunt|Christopher Grayling\",\n                                 RegulatedEntityName), \"Conservative\", Party_Name),\n      \n    # Fix that issue where one donation has been classified as labour and coop\n      \n      Party_Name = if_else(grepl(\"Labour/Co-operative\",Party_Name),\"Labour\",Party_Name)\n        )\n\n# tbh I was only going to look at Con/Lab/Lib/SNP so...\n\ndon_by_party_indiv <- don_by_party %>% \n  select(Party_Name,DonorName,DonorStatus,Value) %>% \n  mutate(Value = str_remove_all(Value, \"¬£|,\"),\n         Value = as.numeric(Value)) %>% \n  group_by(Party_Name,DonorName,DonorStatus) %>% \n  summarise(Value = sum(Value)) %>% ungroup() %>% \n  filter(grepl(\"Labour|Conservative|Liberal Democrats|Scottish National Party\",Party_Name))\n\ndon_by_party <- don_by_party_indiv %>% # be careful here - you mean to say non-agg, not the individual donor category\n  group_by(Party_Name,DonorStatus) %>% \n  summarise(Value = sum(Value)) %>% ungroup() \n\nglimpse(don_by_party_indiv)\n\n\nRows: 945\nColumns: 4\n$ Party_Name  <chr> \"Conservative\", \"Conservative\", \"Conservative\", \"Conservat‚Ä¶\n$ DonorName   <chr> \"(AQ) Networks Ltd\", \"8hwe Ltd\", \"Abraham H Busby\", \"Acces‚Ä¶\n$ DonorStatus <chr> \"Company\", \"Company\", \"Individual\", \"Company\", \"Company\", ‚Ä¶\n$ Value       <dbl> 28000.00, 3000.00, 35068.88, 10000.00, 2400.00, 10000.00, ‚Ä¶\n\n\nYou can download the csv I‚Äôve used here.\nA few things to note:\n\nI have taken donations to individual MPs within parties and donations to parties themselves. I sum these and aggregate to get the total donations going towards the party. I have checked, and I don‚Äôt think this double counts. Do let me know if you reckon otherwise.\nI‚Äôve summed the value of cash and non-cash donations. Yes this is crude and it relies on them reliably stating the value of any in-kind transfers. Not sure what to do otherwise. Also I‚Äôm sure there are so many other unmonetised benefits that accrue from the social aspects of networks (I‚Äôm not using this as an excuse, just pointing it out)\nDonations listed on the Electoral Commission website are only reported if they meet certain conditions (e.g.¬†if its a donation to the central party, then its reported if the donation is greater than or equal to ¬£7,500 - or over ¬£1500 if the donor or lender gives further during the calendar year). I think this means when I say x% of total donations come from y, that total is only total disclosed donations rather than all donations received by the party (I don‚Äôt think 100 people only donating ¬£50 each would be reported). So every time I use the donations data to make claims about sources and totals, only reported donations are in scope.7\n\n\nMost of the Conservative Party and MP donations come from individuals and companies\nPolitical donations to the major parties broken down by donor status. Figures given for donations reported in 2022 so far.\n\n\n\n\n\n\nRoughly ¬£14m of the ¬£17m in donations received by the Conservative Parties so far this year has been from individuals and companies. Not hugely surprising, but interesting to compare to the donation sources of the other parties. Its worth mentioning that opposition parties receive the bulk of public funding through the ‚ÄúShort Money‚Äù payment8 to fund their activities.\nSo that‚Äôs a broad look at the types of donors and quantums involved for the major parties. As individual and company donations are proving so lucrative for the Conservative Party, lets zoom in a bit on them.\n\n\nThe top 10% of individual Conservative Party and MP donors make up 38% of their total donations...\nHover over circles for information.\n\nJitterOrdered dots\n\n\nDashed line represents median of all donations from individuals to the major parties: ¬£10,000\n\n\nCode\nlibrary(plotly)\n\n# Nah change this to ordered points - the jitter is too dense - if its ordered points I can put the median there too.\n\nmedian_indiv_don <- don_by_party_indiv %>% \n  filter(DonorStatus==\"Individual\") \n\nmedian_indiv_don <- median(median_indiv_don$Value)\n\nsum_party <- don_by_party_indiv %>%\n  group_by(Party_Name) %>% \n  summarise(sum(Value)) %>% \n  rename(total_don = `sum(Value)`)\n\ndon_top_dec <- don_by_party_indiv %>% \n  filter(DonorStatus==\"Individual\") %>% \n  group_by(Party_Name) %>% \n  mutate(decile=ntile(Value,n=10)) %>% \n  ungroup() %>% \n  group_by(decile,Party_Name) %>% \n  summarise(sum(Value)) %>% \n  ungroup()\n\ntitle_stat <- don_top_dec %>% \n  left_join(sum_party,by=\"Party_Name\") %>% \n  mutate(perc = `sum(Value)`/total_don)\n\n# should I chuck all my theming stuff in a function? Yes. Will I do it? ...soon\n\np <- don_by_party_indiv %>% \n  filter(grepl(\"Individual\",DonorStatus)) %>% \n  ggplot(aes(Party_Name,Value,label=DonorName,fill=Party_Name)) +\n  geom_jitter(alpha=0.6,size=2,grouponX=TRUE,colour=\"black\") +\n  scale_y_continuous(labels = label_number(suffix = \" k\", scale = 1e-3)) +\n  theme_minimal()+\n  theme(legend.position = \"none\",\n        axis.text = element_text(face=\"bold\",size = 10),\n        axis.title = element_text(face=\"bold\",size = 12)) +\n  coord_flip()+\n  geom_hline(yintercept = 10000,linetype=\"dashed\") +\n  scale_fill_manual(values=c(\"navy\",\"red\",\"gold\",\"yellow\")) +\n  xlab(\"\") + ylab(\"Donation Amount (¬£)\")\n\n\nggplotly(p)\n\n\n\n\nIndividual here means individuals registered on a UK electoral register. Figures given for 2022 so far.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI have used a log scale here to better view the distribution and ordering. Also I don‚Äôt like how the ordered points obscure the density/overplotting. The jitter is probably better here.\n\n\n\n\nCode\np<-don_by_party_indiv[order(don_by_party_indiv$Value),] %>%\n filter(DonorStatus==\"Individual\") %>% \n ggplot(aes(x=seq(Value), y=Value, lab=DonorName,fill=Party_Name)) +\n  geom_point(alpha=0.5,size=3.5) +\n  theme_bw() +\n  scale_y_continuous(labels = label_number(suffix = \" k\", scale = 1e-3), trans=\"log10\") + \n  theme(axis.text.x=element_blank(),\n  panel.spacing.x = unit(1.5, \"lines\"),\n  panel.spacing.y = unit(1.5, \"lines\"),\n  axis.ticks.x=element_blank(),\n  legend.position=\"none\",\n  axis.text = element_text(face=\"bold\",size = 10),\n  axis.title = element_text(face=\"bold\",size = 12)) +\n  xlab(\"Individual donors ordered by ¬£\") +\n  ylab(\"Donation Amount (¬£)\") +\n  facet_wrap(~Party_Name) +\n  geom_hline(yintercept = 10000,linetype=\"dashed\") +\n  scale_fill_manual(values=c(\"navy\",\"red\",\"gold\",\"yellow\"))\n\np<-ggplotly(p)\n\np\n\n\n\n\nIndividual here means individuals registered on a UK electoral register. Figures given for 2022 so far.\n\n\n\n\n\nNow this is quite mad. The top 10% of individual donors providing 38% of total donations to the Conservative party! The equivalent stat for Labour is 7% (but due to the use of Short Money, I wasn‚Äôt sure if that was a fair comparison). In any case, the distribution is quite telling. Lets take a closer look at some of these donors.\n\nThat small dot to the right is Christopher Charles Sheriff Harborne - a businessman and tech investor who has given ¬£1,015,000 so far this year. Harborne was a massive donor to the Brexit party and had been a regular donor to the Conservative Party in previous years.\nNext to Christopher is Mark J Bamford - the chairman of the machinery manufacturer JCB who has given ¬£973,000 so far this year. Another big supporter of Brexit.\nIn third place its Malcom S Healey - an entrepreneur/businessman/rich person (if only there was some word for sitting around and accumulating wealth because you own things) who has donated ¬£550,000 so far this year\n\nI‚Äôd encourage you to hover over the names and do some googling. If there‚Äôs a concern that I‚Äôm unfairly singling out the Conservative donors whilst ignoring the large donations from individuals to other parties:\n\nThese other parties are not currently in power - so I‚Äôm less focused on them\nLarge political donations and the capture of politics by wealth is a problem for any party/organisation/etc. - e.g.¬†see the failure of Labour under Blair to push the matter of capping political donations early in their first term, the Cash-for-Honours scandal, and then the failure of Labour, the Lib Dems, and Conservatives to agree to the suggestions in the Phillips review\n\nWe can do the same comparisons for donations from companies.\n\n\nMore companies give larger donations to the Conservative Party than the other parties. However, the maximum donations from companies are still smaller than donations from some individuals\nHover over circles for information.\n\nJitterOrdered dots\n\n\nDashed line represents median of all donations from companies to the major parties: ¬£10,000 (yes I believe this is also ¬£10k like the indiv. donations)\n\n\nCode\n# Nah change this to ordered points - the jitter is too dense - if its ordered points I can put the median there too.\n\nmedian_comp_don <- don_by_party_indiv %>% \n  filter(DonorStatus==\"Company\") \n\nmedian_comp_don <- median(median_comp_don$Value)\n\n\np <- don_by_party_indiv %>% \n  filter(grepl(\"Company\",DonorStatus)) %>% \n  ggplot(aes(Party_Name,Value,label=DonorName,fill=Party_Name)) +\n  geom_jitter(alpha=0.6,size=2,grouponX=TRUE,colour=\"black\") +\n  scale_y_continuous(labels = label_number(suffix = \" k\", scale = 1e-3)) +\n  theme_minimal()+\n  theme(legend.position = \"none\",\n        axis.text = element_text(face=\"bold\",size = 10),\n        axis.title = element_text(face=\"bold\",size = 12)) +\n  coord_flip()+\n  geom_hline(yintercept = 10000,linetype=\"dashed\") +\n  scale_fill_manual(values=c(\"navy\",\"red\",\"gold\",\"yellow\")) +\n  xlab(\"\") + ylab(\"Donation Amount (¬£)\")\n\n\nggplotly(p)\n\n\n\n\nCompany refers to a UK-registered company which is incorporated in the UK and carries on business in the UK. Figures given for 2022 so far.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI have used a log scale here to better view the distribution and ordering. Also I don‚Äôt like how the ordered points obscure the density/overplotting. The jitter is probably better here.\n\n\n\n\nCode\np<-don_by_party_indiv[order(don_by_party_indiv$Value),] %>%\n filter(DonorStatus==\"Company\") %>% \n ggplot(aes(x=seq(Value), y=Value, lab=DonorName,fill=Party_Name)) +\n  geom_point(alpha=0.5,size=3.5) +\n  theme_bw() +\n  scale_y_continuous(labels = label_number(suffix = \" k\", scale = 1e-3), trans=\"log10\") + \n  theme(axis.text.x=element_blank(),\n  panel.spacing.x = unit(1.5, \"lines\"),\n  panel.spacing.y = unit(1.5, \"lines\"),\n  axis.ticks.x=element_blank(),\n  legend.position=\"none\",\n  axis.text = element_text(face=\"bold\",size = 10),\n  axis.title = element_text(face=\"bold\",size = 12)) +\n  xlab(\"Company donors ordered by ¬£\") +\n  ylab(\"Donation Amount (¬£)\") +\n  facet_wrap(~Party_Name) +\n  geom_hline(yintercept = 10000,linetype=\"dashed\") +\n  scale_fill_manual(values=c(\"navy\",\"red\",\"gold\",\"yellow\"))\n\np<-ggplotly(p)\n\np\n\n\n\n\nCompany refers to a UK-registered company which is incorporated in the UK and carries on business in the UK. Figures given for 2022 so far.\n\n\n\n\n\nFeel free to hover over the names and have a google.\nOkay so that is a brief look at political donations to the main parties in 2022 so far. Do tweet with any feedback or suggestions."
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#donation-time-series",
    "href": "posts/TrussDodgyNetwork/index.html#donation-time-series",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Donation Time Series",
    "text": "Donation Time Series\nThe main issue I‚Äôm having is attaching party classifications for donations to MPs and the parties themselves. The goal would then be to aggregate things up at the party level to see donations towards a particular party (regardless of whether it was to an MP in that party or the party itself).\nThe Electoral Commission database apparently tracks donations going back to 2001 but for MPs it seems a bit gappy? E.g. I can‚Äôt find donations to Tony Blair specifically from the early 2000s. This strikes me as odd enough to investigate further before doing anymore analysis on it. For now I will just look at donations towards the major parties themselves.\nLets start by getting all the data we can on donations to parties from the Electoral Commission. This goes back to 2001. My goal here is to make a dataset where each donation has metadata for what the party association is. This isn‚Äôt actually included consistently/easily in the data, so we‚Äôll have to make a look-up. These are basically the steps I took to clean up this year‚Äôs donation data, but I wanted to be a bit clearer here.\n\n\n\n\n\nCode\nglimpse(ts_party)\n\n\nRows: 61,070\nColumns: 7\n$ RegulatedEntityName <chr> \"Liberal Democrats\", \"Liberal Democrats\", \"Conserv‚Ä¶\n$ RegulatedEntityType <chr> \"Political Party\", \"Political Party\", \"Political P‚Ä¶\n$ DonorName           <chr> \"Kendal & South Westmoreland Liberal Club\", \"Scott‚Ä¶\n$ DonorStatus         <chr> \"Unincorporated Association\", \"Public Fund\", \"Comp‚Ä¶\n$ ReportedDate        <chr> \"28/07/2022\", \"28/07/2022\", \"28/07/2022\", \"28/07/2‚Ä¶\n$ Value               <dbl> 655.00, 1689.01, 4000.00, 1500.00, 10000.00, 10000‚Ä¶\n$ Party_Name          <chr> \"Liberal Democrats\", \"Liberal Democrats\", \"Conserv‚Ä¶\n\n\nAlso, once again I‚Äôm focusing on Con/Lab/Lib/SNP. Just in the interest of time/priorities (apologies). Also I‚Äôm going to just do things as a % of funding each year rather than convert things to real terms right now. I think the absolute value is also important. But for now I just want to get an idea of the extent of individual and company donations.\n\n\n\n\n\nOk ok fine\nTo do list:\nAttempt a time series\nAnd splitting the dataset up into donees that are individuals and organisations.\nhttps://www.parliament.uk/site-information/foi/transparency-publications/hoc-transparency-publications/financial-information/financial-assistance-to-opposition-parties/\nPUT MP REGISTER DATE\nA few things to note.\nSo there you have it. Much like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet.\nAdd a bit about Andrew Law and Kwarteng\nThe break between markets and hedge funds\nNote somewhere about how you‚Äôve taken non-cash values\nhttps://www.theguardian.com/politics/2022/oct/05/liz-truss-raised-500000-for-bid-to-be-leader-register-of-interests-reveals UPDATE THE QUANTUMS\nMENTION HOW YOU ADDED CASH AND NON CASH"
  },
  {
    "objectID": "posts/TrussDodgyNetwork/index.html#donation-time-series-maybe-next-time",
    "href": "posts/TrussDodgyNetwork/index.html#donation-time-series-maybe-next-time",
    "title": "Visualising Truss‚Äôs Dodgy Donor Network (and a Brief Look at UK Political Donations)",
    "section": "Donation Time Series? Maybe Next Time",
    "text": "Donation Time Series? Maybe Next Time\nI wanted to close by looking at the trend of political donations over time broken down by donor status. Unfortunately this is not exactly straightforward and I might revisit this another time.\nThe main issue I‚Äôm having is attaching party classifications for donations to MPs and the parties themselves. The goal would then be to aggregate things up at the party level to see donations towards a particular party (regardless of whether it was to an MP in that party or the party itself).\nAlso, another issue - the Electoral Commission database apparently tracks donations going back to 2001 but for MPs it seems a bit gappy? E.g. I can‚Äôt find donations to Tony Blair specifically from the early 2000s. This strikes me as odd enough to investigate further before doing anymore analysis on it.\nAlso, I‚Äôll probably do some extra stuff like deflating the donation amounts and making sure to pick out where in the time series the Conservative Party are no longer eligible for Short Money etc.\nSo there you have it. I really wanted to close this piece with a fun quip like:\n\n‚ÄúMuch like an actual dodgy doner, Truss and her dodgy donors will poison this country - leaving it sick, weak, and forever in the toilet.‚Äù\n\nBut its just sad. These people are maintaining a pitifully low rate of corporation tax, removing the cap for bankers bonuses, subsidising energy companies, and more, whilst at the same reducing in work benefits for those on Universal Credit.\nAs Daniela Gabor has made clear, this is class war."
  },
  {
    "objectID": "posts/blackhole/index.html",
    "href": "posts/blackhole/index.html",
    "title": "Make your very own fiscal black hole! üßë‚Äçüç≥",
    "section": "",
    "text": "Today at Rogue Analysis we‚Äôll be looking at how to make your very own fiscal black hole! Usually this is a bit tricky to make at home because not everybody has access to a machine capable of spinning economic matters into incredibly dense journalism that no amount of nuance can escape from. Lets get to it! üßë‚Äçüç≥"
  },
  {
    "objectID": "posts/blackhole/index.html#ok-but-for-real",
    "href": "posts/blackhole/index.html#ok-but-for-real",
    "title": "Make your very own fiscal black hole! üßë‚Äçüç≥",
    "section": "Ok but for real",
    "text": "Ok but for real"
  },
  {
    "objectID": "posts/blackhole/index.html#second-attempt",
    "href": "posts/blackhole/index.html#second-attempt",
    "title": "Make Your Own Fiscal Black Hole üßë‚Äçüç≥",
    "section": "Second attempt",
    "text": "Second attempt\n\n\nCode\nimport {regl} from \"@bmschmidt/animating-log-spirals-as-webgl-points\";\nimport {glsl} from \"@stwind/glsl-chunk-tag\";\nimport {slider, radio, select} from '@jashkenas/inputs'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwrapREGL = require('regl');\naspect_ratio = 1;\na = 0.0079;\nk = 1.3302036732051;\nr = 0.8;\nperiod = 0.22;\npoint_size = 2;\nrandom_rotation = 0.16;\nrandom_angle = 0.006;\nrandom_radius = 0.0669;\ndonut_size = 0.01;\nacceleration_factor = 1;\nshear = 0.01;\nn_spirals = 2;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nviewof regl = {\n  let div = DOM.canvas(width, width / aspect_ratio);\n  let regl = wrapREGL(div);\n  div.value = regl;\n  return div;\n}\n\n\n\n\n\n\n\nWhen you‚Äôre cooking up a fiscal black hole,\n\n‚ÄúIt is not this year's budget deficit, nor the level of public debt. Gemma Tetlow, chief economist at the Institute for Government, another think-tank, said that at its most basic, the fiscal hole represented \"the gap [in the public finances] between where we are projected to be and where we want to be\".‚Äù\n‚ÄúThat means the hole depends on the chancellor's own definition of sustainable public finances.‚Äù\nhttps://www.ft.com/content/1772624e-735f-4de8-8202-65fc2334444f\nPlan:\n\nBreak down parameters for current calculation of black hole\nMake sure you can derive current estimates\nFigure out how to put them in parameters"
  },
  {
    "objectID": "posts/blackhole/index.html#ingredients",
    "href": "posts/blackhole/index.html#ingredients",
    "title": "Make your very own fiscal black hole! üßë‚Äçüç≥",
    "section": "Ingredients",
    "text": "Ingredients\nRight. Ingredients. What do we need? Well for starters, we need some reason that there‚Äôs a black hole to begin with. In this case, these are fiscal rules. Here are the latest set in season:\n\nFiscal policy decisions will be guided by updated fiscal rules, which require public sector net debt (excluding the Bank of England) to be falling as a percentage of GDP and public sector net borrowing to be below 3% of GDP by the fifth year of the rolling forecast.\n\nSource: Policy paper Autumn Statement 2022 HTML\nIf we miss the fiscal rules, we get the black hole - bish bash bosh. The government have changed the fiscal rules a few times in the past decade without really providing a clear justification for what they‚Äôre targeting and why. But hey lets not think about that too much! RULES ARE RULES üò§ (even if the government comes up with them and then‚Ä¶gets rid of them‚Ä¶and then claims the new ones should bind policy‚Ä¶and then‚Ä¶never mind). More cooking, less thinking.\nNow you might ask why does public sector net borrowing have to be below 3% of GDP and falling by the fifth year of the forecast?\nWell are you NOT COMMITTED TO THE RESPONSIBLE MANAGEMENT OF PUBLIC FINANCES? üò§\n‚Ä¶Oh you are? You‚Äôre just wondering where the figures come from, whether this is necessary right now, concerned about funding public services properly, and having the flexibility to respond to current macroeconomic conditions?\nWell DO YOU NOT CARE ABOUT ANCHORING MARKET EXPECTATIONS? THEY‚ÄôLL EAT US ALIVE. NO RULES? THE BAILIFFS WILL COME AND TAKE THE GOVERNMENT‚ÄôS SOFA.\n\n\n\nDodgy meme made by me, google images, and photopea.com\n\n\nDID YOU NOT SEE WHAT HAPPENED WITH KWARTENG‚ÄôS BUDGET? (which absolutely was not driven by a range of factors including uncertainty about unfunded cuts, the state of public services, and the inflationary environment). DO YOU WANT THAT? üò§\n‚Ä¶Oh you weren‚Äôt saying no rules? And its probably helpful to communicate clearly to the markets? And the markets don‚Äôt care about your expectations? And the government in fact underlies the system of markets, is the only entity powerful enough to control them, and the illusion that the government are somehow impotent against the markets is an ideological fantasy designed to entrench the view that there is no alternative?\n‚Ä¶Okay look. Just run with it. Also a bunch of other countries are doing this whole fiscal black hole thing and ratcheting up their fiscal rules. Apparently‚Ä¶my friend told me‚Ä¶my friend‚Äôs friend‚Ä¶called Heremy Junt.\nJust‚Ä¶don‚Äôt ask too many questions. Stop trying to be clever. You‚Äôll spoil this cooking blog for everyone.\nWhat else what else. Okay we need a few more ingredients for the fiscal black hole. These are:\n\nThe effective interest rate on the national debt\nThe GDP growth rate\nAnd the crucial special ingredient - the density of the media coverage üôÉ\n\nNow this won‚Äôt be the full thing because the Rogue Analysis cooking blog budget doesn‚Äôt stretch to incorporating the dynamics of multi-year trends, stock-flow adjustments, and what have you. Sadly we also lack the budget for numbers attached to the black hole. Because citing a number without any context usually really helps, right? Apologies üòû. Even so, we can make something resembling the fiscal black hole with these three ingredients and the fiscal rules!"
  },
  {
    "objectID": "posts/blackhole/index.html#okay-but-for-real",
    "href": "posts/blackhole/index.html#okay-but-for-real",
    "title": "Make your very own fiscal black hole! üßë‚Äçüç≥",
    "section": "Okay but for real",
    "text": "Okay but for real\n\n\n\n\n\n\nWARNING\n\n\n\nSatire ends here üôÉ\n\n\nThe fiscal black hole discourse was/is terrible, and the Autumn Statement whilst bad, could have been a lot worse. Here are a few sources that helped me follow what was going on:\n\nCalvert Jump, Rob and Michell, Jo (2022), The Dangerous Fiction of the ‚ÄòFiscal Black Hole‚Äô, London: Progressive Economy Forum - also covered in this BBC article\nKarl Whelan - On the UK‚Äôs Fiscal Black Hole\nSimon Wren-Lewis - Missing a fiscal rule does not make a black hole, and a financial crisis after tax cuts does not mean markets want spending cuts\nAlso recommend following Arun Advani, Carys Roberts, Frances Coppola, George Dibb, and James Meadway on twitter (as long as its about)/mastodon respectively. Its been interesting to see discussions unfold on those networks.\n\nThis blog post was fueled by spite for this piece in the financial times. You might be forgiven for thinking it was satire‚Ä¶"
  },
  {
    "objectID": "posts/blackhole/index.html#method",
    "href": "posts/blackhole/index.html#method",
    "title": "Make your very own fiscal black hole! üßë‚Äçüç≥",
    "section": "Method",
    "text": "Method\nAh. There‚Äôs a bit more trouble. I forget what the amount of each ingredient is meant to be. The chefs in the government are famous for knowing what the exact proportions are and forecasting with absolute certainty. I guess you‚Äôll just have to eyeball it?\nHere‚Äôs one I made earlier1:\n\n\nCode\nimport {slider} from \"@jashkenas/inputs\";\nimport {checkbox} from \"@jashkenas/inputs\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nforeground = \"#446e9b\"; //\"#000000\";\nthickness = 6;\nskew_y = 0;\noffset_y = -0.13;\nscale_x = 0.73;\nscale_y = 0.04;\nlines = 14;\narcs = 32;\nsegments = 12;\nrough_proj = (1+int_rate)/(1+growth_rate)\ncanvas_w = 10000 * Math.log(rough_proj)^2 //canvas_size  ;\ncanvas_h = 10000 * Math.log(rough_proj)^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nviewof ch1 = checkbox({\n  title: \"Do you want to accept the government's fiscal rules?\",\n  options: [\n    { value: \"yes\", label: \"Yes\" },\n    { value: \"also yes\", label: \"Also Yes\" }\n  ],\n  value: [\"yes\"]\n})\n\n\n\n\n\n\n\n\n\nCode\nviewof int_rate = slider({\n  min: 0.002,\n  max: 0.08,\n  step: 0.0001,\n  value: 0.01,\n  format: \".00%\",\n  title: \"What do you reckon the effective interest rate on the national debt will be?\"\n})\n\n\n\n\n\n\n\n\n\nCode\nviewof growth_rate = slider({\n  min: -0.04,\n  max: 0.08,\n  step: 0.0001,\n  value: -0.02,\n  format: \".00%\",\n  title: \"What do you reckon GDP growth rate will be?\"\n})\n\n\n\n\n\n\n\n\n\nCode\nviewof scale_y_inv = slider({\n  min: 0.2,\n  max: 2,\n  value: .95,\n  title: \"Density of Media Coverage\"\n})\n\n\n\n\n\n\n\n\n\nCode\n// https://observablehq.com/@tomlarkworthy/wormhole - credits to Tom Larkworthy\n\nviewof animated_wormhole = {\n  var ctx = DOM.context2d(canvas_w, canvas_h);\n  // We flush fill the foreground color so that it fades into the natural minimum\n  // of the foreground will\n  ctx.lineCap = \"round\"; \n  \n  var seg_scale_z = 0.1;\n  var lineSegStep = 0.1;\n  var thetaOffset = 0.5;\n  \n  function unitToScreen(unitCoords) {\n    return [0.5 * canvas_w + 0.5 * canvas_w * unitCoords[0],\n            0.5 * canvas_h + 0.5 * canvas_h * unitCoords[1]];\n  }\n  \n  function quadratic(x, a, b, c) {\n    return x * a * a + x * b + c;\n  }\n  \n  function arcPoint(arc, segment) {\n    return [segment * scale_x * seg_scale_z * Math.cos(2 * arc * Math.PI),\n            offset_y + scale_y_inv / (segment * segment + 1) + quadratic(segment, 0, scale_y, skew_y) * Math.sin(2 * arc * Math.PI)];\n  }\n  \n  function segment_f(segment) {\n    return (1 + segment) - (Date.now() % 1000) / 1000;\n  }\n  \n  function line_w(segment) {\n    var unit = segment_f(segment) / segments;\n    return Math.max((unit + Math.min(0, - 2 * unit + 1)) * thickness, 0.1);\n  }\n  \n  while (true) {\n    ctx.globalCompositeOperation = \"destination-out\";\n    ctx.fillStyle = \"black\";\n    ctx.fillRect(0,0, canvas_w, canvas_h);\n    ctx.globalCompositeOperation = \"source-over\";\n    \n    // Rings\n    for (var segment = 0; segment < segments; segment++) {\n      ctx.lineWidth = line_w(segment);\n      ctx.strokeStyle = foreground;\n      ctx.beginPath();\n      \n      for (var arc = thetaOffset; arc <= arcs + thetaOffset; arc++) {\n        var arc_f = arc / arcs;\n        var arc_pos = unitToScreen(arcPoint(arc_f, segment_f(segment)));\n        ctx.lineTo.apply(ctx, arc_pos);\n      }\n      ctx.stroke();\n    }\n    // Inward lines\n    for (var line = thetaOffset; line < lines; line++) {\n      ctx.strokeStyle = foreground;\n      ctx.beginPath();\n      var line_f = line / lines;\n      for (var segment = -3; segment < segments-1; segment+= lineSegStep) {\n        ctx.beginPath();\n        ctx.moveTo.apply(ctx, unitToScreen(arcPoint(line_f, segment_f(segment - lineSegStep))));\n        var arc_pos = unitToScreen(arcPoint(line_f, segment_f(segment)));\n        ctx.lineTo.apply(ctx, arc_pos);\n        ctx.lineWidth = line_w(segment);\n        ctx.stroke();\n      }\n    }\n    \n    yield ctx.canvas\n  }  \n}\n\n\n\n\n\n\n\nSo there you have it. A fiscal black hole. Make sure to garnish with spending cuts and tax increases. Don‚Äôt worry too much about the distribution of the tax increases - it adds spice! Bon app√©tit.\nTune in next time, where we‚Äôll be making an Eton mess of the economy!"
  },
  {
    "objectID": "tea.html",
    "href": "tea.html",
    "title": "Tea ‚òï",
    "section": "",
    "text": "Woah you actually clicked on this? Here‚Äôs a masala chai recipe for your troubles:\nIngredients:\n\n10 Green Cardamom Pods\n12-15 Black Peppercorns\n2 Bayleaves\n1 Large Cinnamon Stick\n6-7 Cloves\n3 Mugs Water\n1 thumb size piece of Fresh Ginger\n3 Tea Bags\n2 tsp Sugar\n1/3-1/2 Mug Whole Milk\n2/3 Mug Evaporated Milk (remember that‚Äôs EVAPORATED NOT CONDENSED MILK‚Ä¶but I‚Äôve tried with condensed and it was good too)"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html",
    "title": "Academic casualisation across the UK",
    "section": "",
    "text": "Last week LSE UCU published a report called ‚ÄúThe Crisis of Academic Casualisation at LSE‚Äù. I would recommend taking a look. They summarise their main findings in a twitter thread too. It is a damning indictment of LSE and its purported aims as a so called higher education institution. Take a look at one of the highlights:\nIn truth, it makes you ashamed to be associated with such a racket. LSE have created precarity for staff. LSE increase the intake of domestic and international students and gladly take their money. LSE MSc courses are ridiculously priced and sometimes very similar courses have massive price differentials solely due to names. LSE have money to spend on buildings named after benefactors but they can‚Äôt help build the careers of fixed-term staff. Don‚Äôt get me started on the fact that they named a building after Paul Marshall (yes the GB news investor and leave campaign funder). Recently, LSE unilaterally disaffiliated from Stonewall without any prior consultation or decent explanation to students and staff. LSE (like many universities) is an institution that serves power and pays lip service to the interests of staff and students. There is much more to say here1 but I‚Äôll move on to the topic at hand.\nIn response to LSE UCU‚Äôs casualisation report I saw a few people saying they‚Äôd like to see similar analysis for their institution. LSE UCU also encouraged others to make use of their code to conduct similar analysis. So I thought why not try and do similar analysis for every institution? It could be useful and I like a challenge.\nI‚Äôd like to stress that all of what follows builds on analysis from Marion Lieutaud (2023) and LSE UCU members. Lieutaud drew on HESA (Higher Education Statistics Authority) data for much the analysis. The heavy of lifting of getting the data in a good format was basically already done by Lieutaud. All I‚Äôve done is taken that data and shoved it in some charts so you can (hopefully) see every institution."
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#the-russell-group-or-the-hustle-group",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#the-russell-group-or-the-hustle-group",
    "title": "Academic casualisation across the UK",
    "section": "The Russell Group or the hustle group?",
    "text": "The Russell Group or the hustle group?\nSo lets take a look at all the data Lieutaud put together. Specifically, what is the distribution of fixed term contracts like across all institutions since 2014-15? And how does the Russell Group3 look? Well it looks like this:\n\n\nThe Russell Group tend to have a higher proportion of staff on fixed term contracts\n\n\n% of staff that are fixed term - broken down by year and institution. Hover to see institution names.\n\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\";\n\nboxdata = transpose(boxdata_chart_convert);\n\nPlot.plot({\n  height: 700, //6 or 700?\n  marginBottom: 60,\n  \n  x: {\n    label: \"% of Staff that are Fixed-Term‚Üí\",\n    //domain: [0, 100],\n    grid: true,\n    nice: true,\n    ticks: 5,\n    labelOffset: 55\n  },\n  \n  y: {\n    label: null,\n    grid: true\n  },\n  \n  facet: {data: boxdata, y: \"date\", marginLeft: 175},\n  \n  fy: {label: null},\n  \n  marks: [\n    \n    Plot.boxX(boxdata, {x: \"prop_casual\", fillOpacity: 0.4}),\n  \n    Plot.dot(boxdata, \n    \n    Plot.dodgeY({\n      x: \"prop_casual\", \n      fill: \"rg_boxplot_flag\", \n      anchor: \"middle\", \n      r: 3.5, //7\n      fillOpacity: 0.8,\n      padding: 0.5,\n      title: (d) =>`${d.HE_provider_short} \\n Score: ${d.prop_casual}`\n      }))\n      \n  ],\n  \n  color: {\n  legend: false,\n  range: [\"#adb5bd\",\"#de425b\"],\n  ordinal: true,\n  style: {\n  fontSize: \"1em\"\n  }\n  },\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor visibility, excludes institutions reporting 0. These tend to be smaller specialised institutions.Excluded large outlier Court Theatre Training Company Ltd which reported 100% in last few years. Source: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà\n\nWhilst the median isn‚Äôt shifting much every year, you can spot some patterns. Russell Group universities tend to have a higher proportion of staff on fixed term contracts. They are leading universities‚Ä¶for casualisation. Big L for the Russell Group and its stated aim to (*checks google):\n\nhelp ensure that our universities have the optimum conditions in which to flourish and continue to make social, economic and cultural impacts through their world-leading research and teaching.\n\nYou can hover over the dots above to see institution names."
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#number-of-permanent-and-fixed-term-academic-staff",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#number-of-permanent-and-fixed-term-academic-staff",
    "title": "Academic casualisation across the UK",
    "section": "Number of permanent and fixed-term academic staff",
    "text": "Number of permanent and fixed-term academic staff\nNow lets actually move on to replicating stuff from the report in an interactive way. Try out the dropdown below and see what fixed-term and permanent staff trends look like at your institution (I am sorry I couldn‚Äôt make the cool animated chart that Lieutaud made).\n\n\nNumber of fixed term and permanent academic staff at:\n\n\nviewof HE = Inputs.select(he_names_convert.HE_provider_short, {value: \"LSE\"})\n\n\n\n\n\n\n\nmydata = transpose(data_casual_numbers_convert)\n.filter(d => d.HE_provider_short == HE);\n\nPlot.plot({\n  \n  marginTop:35,\n  marginLeft: 70,\n  marginBottom: 95,\n  marginRight: 70,\n\n  x: {\n    label: null,\n    nice: true,\n    grid: true,\n    tickPadding: 10,\n    tickRotate: -45\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"‚Üë Number of Staff\",\n    grid: true,\n    nice: true,\n    ticks: 5\n  },\n  \n  marks: [\n\n    Plot.line(mydata, {\n      x: \"date\", \n      y: \"value\", \n      z: \"contract.type\",\n      strokeWidth: 4,\n      stroke: \"contract.type\"\n    }),\n    \n    Plot.dot(mydata, {\n      x: \"date\", \n      y: \"value\", \n      z: \"contract.type\",\n      strokeWidth: 4,\n      stroke: \"contract.type\"\n    }),\n    \n    Plot.text(mydata, Plot.selectLast({\n      x: \"date\",\n      y: \"value\",\n      z: \"contract.type\",\n      text: \"contract.type\",\n      textAnchor: \"start\",\n      dx: 10\n    }))\n    \n  ],\n  \n  color: {\n  legend: true,\n  type: \"ordinal\",\n  range: [\"#de425b\",\"#446e9b\"],\n  width: 300,\n  swatchWidth: 30,\n  style: {\n  fontSize: \"1em\"\n  }\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#proportion-of-casual-fixed-term-staff-amongst-academic-staff",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#proportion-of-casual-fixed-term-staff-amongst-academic-staff",
    "title": "Academic casualisation across the UK",
    "section": "Proportion of casual (fixed-term) staff amongst academic staff",
    "text": "Proportion of casual (fixed-term) staff amongst academic staff\nNext lets take a look at the proportion of fixed-term staff amongst academic staff. I am curious as to whats happening in UCL and Glasgow. I did hear something about the way UCL classify permanent jobs being shady (I linked a tweet in the caveats above). But I‚Äôm not really sure. Do shout if you know! Also worth noting that the Russell Group average is somewhat flat and Oxford‚Äôs proportion of fixed-term staff is mad. I think only the Open University is ahead (but there was some big progress last year).\nRemember to use the dropdown to check out other places.\n\n\nProportion (%) of fixed term staff among academic staff at:\n\n\nviewof HE1 = Inputs.select(he_names_convert.HE_provider_short, {value: \"LSE\"})\n\n\n\n\n\n\n\n\nmydata1 = transpose(prop_casual_chart_convert)\n.filter(d => d.bg == 1);\n\nmydata1_filter = transpose(prop_casual_chart_convert)\n.filter(d => d.HE_provider_short == HE1);\n\n// plot\n\nPlot.plot({\n\n  marginTop:50,\n  marginRight: 100,\n  marginLeft: 50,\n  marginBottom: 95,\n  \n  x: {\n    label: null,\n    nice: true,\n    grid: false,\n    tickPadding: 10,\n    tickRotate: -45\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"‚Üë % of Staff\",\n    grid: true,\n    nice: true,\n    ticks: 5\n  },\n  \n  marks: [\n  \n    Plot.areaY(mydata1, Plot.windowY({\n      x: \"date\", \n      y1: \"low\", \n      y2: \"high\",\n      fill: \"#dedede\",\n      fillOpacity: 0.7,\n      k: 1 \n    })),\n    \n    Plot.line(mydata1, {\n      x: \"date\", \n      y: \"prop_casual\", \n      z: \"HE_provider\",\n      stroke: \"#ccc\",\n      strokeWidth: 3,\n    }),\n    \n    Plot.line(mydata1, {\n      x: \"date\",\n      y: \"rg_stat\",\n      stroke: \"black\",\n      strokeDasharray: \"5,5\",\n      strokeWidth:3\n    }),\n    \n    Plot.line(mydata1_filter, {\n      x: \"date\", \n      y: \"prop_casual\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    }),\n    \n    Plot.dot(mydata1_filter, {\n      x: \"date\", \n      y: \"prop_casual\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    }),\n    \n    Plot.text(mydata1, Plot.selectLast({\n      x: \"date\",\n      y: \"prop_casual\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 15\n    })),\n    \n    \n    Plot.text(mydata1_filter, Plot.selectLast({\n      x: \"date\",\n      y: \"prop_casual\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 15\n    })),\n    \n    Plot.text(mydata1, Plot.selectLast({\n      x: \"date\",\n      y: \"rg_stat\",\n      text: \"rg_lab\",\n      textAnchor: \"start\",\n      dx: -280,\n      dy: 20\n    }))\n    \n  ],\n  \n  color: {\n  legend: false\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#relative-changes-in-share-of-permanent-academic-staff-numbers-since-2014-2015",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#relative-changes-in-share-of-permanent-academic-staff-numbers-since-2014-2015",
    "title": "Academic casualisation across the UK",
    "section": "Relative changes in share of permanent academic staff numbers since 2014-2015",
    "text": "Relative changes in share of permanent academic staff numbers since 2014-2015\nOk this is a slightly different chart to the one that is in the report. The report had ‚Äúrelative changes in the number of permanent academic staff since 2014-15‚Äù. I‚Äôve done ‚Äúchanges in the share of permanent academic staff‚Äù. This makes % changes relative to the share of permanent academic staff, and brings out trends in staff composition a bit more. Shout out to Peter Wyckoff for suggesting!\nOnce again, do use the dropdown to check out other places!\n\n\nProportionate changes (%) in share of permanent academic staff at:\n\n\nviewof HE2 = Inputs.select(he_names_convert.HE_provider_short, {value: \"LSE\"})\n\n\n\n\n\n\n\n\nmydata2 = transpose(prop_change_perm_convert)\n.filter(d => d.bg == 1);\n\nmydata2_filter = transpose(prop_change_perm_convert)\n.filter(d => d.HE_provider_short == HE2);\n\n// plot\n\nPlot.plot({\n  \n  marginTop:30,\n  marginRight: 100,\n  marginLeft: 50,\n  marginBottom: 95,\n  \n  x: {\n    label: null,\n    nice: true,\n    grid: false,\n    tickPadding: 10,\n    tickRotate: -45\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"Baseline (2014/15) = 0\",\n    grid: true,\n    nice: true,\n    ticks: 5,\n    tickFormat: (f => x => f((x - 1) * 100))(d3.format(\"+d\"))\n  },\n  \n  marks: [\n    \n    Plot.line(mydata2,\n    Plot.normalizeY({\n      x: \"date\", \n      y: \"prop_permanent\", \n      z: \"HE_provider\",\n      stroke: \"#ccc\",\n      strokeWidth: 3,\n    })),\n    \n    Plot.line(mydata2, \n    Plot.normalizeY({\n      x: \"date\",\n      y: \"rg_stat\",\n      stroke: \"black\",\n      strokeDasharray: \"5,5\",\n      strokeWidth:3\n    })),\n    \n    Plot.line(mydata2_filter,\n      Plot.normalizeY({\n      x: \"date\", \n      y: \"prop_permanent\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    })),\n    \n    Plot.dot(mydata2_filter,\n      Plot.normalizeY({\n      x: \"date\", \n      y: \"prop_permanent\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    })),\n    \n    Plot.text(mydata2, \n    Plot.selectLast(\n    Plot.normalizeY({\n      x: \"date\",\n      y: \"prop_permanent\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 10\n    }))),\n    \n    Plot.text(mydata2_filter, \n    Plot.selectLast(\n    Plot.normalizeY({\n      x: \"date\",\n      y: \"prop_permanent\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 10\n      //dy: -15\n    }))),\n    \n    Plot.text(mydata2, \n    Plot.selectLast(\n    Plot.normalizeY({\n      x: \"date\",\n      y: \"rg_stat\",\n      text: \"rg_lab\",\n      textAnchor: \"start\",\n      dx: -220,\n      dy: 20\n    })))\n    \n  ],\n  \n  color: {\n  legend: false\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#student-to-staff-ratio-permanent-academic-staff-only-since-2014-2015",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#student-to-staff-ratio-permanent-academic-staff-only-since-2014-2015",
    "title": "Academic casualisation across the UK",
    "section": "Student-to-staff ratio (permanent academic staff only) since 2014-2015",
    "text": "Student-to-staff ratio (permanent academic staff only) since 2014-2015\nThis is the student to permanent staff ratio (I‚Äôm going to be a bit light on the write-up for the rest of this post because I want to get this out and I have some other work to do - apologies).\n\n\nStudent to permanent staff ratio at:\n\n\nviewof HE3 = Inputs.select(he_names_convert.HE_provider_short, {value: \"LSE\"})\n\n\n\n\n\n\n\n\nmydata3 = transpose(staff_student_chart_convert)\n.filter(d => d.bg == 1);\n\nmydata3_filter = transpose(staff_student_chart_convert)\n.filter(d => d.HE_provider_short == HE3);\n\n// plot\n\nPlot.plot({\n  \n  marginTop:50,\n  marginRight: 100,\n  marginLeft: 50,\n  marginBottom: 95,\n  \n  x: {\n    label: null,\n    nice: true,\n    grid: false,\n    tickPadding: 10,\n    tickRotate: -45\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"‚Üë Students to perm staff ratio\",\n    grid: true,\n    nice: true,\n    ticks: 5\n  },\n  \n  marks: [\n    \n    Plot.areaY(mydata3, Plot.windowY({\n      x: \"date\", \n      y1: \"low\", \n      y2: \"high\",\n      fill: \"#dedede\",\n      fillOpacity: 0.7,\n      k: 1 \n    })),\n    \n    Plot.line(mydata3, {\n      x: \"date\", \n      y: \"staffstudent_ratio\", \n      z: \"HE_provider\",\n      stroke: \"#ccc\",\n      strokeWidth: 3,\n    }),\n    \n    Plot.line(mydata3, {\n      x: \"date\",\n      y: \"rg_stat\",\n      stroke: \"black\",\n      strokeDasharray: \"5,5\",\n      strokeWidth:3\n    }),\n    \n    Plot.line(mydata3_filter, {\n      x: \"date\", \n      y: \"staffstudent_ratio\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    }),\n    \n    Plot.dot(mydata3_filter, {\n      x: \"date\", \n      y: \"staffstudent_ratio\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    }),\n    \n    Plot.text(mydata3_filter, Plot.selectLast({\n      x: \"date\",\n      y: \"staffstudent_ratio\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 15\n    })),\n    \n    Plot.text(mydata3, Plot.selectLast({\n      x: \"date\",\n      y: \"staffstudent_ratio\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 15\n    })),\n    \n    Plot.text(mydata3, Plot.selectLast({\n      x: \"date\",\n      y: \"rg_stat\",\n      text: \"rg_lab\",\n      textAnchor: \"start\",\n      dx: -270,\n      dy: -10\n    }))\n    \n  ],\n  \n  color: {\n  legend: false\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#relative-changes-in-student-numbers-since-2014-2015",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#relative-changes-in-student-numbers-since-2014-2015",
    "title": "Academic casualisation across the UK",
    "section": "Relative changes in student numbers since 2014-2015",
    "text": "Relative changes in student numbers since 2014-2015\nNext we have relative changes in student numbers. My main gripe with all this data is how it doesn‚Äôt go back before 2014-15. There is more to be understood here. But anyway, we know many places like LSE are increasing student numbers, have high levels of international students, charge them mad fees, and then this doesn‚Äôt translate into solid contracts and good working conditions? Outstanding move to exploit students and staff at the same time.\n\n\nProportionate changes (%) in number of students at:\n\n\nviewof HE4 = Inputs.select(he_names_convert.HE_provider_short, {value: \"LSE\"})\n\n\n\n\n\n\n\n\nmydata4 = transpose(student_nb_chart_convert)\n.filter(d => d.bg == 1);\n\nmydata4_filter = transpose(student_nb_chart_convert)\n.filter(d => d.HE_provider_short == HE4);\n\n// plot\n\nPlot.plot({\n  \n  marginTop:30,\n  marginRight: 100,\n  marginLeft: 50,\n  marginBottom: 95,\n  \n  x: {\n    label: null,\n    nice: true,\n    grid: false,\n    tickPadding: 10,\n    tickRotate: -45\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"Baseline (2014/15) = 0\",\n    grid: true,\n    nice: true,\n    ticks: 5,\n    tickFormat: (f => x => f((x - 1) * 100))(d3.format(\"+d\"))\n  },\n  \n  marks: [\n    \n    Plot.line(mydata4,\n    Plot.normalizeY({\n      x: \"date\", \n      y: \"Total\", \n      z: \"HE_provider\",\n      stroke: \"#ccc\",\n      strokeWidth: 3,\n    })),\n    \n    Plot.line(mydata4, \n      Plot.normalizeY({\n      x: \"date\",\n      y: \"rg_stat\",\n      strokeDasharray: \"5,5\",\n      stroke: \"black\",\n      strokeWidth:3\n    })),\n    \n    Plot.line(mydata4_filter,\n      Plot.normalizeY({\n      x: \"date\", \n      y: \"Total\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    })),\n    \n    Plot.dot(mydata4_filter,\n      Plot.normalizeY({\n      x: \"date\", \n      y: \"Total\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    })),\n    \n    Plot.text(mydata4, \n    Plot.selectLast(\n    Plot.normalizeY({\n      x: \"date\",\n      y: \"Total\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 10\n    }))),\n    \n    Plot.text(mydata4_filter, \n    Plot.selectLast(\n    Plot.normalizeY({\n      x: \"date\",\n      y: \"Total\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 10\n      //dy: -15\n    }))),\n    \n    Plot.text(mydata4, \n    Plot.selectLast(\n    Plot.normalizeY({\n      x: \"date\",\n      y: \"rg_stat\",\n      text: \"rg_lab\",\n      textAnchor: \"start\",\n      dx: -495,\n      dy: 50\n    })))\n    \n  ],\n  \n  color: {\n  legend: false\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#proportion-of-international-students-among-all-enrolled-students-over-time-since-2014-2015",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#proportion-of-international-students-among-all-enrolled-students-over-time-since-2014-2015",
    "title": "Academic casualisation across the UK",
    "section": "Proportion of international students among all enrolled students, over time since 2014-2015",
    "text": "Proportion of international students among all enrolled students, over time since 2014-2015\nFinally here is the proportion of international students across universities. Please try out at least one of the dropdowns. I tried quite hard to make them all work and it would make me happy.\n\n\nProportion of international students at:\n\n\nviewof HE5 = Inputs.select(he_names_convert.HE_provider_short, {value: \"LSE\"})\n\n\n\n\n\n\n\n\nmydata5 = transpose(prop_non_uk_student_convert)\n.filter(d => d.bg == 1);\n\nmydata5_filter = transpose(prop_non_uk_student_convert)\n.filter(d => d.HE_provider_short == HE5);\n\n// plot\n\nPlot.plot({\n  \n  marginTop:50,\n  marginRight: 100,\n  marginLeft: 50,\n  marginBottom: 95,\n  \n  x: {\n    label: null,\n    nice: true,\n    grid: false,\n    tickPadding: 10,\n    tickRotate: -45\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"‚Üë % Non uk students\",\n    grid: true,\n    nice: true,\n    ticks: 5\n  },\n  \n  marks: [\n    \n    Plot.areaY(mydata5, Plot.windowY({\n      x: \"date\", \n      y1: \"low\", \n      y2: \"high\",\n      fill: \"#dedede\",\n      fillOpacity: 0.7,\n      k: 1 \n    })),\n    \n    Plot.line(mydata5, {\n      x: \"date\", \n      y: \"prop_non_uk\", \n      z: \"HE_provider\",\n      stroke: \"#ccc\",\n      strokeWidth: 3,\n    }),\n    \n    Plot.line(mydata5, {\n      x: \"date\",\n      y: \"rg_stat\",\n      stroke: \"black\",\n      strokeDasharray: \"5,5\",\n      strokeWidth:3\n    }),\n    \n    Plot.line(mydata5_filter, {\n      x: \"date\", \n      y: \"prop_non_uk\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    }),\n    \n    Plot.dot(mydata5_filter, {\n      x: \"date\", \n      y: \"prop_non_uk\", \n      z: \"HE_provider\",\n      strokeWidth: 3,\n      stroke: \"#de425b\"\n    }),\n    \n    Plot.text(mydata5_filter, Plot.selectLast({\n      x: \"date\",\n      y: \"prop_non_uk\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 15\n    })),\n    \n    Plot.text(mydata5, Plot.selectLast({\n      x: \"date\",\n      y: \"prop_non_uk\",\n      z: \"HE_provider\",\n      text: \"HE_provider_short\",\n      textAnchor: \"start\",\n      dx: 15\n    })),\n    \n    Plot.text(mydata5, Plot.selectLast({\n      x: \"date\",\n      y: \"rg_stat\",\n      text: \"rg_lab\",\n      textAnchor: \"start\",\n      dx: -280,\n      dy: 28\n    }))\n    \n  ],\n  \n  color: {\n  legend: false\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#conclusion",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#conclusion",
    "title": "Academic casualisation across the UK",
    "section": "Conclusion",
    "text": "Conclusion\nSo I rushed a bit to get this out. I might go back and redraft this post when I have more time. But I hope that was useful. A key takeaway here is that the share and trend of casualisation differs across institutions but is somewhat concentrated in the Russell Group/larger unis/larger cities (okay yes that is not very shocking). I wasn‚Äôt able to do a proper dig into the data, but I‚Äôd encourage you to take a look yourself and think about the trends. I am sure there is so much more analysis to be done.\nPlease let me know if you spot any issues or have any suggestions. Given the work I have to do, I can‚Äôt promise I‚Äôll get to it. But we‚Äôll see! Once again, do check out the LSE UCU report on casualisation. You can also take a look at my code here, for this blog post here, or you can click the button on the top right of this page to view code. I‚Äôve also saved the cleaned up dataset as a csv in this folder on github.\nI should probably close with some words of support for the union and strikes, but I hope the work above conveys my support.\nSolidarity forever."
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#an-attempt-to-view-pressures-across-the-sector",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#an-attempt-to-view-pressures-across-the-sector",
    "title": "Academic casualisation across the UK",
    "section": "An attempt to view pressures across the sector",
    "text": "An attempt to view pressures across the sector\nI added this extra chart on 10/04/2023. It is a bit dense, but I was attempting to view:\n\nThe student to permanent staff ratio\n% of fixed-term staff\nTotal number of students\nBroken down by Russell Group/wider sector\nDisaggregated for institution\n\nBetween 2014/15 - 2021/22.I thought this might help explore and explain where the pressures lie in the sector.\n\n\nStudent numbers have boomed compared to staffing capacity\n\n\n\nHover to check institution. ‚óØ size = Total number of students\n\n\n// Auditions:\n//viewof year = Inputs.select(year_convert.year, {value: \"2015\"})\n//viewof year1 = Inputs.range(d3.extent(year_convert, d => d.year), {step: 1, label: \"Year\"})\n//import {Scrubber} from \"@mbostock/scrubber\";\n//import {Scrubber} from \"@resmartizh/scrubber\"\n//viewof year_slider = Scrubber([2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022], {autoplay: false, delay: 1000})\n//import {legendRadius} from \"@recifs/a-radius-legend-for-plot-665\";\n//legendRadius(chart.scale(\"r\"), {label: \"number of students\" })\n\nviewof year_slider = Inputs.range([2015, 2022], {value: 2015, step: 1, label: \"Year\"})\n\n\n\n\n\n\n\n\nmydata_hr = transpose(hr_chart_convert)\n.filter(d => d.year == year_slider);\n\n// plot\n\nchart = Plot.plot({\n  \n  marginTop:30,\n  marginRight: 160,\n  marginBottom: 65,\n  height: 500,\n  \n  x: {\n    label: \"Student to Permanent Staff Ratio ‚Üí\",\n    nice: true,\n    domain: [0, 50],\n    grid: true,\n    tickPadding: 10\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"‚Üë % of Staff that are Fixed-Term\",\n    domain: [0, 90],\n    grid: true,\n    nice: true,\n    ticks: 5,\n    tickPadding: 10\n  },\n  \n  facet: {data: mydata_hr, y: \"rg_flag\", marginBottom: 10},\n  \n  fy: {label: null},\n  \n  marks: [\n    \n    Plot.dot(mydata_hr,{\n      x: \"staffstudent_ratio\", \n      y: \"prop_casual\",\n      r: \"total\",\n      stroke: \"rg_flag\",\n      strokeWidth: 1,\n      title: (d) =>`${d.he_provider} \\n X - Student to permanent staff ratio: ${d.staffstudent_ratio} \\n Y - % of casual: ${d.prop_casual} \\n Z/Size - Total students: ${d.total}`,\n      fill: \"rg_flag\",\n      fillOpacity: 0.3\n    })\n    \n  ],\n  \n  color: {\n  legend: false,\n  range: [\"#de425b\",\"#446e9b\"]\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcludes student staff ratios > 50 for visibility. This catches the Open University and also smaller specialised institutions.  Source: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Year formatting: 2021-2022 = 2022 Prompted by a suggestion from Bernard Keenan @psychicyogamat  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/LSEUCU_Casualisation_YIK/index.html#annex-an-attempt-to-view-pressures-across-the-sector",
    "href": "posts/LSEUCU_Casualisation_YIK/index.html#annex-an-attempt-to-view-pressures-across-the-sector",
    "title": "Academic casualisation across the UK",
    "section": "Annex: An attempt to view pressures across the sector",
    "text": "Annex: An attempt to view pressures across the sector\nI added this extra chart on 10/04/2023. It is a bit dense, but I was attempting to view pressures across the sector by looking at:\n\nThe student to permanent staff ratio (x axis)\n% of fixed-term staff (y axis)\nTotal number of students (size of ‚óØ )\nBroken down by Russell Group/wider sector\nDisaggregated for institution (individual ‚óØ )\n\nAll between 2014/15 - 2021/22. It would be great if we could go back a few more years to see the proper effect of lifting the student number cap.\nI thought this chart might help explore and explain where the pressures lie in the sector. A movement to the northeast would suggest increased pressure in the sector as the student to permanent staff ratio increases (X) and more fixed-term staff are brought in (Y).\nWhilst its dense, lets focus on the Russell Group for a second. The boom in student numbers (size of ‚óØ ) is accompanied by increases in the student to permanent staff ratio. But there is a much more dramatic pivot to fixed-term staff - especially for LSE, KCL, and QMUL.\nNext, lets focus on the wider sector. Student numbers have also boomed. The % of staff that are fixed-term has also increased, but what is more dramatic is the increase in the student to permanent staff ratio.\nSo the Russell Group can respond to the crisis of higher education by pumping up fixed term contracts (especially if domestic and international student numbers have increased), but the wider sector faces greater pressure from student numbers on permanent staff.\nSo my rough interpretation is something like, Russell Group use improved financial position, ‚Äúprestige‚Äù attraction, and cash flow from booming student numbers (domestic but especially international) to keep pressures at bay. But the rest of the sector can do this to a lesser extent and feels the direct pressure of increased student numbers hitting the student to permanent staff ratio.\nLet me know if you disagree and what you reckon! Maybe this chart was just too dense.\n\n\nStudent numbers have boomed compared to staffing capacity\n\n\n\nHover to check institution. ‚óØ size = Total number of students\n\n\nviewof year_slider = Inputs.range([2015, 2022], {value: 2015, step: 1, label: \"Year\"})\n\n\n\n\n\n\n\n\nmydata_hr = transpose(hr_chart_convert)\n.filter(d => d.year == year_slider);\n\n// plot\n\nchart = Plot.plot({\n  \n  marginTop:30,\n  marginRight: 160,\n  marginBottom: 65,\n  height: 500,\n  \n  x: {\n    label: \"Student to Permanent Staff Ratio ‚Üí\",\n    nice: true,\n    domain: [0, 50],\n    grid: true,\n    tickPadding: 10\n  },\n  \n  y: {\n    className: \"axis-lab\",\n    label: \"‚Üë % of Staff that are Fixed-Term\",\n    domain: [0, 90],\n    grid: true,\n    nice: true,\n    ticks: 5,\n    tickPadding: 10\n  },\n  \n  facet: {data: mydata_hr, y: \"rg_flag\", marginBottom: 10},\n  \n  fy: {label: null},\n  \n  marks: [\n    \n    Plot.dot(mydata_hr,{\n      x: \"staffstudent_ratio\", \n      y: \"prop_casual\",\n      r: \"total\",\n      stroke: \"rg_flag\",\n      strokeWidth: 1,\n      title: (d) =>`${d.he_provider} \\n X - Student to permanent staff ratio: ${d.staffstudent_ratio} \\n Y - % of casual: ${d.prop_casual} \\n Z/Size - Total students: ${d.total}`,\n      fill: \"rg_flag\",\n      fillOpacity: 0.3\n    }),\n    \n    Plot.arrow(mydata_hr, {\n      x1: 30,\n      y1: 40,\n      x2: 40,\n      y2: 60,\n      bend: false,\n      stroke: 3,\n      fy: [\"Russell Group\"]\n    }),\n    \n    Plot.text(mydata_hr,\n      Plot.selectFirst({\n        textAnchor: \"start\",\n        x: (d) => 35,\n        y: (d) => 68,\n        text: [`More pressure`],\n        fy: [\"Russell Group\"],\n        fontSize: 24,\n      })\n    )\n    \n  ],\n  \n  color: {\n  legend: false,\n  range: [\"#de425b\",\"#446e9b\"]\n  }\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcludes student staff ratios > 50 for visibility.  This exclusion catches the Open University and also smaller specialised institutions.  Source: M. Lieutaud (2023) LSE UCU Analysis of HESA data  Year formatting: 2021-2022 = 2022 Prompted by a suggestion from Bernard Keenan @psychicyogamat  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/paycuts/index.html",
    "href": "posts/paycuts/index.html",
    "title": "The Vices of the Vice-Chancellors",
    "section": "",
    "text": "In response to UCU‚Äôs marking and assessment boycott (MAB), universities are threatening disproportionate and unjust pay deductions. This is unnecessary. Ulster University and London Metropolitan University have publicly said they will make no deductions (for the moment). Queen Mary University of London has managed to get a commitment from senior management to not deduct in response to MAB (for now and on the basis of a prior local agreement). Hertfordshire University has managed to reduce deductions from 100% to a school by school formula. Meanwhile, 67 other universities have decided on arbitrarily large pay deductions in the midst of a cost of living crisis.\nWealthy vice chancellors stand behind this disgraceful choice. So I think it only makes sense to take a look at their salaries in light of this choice. On average, this group earn ¬£300k and most of them are deciding to deduct between 50% and 100% of salaries. If we were to cut the salaries of the median vice chancellor by 50%, this would be ¬£150k. That still leaves them comfortably in the top 2% of earners by income in the UK.\nPerhaps one might respond by saying the mechanisms that led to the decision to cut pay are more complex than blaming a single figurehead vice-chancellor. Agreed. But what is the point of these people if they are not accountable or liable for the decisions taken in the institutions they purport to run? Worse, there is an utter hypocrisy in leading an organisation where you condone pay cuts whilst sitting in the top 2% of earners.\nI‚Äôve taken the list of pay cut threats that Dr Kait Clark compiled. I then combined this with HESA data on head of provider (or vice chancellor) remuneration. Here is a glimpse of the dataset:\n\n\nCode\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(readr)\n\n# Get paycut info - https://twitter.com/DrKaitClark/status/1649038370029314052\n# I chucked this data into a csv - you can find it on my github. I'll link this. Will probably need updating as things change.\n\npaycut_threats <- read_csv(\"paycut_threats.csv\") %>% \n  clean_names() %>% \n  mutate(paycut_amount=as.character(paycut_amount))\n\n# Get VC salary data\n\nremun_2122_head <- read_csv(\"dt031-table-11_head_of_provider_remuneration.csv\", \n    skip = 17) %>% \n  clean_names() %>% \n  select(ukprn,total_remuneration_before_salary_sacrifice) %>% \n  rename(tot_remun_b4_sal_sac_21_22 = total_remuneration_before_salary_sacrifice)\n\n# And get VC salary data for previous year if data missing\n\nremun_2021_head  <- read_csv(\"dt031-table-11_head_of_provider_remuneration_PREVIOUS_YEAR.csv\", \n    skip = 17) %>% \n  clean_names() %>% \n  select(ukprn,he_provider,total_remuneration_before_salary_sacrifice) %>% \n  rename(tot_remun_b4_sal_sac_20_21 = total_remuneration_before_salary_sacrifice)\n\n# merge this year VC salary and last year VC salary\n\nremun_head_df <- remun_2021_head %>% \n  left_join(remun_2122_head,by=\"ukprn\") %>%\n  mutate(imput_tot_remun_b4_sal_sac = ifelse(is.na(tot_remun_b4_sal_sac_21_22),tot_remun_b4_sal_sac_20_21,tot_remun_b4_sal_sac_21_22)) %>% # impute with last year if missing\n  mutate(he_provider_short = str_remove_all(he_provider, \"The University of |University of |The |the | University\")) %>% \n  select(ukprn,he_provider,he_provider_short,imput_tot_remun_b4_sal_sac)\n\nremove(remun_2021_head,remun_2122_head) # drop objects we're done with\n\n# merge the VC salaries with the paycut %s\n\nremun_head_paycut_df <- paycut_threats %>% \n  \n  # sigh - do some name editing so all the merging works out\n  \n  mutate(university =case_match(university,\n                                    \"QMUL\"~\"Queen Mary of London\",\n                                    \"OU\"~\"Open\",\n                                    \"Queen Margaret\"~\"Queen Margaret, Edinburgh\",\n                                    \"SOAS\"~\"SOAS of London\",\n                                    \"UEA\"~\"East Anglia\",\n                                    \"Cardiff Met\"~\"Cardiff Metropolitan\",\n                                    \"CCCU\"~\"Canterbury Christ Church\",\n                                    \"London Met\"~\"London Metropolitan\",\n                                    \"MMU\"~\"Manchester Metropolitan\",\n                                    \"Northumbria\"~\"Northumbria at Newcastle\",\n                                    \"UCL\"~\"University College London\",\n                                    \"UOC\"~\"Chester\",\n                                    \"USW\"~\"South Wales\",\n                                    \"UWE\"~\"West of England, Bristol\",\n                                    \"UWS\"~\"West of Scotland\",\n                                    \"Birkbeck\"~\"Birkbeck College\",\n                                    \"City\"~\"City, of London\",\n                                    \"DMU\"~\"De Montfort\",\n                                    \"KCL\"~\"King's College London\",\n                                    \"LJMU\"~\"Liverpool John Moores\",\n                                    \"UCLan\"~\"Central Lancashire\",\n                                    \"UEL\"~\"East London\",\n                                     .default = university\n                                       )) %>% \n  \n  # finally merge it\n  \n  left_join(remun_head_df,by=c(\"university\"=\"he_provider_short\")) %>% \n  \n  # exclude London met and ulster who are actually not doing any paycuts!\n  \n  filter(!paycut_amount==\"0%\",\n         \n  # exclude Northumbria because their salary data is dodgy. Its like 60k one year. Then 0 year before. Then when I googled its like 250k. I'm just gonna exclude to be safe\n  \n         !university==\"Northumbria at Newcastle\") %>% \n  \n  mutate(imput_tot_remun_b4_sal_sac=ifelse(university==\"Bristol\",285,imput_tot_remun_b4_sal_sac)) %>%  # 285 for Bristol - https://www.bristol.ac.uk/university/governance/executive/vice-chancellor-and-president/\n  \n  # Filter the unis that have recently come to agreements\n  \n  filter(!university%in%c(\"Queen Mary of London\",\"Hertfordshire\"))\n\n#summary(remun_head_paycut_df) # summary stats\n\n# Ok so dodgy salary data at Northumbria aside. Drop it and caveat in the visual. Go make it.\n\nglimpse(remun_head_paycut_df)\n\n\nRows: 67\nColumns: 5\n$ university                 <chr> \"Aston\", \"Brighton\", \"Coventry\", \"Derby\", \"‚Ä¶\n$ paycut_amount              <chr> \"100%\", \"100%\", \"100%\", \"100%\", \"100%\", \"10‚Ä¶\n$ ukprn                      <dbl> 10007759, 10000886, 10001726, 10007851, 100‚Ä¶\n$ he_provider                <chr> \"Aston University\", \"The University of Brig‚Ä¶\n$ imput_tot_remun_b4_sal_sac <dbl> 176, 250, 369, 308, 303, 190, 309, 408, 235‚Ä¶\n\n\nNext, I put this into a visual comparing head of university salary to the paycut threat, You can hover over and check who is who. Do let me know if the list of pay cuts needs updating.\n\n\n\n\n\n\nCaveats\n\n\n\n\n\nHere are some caveats to my analysis:\n\nWhere vice chancellor/head of uni remuneration data is missing in HESA for this year, I have used previous years data\nIf it is still missing, I‚Äôve looked up the VC salary e.g.¬†for Bristol\nI dropped Northumbria because the HESA figure seemed incorrect in comparison to light research (60k vs a few 100k). Also, the previous year of data was empty for them, and googling the salary came up with slightly out of date results\nI took the list Dr Kait Clark compiled and updated it for developments at Hertfordshire and QMUL\n\n\n\n\n\n\n\n\n\nWealthy vice chancellors are threatening huge pay cuts in response to the boycotts (MAB). On average, this group earns ¬£300k and we are in a cost of living crisis\n\n\nHover to see institution names.\n\n\n\nCode\nviewof facet_cat = Inputs.toggle({label: \"Combine together?\", values: [null, \"paycut_amount\"]});\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\";\n\nboxdata = transpose(remun_head_paycut_df_convert);\n\nPlot.plot({\n  height: 610,\n  marginBottom: 60,\n  \n  x: {\n    className: \"axis-lab\",\n    label: \"Head of University Salary (¬£000s)‚Üí\",\n    grid: true,\n    nice: true,\n    ticks: 5,\n    labelOffset: 55\n  },\n  \n  y: {\n    label: \"Pay cuts\",\n    grid: true,\n    ticks: null\n  },\n  \n  facet: {data: boxdata, y: facet_cat, marginLeft: 70},\n  \n  sort: {fy: \"y\"},\n  \n  fy: {label:\"Pay cuts\",domain: [\"100%\", \"80%\", \"60%\", \"50%\", \"25%\", \"20%\"]},\n  \n  marks: [\n  \n    Plot.frame(),\n  \n    Plot.dot(boxdata, \n    \n    Plot.dodgeY({\n      x: \"imput_tot_remun_b4_sal_sac\", \n      fill: \"paycut_amount\",\n      stroke: \"black\",\n      anchor: \"middle\", \n      r: 5.5,\n      fillOpacity: 0.8,\n      padding: 0.5,\n      title: (d) =>`${d.he_provider} \\n VC Salary: ¬£${d.imput_tot_remun_b4_sal_sac},000 \\n Pay cuts: ${d.paycut_amount}`\n      }))\n      \n  ],\n  \n  color: {\n  legend: false,\n  domain: [\"100%\", \"80%\", \"60%\", \"50%\", \"25%\", \"20%\"],\n  scheme: \"Reds\",\n  reverse: true,\n  style: {\n  fontSize: \"1em\"}\n  },\n  \n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity head salary vs.¬†proposed pay cuts in response to marking and assessment boycott  Broken down by pay cut amount and institution. Source: List of salary deductions - @DrKaitClark. Head of university salary data from HESA.  Graphic: Yusuf Imaad Khan / @yusuf_i_k  Rogue Analysisüìà"
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "",
    "text": "The other day I made some charts for The Polycrisis newsletter. They were for a piece on the globalisation debate, written by Anthea Roberts and Nicolas Lamp. In this post, I offer some general reflections, and run through the steps to make one of the trickier charts that appeared in it."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#hockey-sticks-and-crosses",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#hockey-sticks-and-crosses",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "Hockey sticks and crosses",
    "text": "Hockey sticks and crosses\nFor some background, here is the opening of Roberts‚Äô and Lamp‚Äôs piece entitled ‚ÄúHockey Sticks and Crosses - Images that define the globalization debate‚Äù:\n\n‚ÄúThey say a picture is worth a thousand words. Two types of images are key to understanding current debates about economic globalization: the hockey stick chart, representing the stunning and inexorable growth of some phenomenon; and the cross chart, whose lines represent changes in relative power and prosperity.\nThere are good and bad hockey sticks, and the job of policy makers the world over is to harness the former while curbing the latter. But the domestic and international politics of addressing these hockey sticks is complicated by their intersection with distributive conflicts‚Äîwhich can be seen in the form of crosses.‚Äù\n\nDrawing on previous work, and using the images of hockey sticks and crosses, Roberts and Lamp identify a series of narratives that ‚Äúdominate Western debate about the virtues and vices of economic globalization‚Äù.\nThere is the hockey stick of global prosperity, that the establishment wheels out to defend their pursuit of endless growth and the continued march of globalisation.\n\nThis is mirrored in the hockey stick of doom, where globalisation and the pursuit of growth are driving up cumulative carbon emissions and fueling the climate crisis.\n\nAll the while, the polarity of the international system is shifting and this can be seen in the cross of global power, where US hegemony is increasingly challenged by China.\n\nI‚Äôd recommend reading the whole piece because these huge ideas are clearly set out, and the framing is interesting to think with. The charts are a bit mediocre, but they get the job done.\nCrucially, Roberts and Lamp resist reduction or overly simplistic framings of these issues:\n\n‚ÄúNo single narrative or image can capture the multifaceted nature of complex issues like economic globalization and the climate crisis. Understanding different perspectives and how they interact is crucial.‚Äù\n\nI have a lot of time for this kind of approach - which could be understood as perspectival humility. It seems a reasonable precept for thinking about the polycrisis, and it‚Äôs upfront about the limits of our individual perspectives or frameworks. It did make me think about places where perspectival humility is lacking. I have in mind debates with some opponents of degrowth who seem to dismiss, deride, or entirely misread the relevant literature.1\nAs I understand it, the point is that all of these hockey sticks and crosses, the seemingly inexorable growth and distributional conflicts, are happening everywhere all at once, with varying degrees of consequence. People are acting in the world in accordance with the narratives they find salient, and to have any hope of moving forward, we must understand this. The task may be to recognise the immense fallout of global shifts and attempt a project of reconciling our viewpoints to the modern world, and to each other. As the Instagram influencer Rumi2 puts it:\n\nThe truth was a mirror in the hands of God. It fell to earth and broke into pieces. Everyone took a piece, saw their own reflection, and thought they had the truth.\n\nSlightly grandiose but it captures the point nicely. Enough preambling. The main point of this Rogue Analysis post is to run through the steps to make a chart that appeared in the piece. 24 hockey sticks representing prosperity and doom."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#the-great-acceleration",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#the-great-acceleration",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "The Great Acceleration",
    "text": "The Great Acceleration\nThe chart in question depicts what has come to be known as the ‚ÄúGreat Acceleration‚Äù - a surge of human activity in the mid-twentieth century. In an incredible effort to ‚Äúbuild a more systematic picture of the human-driven changes to the Earth System‚Äù, the International Geosphere-Biosphere Programme (IGBP) collected data on 24 indicators. 12 to record the trajectory of the human enterprise, and 12 for features of the Earth System.\nIt is worth reflecting on this striking paragraph on their findings. It appears in the the 2004 IGBP report synthesising research from their first phase:\n\nOne feature stands out as remarkable. The second half of the twentieth century is unique in the entire history of human existence on Earth. Many human activities reached take-off points sometime in the twentieth century and have accelerated sharply towards the end of the century. The last 50 years have without doubt seen the most rapid transformation of the human relationship with the natural world in the history of humankind.\nSteffen et al., 2004, p.131\n\nAs I understand it, the Great Acceleration has also formed part of the basis for defining the proposed geological epoch, the Anthropocene. The more I think about it and try to understand what the IGBP team did, the more remarkable it becomes.\nThis year, Professor Will Steffen, the climate scientist who led the IGBP, who led this research, and who did so much more, passed away.\n\nThank you to Tim/@70sBachchan for sharing and discussing various versions of this chart, and pointing me to the sources behind it.\nWhere possible, the IGBP collected data starting from 1750 (or earlier) to capture the beginning of the industrial revolution. The 2004 version of the paper takes the time series up to the year 2000. The 2015 version of the paper takes it up to 2010. Some of the indicators differ between versions. The choices of indicator, the collection of the data, and the dataset construction are described in the respective versions. However long this blog post runs, just remember that this is a drop in the ocean compared to what the IGBP team did.\nHere is an earlier version of the chart from 2004:\n\nI don‚Äôt think its exaggeration to say that this is one of the most important visualisations in human history. It is incredible how we can know this. How long we have known this for. And how many far-reaching ramifications the Great Acceleration has had, and that we are yet to fully comprehend.\nHere is the version from the 2015 paper with a data update, some indicators which changed, and a line to highlight 1950, where most of the indicators start to surge:\n\nIt can be a bit overwhelming to be confronted with all this information, and a natural question is ‚Äúhow is this being measured?‚Äù. To be clear about my limitations, I do not fully understand all the indicators listed here, or all of the methods and efforts to collect this data. I am deferring to scientific expertise, the combined efforts of those who worked on this project, and I am simply trying to reproduce a chart. That said, I found a version of the 2015 Steffen et al.¬†paper with all the units, captions, and sources in a list. So I‚Äôve put them all under the button below in case its helpful to see:\n\n\n\n\n\n\nThe 24 indicators - units/captions/sources\n\n\n\n\n\nTrends from 1750 to 2010 in indicators for the structure and functioning of the Earth System.\n1. Carbon dioxide. Unit: parts per million. Caption: Carbon dioxide from firn and ice core records (Law Dome, Antarctica) and Cape Grim, Australia (deseasonalised flask and instrumental records); spline fit. Source: D. Etheridge CSIRO, Australia; Etheridge et al.¬†1996; MacFarling Meure et al.¬†2004 and 2006; Langenfelds et al., 2011.\n2. Nitrous oxide. Unit: parts per billion. Caption: Nitrous oxide from firn and ice core records (Law Dome, Antarctica) and Cape Grim, Australia (deseasonalised flask and instrumental records); spline fit. Source: D. Etheridge CSIRO, Australia; MacFarling Meure et al.¬†2004 and 2006; Langenfelds et al., 2011.\n3. Methane. Unit: parts per billion. Caption: Methane from firn and ice core records (Law Dome, Antarctica) and Cape Grim, Australia (deseasonalised flask and instrumental records); spline fit. Source: D. Etheridge CSIRO, Australia; Etheridge et al.¬†1998; MacFarling Meure et al.¬†2004 and 2006; Langenfelds et al., 2011.\n4. Stratospheric ozone. Unit: Percentage. Caption: Maximum percentage total column ozone decline (2-year moving average) over Halley, Antarctica during October, using 305 DU, the average October total column ozone for the first decade of measurements, as a baseline. Source: Data provided by J. D. Shanklin, British Antarctic Survey, UK. www.antarctica.ac.uk/met/jds/ozone/index.html#data\n5. Surface temperature. Unit: Degrees Celsius. Caption: Global surface temperature anomaly (HadCRUT4: combined land and ocean observations, relative to 1961-1990, 20 y Gaussian smoothed). Source: P. Jones, Climatic Research Unit, UK in conjunction with the Hadley Centre (UK). http://www.cru.uea.ac.uk/cru/info/warming/gtc.csv\n6. Ocean acidification. Unit: nmol kg-1. Caption: Ocean acidification expressed as global mean surface ocean hydrogen ion concentration from a suite of models (CMIP5) based on observations of atmospheric CO2 until 2005 and thereafter RCP8.5. Source: James Orr, LSCE/IPSL, France; Bopp et al.¬†2013 and IPCC Fifth assessment report, Working Group 1, Chapter 6 (Ciais et al.¬†2013).\n7. Marine fish capture. Unit: Million tonnes. Caption: Global marine fishes capture production (the sum of coastal, demersal and pelagic marine fish species only), i.e., it does not include mammals, molluscs, crustaceans, plants etc. There are no FAO data available prior to 1950. Source: Data is from the FAO Fisheries and Aquaculture Department online database (FAO-FIGIS 2013).\n8. Shrimp aquaculture. Unit: Million tonnes. Caption: Global aquaculture shrimp production (the sum of 25 cultured shrimp species) as a proxy for coastal zone modification. Source: Data is from the FAO Fisheries and Aquaculture Department online database FishstatJ (FAO 2013).\n9. Nitrogen to coastal zone. Unit: Mtons yr-1. Caption: Model-calculated human-induced perturbation flux of nitrogen into the coastal margin (riverine flux, sewage and atmospheric deposition). Source: Mackenzie et al., 2002.\n10. Tropical forest loss. Unit: Percentage. Caption: Loss of tropical forests (tropical evergreen forest and tropical deciduous forest, which also includes the area under woody parts of savannas and woodlands) compared with 1700. Source: Julia Pongratz, Carnegie Institution of Washington, Stanford, US; Pongratz et al.¬†2008. AD 1700 to 1992 is based on reconstructions of land use and land cover (Pongratz et al.¬†2008). Beyond 1992 is based on the IMAGE land use model.\n11. Domesticated land. Unit: Percentage. Caption: Increase in agricultural land area, including cropland and pasture as a percentage of total land area. Source: Julia Pongratz, Carnegie Institution of Washington, Stanford, US; Pongratz et al.¬†2008. AD 1700 to 1992 is based on reconstructions of land use and land cover (Pongratz et al., 2008). Beyond 1992 is based on the IMAGE land use model.\n12. Terrestrial biosphere degradation. Unit: Percentage. Caption: Percentage loss of terrestrial mean species abundance relative to abundance in undisturbed ecosystems as an approximation for degradation of the terrestrial biosphere. Source: R. Alkemade, PBL Netherlands Environmental Assessment Agency: modeled mean species abundance using GLOBIO3 based on HYDE reconstructed historical land use change estimates (until 1990) then IMAGE model estimates (Alkemade et al.¬†2009, www.globio.info, ten Brink et al., 2010).\nTrends from 1750 to 2010 in globally aggregated indicators for socio-economic development.\n1. Population. Unit: billion. Caption: Global population data according to the HYDE (History Database of the Global Environment) database. Data before 1950 are modeled. Data are plotted as decadal points. Sources: HYDE database 2013; Klein Goldewijk et al.¬†2010.\n2. Real GDP. Unit: trillion US real 2010 dollars. Caption: Global real GDP (Gross Domestic Product) in year 2010 US dollars. Data are a combination of Maddison for the years 1750 to 2003 and Shane for 1969-2010. Overlapping years from Shane data are used to adjust Maddison data to 2010 US dollars. Sources: Maddison 1995; M. Shane, Research Service, United States Department of Agriculture (USDA); Shane 2014.\n3. Foreign direct investment. Unit: trillion US dollars. Caption: Global Foreign direct investment in current (accessed 2013) US dollars based on two data sets: IMF (International Monetary Fund) from 1948-1969 and UNCTAD (United Nations Conference on Trade and Development) from 1970-2010. Sources: IMF 2013; UNCTAD 2013.\n4. Urban population. Unit: billion. Caption: Global urban population data according to the HYDE database. Data before 1950 are modeled. Data are plotted as decadal points. Sources: HYDE database 2013; Klein Goldewijk et al.¬†2010.\n5. Primary energy use. Unit: Exajoule (EJ). Caption: World primary energy use. 1850 to present based on Grubler et al.¬†2012 1750-1849 data are based on global population using 1850 data as a reference point. Sources: A. Grubler, International Institute for Applied Systems Analysis (IIASA); Grubler et al.¬†2012.\n6. Fertilizer consumption. Unit: million tonnes. Caption: Global fertilizer (nitrogen, phosphate and potassium) consumption based on International Fertilizer Industry Association (IFA) data. Sources: Olivier Rousseau, IFA; IFA database.\n7. Large dams. Unit: thousand dams. Caption: Global total number of existing large dams (minimum 15 m height above. foundation) based on the ICOLD (International Committee on Large Dams) database. Source: ICOLD database register search. Purchased 2011.\n8. Water use. Unit: thousand km 3. Caption: Global water use is sum of irrigation, domestic, manufacturing and electricity water withdrawals from 1900 to 2010 and livestock water consumption from 1961-2010. The data are estimated using the WaterGAP model. Source: M. Fl√∂rke, Center for Environmental Systems Research, University of Kassel; Fl√∂rke et al.¬†2013; aus der Beek et al.¬†2010; Alcamo et al.¬†2003.\n9. Paper production. Unit: million tonnes. Caption: Global paper production from 1961 to 2010. Sources: Based on FAO on-line statistical database FAOSTAT.\n10. Transportation. Unit: million motor vehicles. Caption: Global number of new motor vehicles per year. From 1963-1999 data include passenger cars, buses and coaches, goods vehicles, tractors, vans, lorries, motorcycles and mopeds. Data 2000-2009 include cars, buses, lorries, vans and motorcycles. Sources: IRF (International Road Federation) 2011.\n11. Telecommunications. Unit: billion telephone subscriptions. Caption: Global sum of fixed landlines (1950-2010) and mobile phone subscriptions (1980-2010). Landline data are based on Canning for 1950-1989 and UN data from 1990-2010 while mobile phone subscription data are based solely on UN data. Sources: Canning 1998; United Nations Statistics Division (UNSD) 2014.\n12. International tourism. Unit: million arrivals. Caption: Number of international arrivals per year for the period 1950-2010. Sources: Data for 1950-1994 are from UNWTO (United Nations World Tourism Organization) 2006 and data for 1995-2004 are from UNWTO 2011, data for 2005-2010 are from UNWTO 2014.\n\n\n\nLet‚Äôs take a look at a few more versions of this chart. Here is a more stylised version\n\nHere is a stacked area chart version. I think the values have been scaled and stacked? I‚Äôm not sure.\n\n\nAnd finally, here is a version that I think appeared as a multi-page spread in New Scientist:\n\nDramatic. But the details of each indicator are lost. And the y-axis is ???? I really do applaud the vision though. It seems like a good idea to keep trying to expand our representational apparatus, and find new ways to explore and explain things.\nThese are a few examples of people visualising this. There are many more examples, and lots to learn from. So when I was about to make a version of this chart, I was thinking ‚Äúgiven all these other great charts, what am I going to add here?‚Äù.\nIt seemed unlikely that I could improve on the small multiple shaded area charts. I tried to keep it simple with 3 clear goals. Clarity, minimalism, and fitting all the charts on a mobile screen. Because let‚Äôs face it. Most people see charts on their phone these days.\nHere is my take on the Great Acceleration charts:\n\nI think the mobile screen goal worked on the Phenomenal World website (the image above is a bit big). I hope its a bit clearer. And I‚Äôve tried to keep it pretty minimal. I know of two errors on this version of the chart:\n\n\n\n\n\n\nErrors\n\n\n\n\nMy data cleaning cut off a small chunk of the fertilizer consumption time series at the start. The overall trend is still visible.\nI filtered a single value that was 0 in 1943 for surface temperature. Thankfully, the plotting tool interpolated and its virtually indistinguishable with or without that observation.\n\n\n\nDo let me know if you spot others. My code is here. There is a great deal more to be improved on too:\n\n\n\n\n\n\nPossible improvements\n\n\n\n\n\n\nThe gradient is not mapped to the Y values. It simply starts at an arbitrary % value - 25% -this notebook might fix the issue\nIn the original paper, the stratospheric ozone Y axis goes to 100% to show that ozone loss actually levelled off. This is kind of visible here, but really it should have been faithful to the original\nThe clipping might be too sharp\nWhere the Y axis doesn‚Äôt start at 0, I tried to make the hard line at the bottom conditional. I think the gradient could have faded upwards as well in order to deemphasise a hard breaking point\nThe aspect ratio is fairly arbitrary and I tried to scale it so it would fit on the website. The 4x6 layout might be squashing things too much\nThe coastal nitrogen data seems to be different compared to other charts. I think I know why - the time series goes back before 1750, there are large gaps, and it seems other versions of the visual interpolate using pre-1750 values. I could be wrong.\nMaybe the messaging of this chart is irresponsible. I‚Äôm not sure. We‚Äôre hurtling towards catastrophe and I‚Äôm nitpicking about the details of reproducing a chart that‚Äôs been about for a while. Its very absurd.\n\n\n\n\nIf you‚Äôd like to have a go at improving this chart, you can edit the code live over here."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#sourcing-the-data",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#sourcing-the-data",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "Sourcing the data",
    "text": "Sourcing the data\nRight. Let‚Äôs make this chart. Where is the data? Easy - the IGBP website. Okay so its sitting in an Excel spreadsheet. No worries. I‚Äôm sure this will all be cleaned up. There are probably like 1 or 2 sheets. Max 3. Quick job. No worr-\n\nAh.\n\nSo. Each of the 24 variables for the Great Acceleration chart are sitting in a different sheet. And‚Ä¶all the sheets are structured slightly different. That‚Äôs fine ü•≤. Let‚Äôs press on. Our goal is to take each of the 24 sheets, stitch them together, and put them all in a tidy data format to work with."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#cleaning-it-up",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#cleaning-it-up",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "Cleaning it up",
    "text": "Cleaning it up\nFirst, load up some packages to help clean this all up.\n\npacman::p_load(tidyverse, readxl, janitor, here)\n\n(I flip between base R and tidyverse/purrr because I can‚Äôt always make things work in tidyverse/purrr. I will try to be more consistent in the future.)\nNext, let‚Äôs write a function to get every single sheet from the spreadsheet.\n\n# First write a function to get all data from the excel sheets\n\nget_data_from_all_sheets <- function(file_path) {\n  sheets <- readxl::excel_sheets(file_path)\n  data <- list()\n  \n  for (sheet_name in sheets) {\n    sheet_data <- readxl::read_excel(file_path, sheet = sheet_name)\n    data[[sheet_name]] <- sheet_data\n  }\n  \n  return(data)\n}\n\n# Use the function\n\nsheet_data <- get_data_from_all_sheets(\"igbp_great_acceleration_data_collection.xlsx\")\n\nLet‚Äôs take a look at what this gives us:\n\nsummary(sheet_data)\n\n                                Length Class  Mode\nRead me                          5     tbl_df list\n1 Population                     6     tbl_df list\n2 Real GDP                       6     tbl_df list\n3 FDI                            6     tbl_df list\n4 Urban population               6     tbl_df list\n5 Primary energy use             3     tbl_df list\n6 Fertilizer consumption         6     tbl_df list\n7 Large dams                     6     tbl_df list\n8 Water use                      6     tbl_df list\n9 Paper production               6     tbl_df list\n10 Transportation               13     tbl_df list\n11 Telecommunications            6     tbl_df list\n12 International Tourism         3     tbl_df list\n1 CarbonDioxide                  2     tbl_df list\n2 NitrousOxide                   3     tbl_df list\n3 Methane                        2     tbl_df list\n4 Ozone                          2     tbl_df list\n5 Temperature                    2     tbl_df list\n6 OceanAcidification             2     tbl_df list\n7 Marine fish                    2     tbl_df list\n8 ShrimpAqu                      2     tbl_df list\n9 Nitrogen                       2     tbl_df list\n10 TropicalForest                2     tbl_df list\n11 DomLand                       2     tbl_df list\n12 Terrestrial biosph degradati  2     tbl_df list\n\n\nGreat. We have all the sheets as a big list of tibbles (I promise that‚Äôs a real term). The 24 variables for the Great Acceleration and one for a ‚ÄúRead me‚Äù. Let‚Äôs take a closer look at what is in each tibble by clicking ‚Äúsheet_data‚Äù in our environment:\n\nAnd if we click on that little icon next to `2 Real GDP`?\n\nRight. So its not picking up the right row as the correct heading for each column (or variable). Let‚Äôs sort this and a few other problems out.\nFirst, remove the read me tibble - we don‚Äôt need it for the chart. But we should read it!\n\n# Remove read me (but obviously have a read of it)\nsheet_data <- sheet_data[!names(sheet_data) %in% c(\"Read me\")]\n\nNext, let‚Äôs fix which row each tibble is picking up as the first row:\n\n# Fix the var name row - should have one column in it that contains \"Year\"\n\nfixed_list <- lapply(sheet_data, function(tibble) {\n  year_row <- which(apply(tibble, 1, function(row) any(grepl(\"Year\", row, ignore.case = FALSE))))\n  \n  if (length(year_row) > 0) {\n    col_names <- as.character(tibble[year_row[1], ])\n    tibble <- tibble[-year_row, ]\n    names(tibble) <- col_names\n  }\n  \n  return(tibble)\n})\n\nAnd then, we can get rid of any completely empty variables (i.e.¬†columns that its picking up that have no title and no data):\n\n# Get rid of empty vars\n\nfixed_list <- lapply(fixed_list, function(tibble) {\n  tibble <- tibble[, colSums(is.na(tibble)) < nrow(tibble)]\n  tibble <- tibble[, colnames(tibble) != \"\" & !is.na(colnames(tibble))]\n  return(tibble)\n})\n\nWe are getting somewhere! How are we going to link all these tibbles up? Right now we have 24 different tables sitting in a list (an object that contains an ordered bunch of objects). We need to squash this and create a giant table with all the 24 variables in it. How do we get there? We need a common variable to match all the variables together. Like a key or something. What is that variable? Well in this case its the year variable. All this data is about the world. But the thing linking each observation is the year its happening.\nBut here is a snag. In some of the sheets, year is called ‚ÄúYear AD‚Äù. So it won‚Äôt match all the other variables called ‚ÄúYear‚Äù. Let‚Äôs rename that to help with the matching:\n\n# Rename any slightly different year columns so we can join it all up\n\nfixed_list <- map(fixed_list, ~ {\n  .x %>%\n    rename_with(~ if_else(. %in% c(\"Year AD\"), \"Year\", .), .cols = everything())\n})\n\nWe can also trim the dataset a bit for our purposes. For some variables, it includes breakdowns for classifications like OECD/BRICs etc. I only need the values for the whole world. For my purposes, I‚Äôm just going to get rid of the rest of the categories because I don‚Äôt need them:\n\n# Remove redundant vars\n\nfixed_list <- map(fixed_list, ~ select(.x, -one_of(c(\"OECD\", \"BRICS\", \"Rest\", \"OEDC\", \"OECD accumulative\", \"BRICS accumulative\", \"Rest  accumulative\")))) ## rest accumalative - 2 spaces\n\nNearly there. Let‚Äôs get rid of any completely empty spaces by getting rid of any NAs appearing in the year variable across all the tibbles. I made a mistake here last time, but thankfully it was small. See the errors callout above.\n\n# Get rid of NAs in Year variable *only*\n\nfixed_list <- map(fixed_list, ~ filter(.x, !is.na(Year)))\n\nRemember how I only left values for the world? Not BRICs or OECD? Well in each tibble, those values appear under a column called ‚ÄúWorld‚Äù. So you click population, it will say ‚ÄúYear‚Äù in one column then ‚ÄúWorld‚Äù in another. And if we join that altogether, it will be a bit tricky to distinguish which World values are for what indicator.\nR will probably do some renaming to tackle the duplication, but it might not be understandable for us. So let‚Äôs just add the list element name of each indicator to every ‚ÄúWorld‚Äù variable. E.g ‚Äú1 Population_World‚Äù or ‚Äú4 Urban population_World‚Äù.\n\n# Add list element name to each var except year so we can discern what each var is referring to\n\nfixed_list <- imap(fixed_list, ~ {\n  var_names <- names(.x)\n  year_var <- var_names[var_names == \"Year\"]\n  other_vars <- var_names[var_names != \"Year\"]\n  other_vars_new <- paste0(.y, \"_\", other_vars)\n  names(.x) <- c(year_var, other_vars_new)\n  .x\n})\n\nNow comes the bit that is at least somewhat satisfying:\n\nTake all of the tibbles that are in the list (I promise you these are meaningful terms)\nReduce them into a giant tibble - by joining each one of them by the ‚ÄúYear‚Äù variable\nClean up all the names (remove spaces/upper case etc.)\nAnd then flip the whole thing from being 25 variables (columns) by 331 observations (rows), to 3 variables (columns) to 7944 observations (rows). This is because I stuck each of the 24 great acceleration variables into a column called indicator. And I‚Äôm left with a long dataset with just 3 variables ‚Äúyear‚Äù, ‚Äúindicator‚Äù, and ‚Äúvalues‚Äù.\n\nHere is what I mean by flipping:\n\n\n\nhttps://www.statology.org/long-vs-wide-data/\n\n\nLet‚Äôs go ahead and do that:\n\n# Join everything in the nested list together, put it in long format, and then classify trend type for facet\n\nigbp_combined_df <- reduce(fixed_list, full_join, by = \"Year\") %>% \n  clean_names() %>% \n  pivot_longer(2:25, names_to = \"indicator\", values_to = \"values\")\n\nOkay! That is the bulk of the cleaning done. We have gone into the spreadsheet, grabbed the 24 sheets we needed, and stitched them all together. I‚Äôm sure there‚Äôs an easier way to achieve this, and I apologise if I haven‚Äôt explained this in enough detail. I have linked relevant concepts in the explanation above, and hopefully the steps give you a sense of whats happening.\nThere are a few more steps to clean up the data, but I‚Äôll spare you all the details in this write up. If you‚Äôre super curious, you can click the buttons below to see all the steps and code for the rest of the cleaning. But some of these are extremely minor points.\n\n\n\n\n\n\nClick for steps\n\n\n\n\n\n\nClassify indicators as ‚Äúearth-systems‚Äù or ‚Äúsocio-economic‚Äù trends\nFilter for years between 1750-2010 (some indicators included projections)\nEnsure all the values are numeric so you can plot them\nFilter NAs once in long format (I thought it was appropriate at this stage and wouldn‚Äôt change the chart)\nArrange the data by year\nSort out the factor levels (i.e.¬†make all the socio-economic trends appear before earth systems trends in same order as spreadsheet/paper)\nFilter a potential outlier that is stretching the Y axis for ozone loss but otherwise isn‚Äôt visible on the chart?\nConvert the years as numeric for now (because I end up parsing them as years in javascript later on)\nRound year values (e.g.¬†1999.6 = 2000)\nWrite proper labels and y axis labels for units\nEdit one variable because its giving % as decimal rather than % digits (i.e.¬†0.1 instead of 10%), and its simpler to fix this now\nCalculate minimum values for surface temperature so you can shade underneath the line when it goes below 0 (the plotting tool defaults to shading between 0 and values)\n\n\n\n\n\n\nCode\nigbp_combined_df <- igbp_combined_df %>%\n  \n  mutate(trend_type = if_else(indicator%in%c(\n    \n    \"x1_population_world\",                                                 \n    \"x2_real_gdp_world\",                                                   \n    \"x3_fdi_world\",                                                        \n    \"x4_urban_population_world\",                                           \n    \"x5_primary_energy_use_exajoule_ej\",                                   \n    \"x6_fertilizer_consumption_world_incl_historic\",                       \n    \"x7_large_dams_world_accumulative\",                                    \n    \"x8_water_use_world\",                                                  \n    \"x9_paper_production_world\",                                           \n    \"x10_transportation_world\",                                            \n    \"x11_telecommunications_world\",                                        \n    \"x12_international_tourism_world\"),\n    \n  \"Socio-economic Trends\", \"Earth System Trends\")) %>% \n    \n  # Filter for range (some variables include projections)\n  \n  filter(!year>2010,\n         !year<1750) %>% \n  \n  # convert to numeric\n  \n  mutate(values = as.numeric(values)) %>% \n  \n  # Filter empty observations\n  \n  filter(!is.na(values)) %>% \n  \n  # Order by year\n  \n  arrange(year) %>% \n  \n  # sort out the levels\n  \n  arrange(factor(indicator, levels=c(\n    \n    \"x1_population_world\",                                                 \n    \"x2_real_gdp_world\",                                     \n    \"x3_fdi_world\" ,                                                       \n    \"x4_urban_population_world\"  ,                                         \n    \"x5_primary_energy_use_exajoule_ej\"  ,                                 \n    \"x6_fertilizer_consumption_world_incl_historic\" ,                      \n    \"x7_large_dams_world_accumulative\",                    \n    \"x8_water_use_world\",                   \n    \"x9_paper_production_world\",                  \n    \"x10_transportation_world\",                 \n    \"x11_telecommunications_world\",                \n    \"x12_international_tourism_world\",\n    \n    \"x1_carbon_dioxide_carbon_dioxide_ppm\",              \n    \"x2_nitrous_oxide_nitrous_oxide_ppb\",             \n    \"x3_methane_methane_ppb\",            \n    \"x4_ozone_ozone_percent_loss\",           \n    \"x5_temperature_temperature_anomaly_deg_c\",          \n    \"x6_ocean_acidification_mean_hydrogen_ion_concentraion_h_nmol_kg\",     \n    \"x7_marine_fish_marine_fish_capture_million_tonnes\",    \n    \"x8_shrimp_aqu_shrimp_aquaculture_million_tonnes\",   \n    \"x9_nitrogen_nitrogen_flux_mtons_yr_1\",  \n    \"x10_tropical_forest_tropical_forset_loss_percent\", \n    \"x11_dom_land_domesticated_land_percent\",\n    \"x12_terrestrial_biosph_degradati_percent_decr_mean_species_abundance\"\n    \n  ))) %>% \n  \n  filter(\n    \n    #!values==0, # unsure why you did this in original version. Fix it and note error. Removes one observation in 1943. Thankfully Observable Plot interpolated and it does not affect the visual\n    \n    ## filter x4 ozone for 1962 - this year keeps extending the y axis very far because there is a single negative value for ozone loss. I need to be careful here. I HAVE MADE A JUDGMENT HERE TO OMIT THIS VALUE FROM THE DATASET - IT IS AN ANALYTICAL CHOICE BASED ON ME THINKING THAT THE READER SHOULD BE ABLE TO VIEW THE GENERAL TRENDS WITH THE MAXIMUM CLARITY POSSIBLE. LET THIS BE KNOWN. \n  \n    !(year == \"1962\" & indicator == \"x4_ozone_ozone_percent_loss\" & values<0)) %>% \n  \n  # Keep as numeric for now - convert in OJS\n  mutate(year = as.numeric(year),\n         \n         # Some values for year presented as e.g. 2010.5 - round this for now\n         year = ceiling(year),\n         \n         # Rename indicators to readable versions\n         \n         labels = case_match(indicator,\n                             \"x1_population_world\"~\"World Population\",                                                 \n                             \"x2_real_gdp_world\"~\"Real GDP\",                                                   \n                             \"x3_fdi_world\"~\"Foreign Direct Investment\",                                                       \n                             \"x4_urban_population_world\"~\"Urban Population\",                                         \n                             \"x5_primary_energy_use_exajoule_ej\"~\"Primary Energy Use\",                                 \n                             \"x6_fertilizer_consumption_world_incl_historic\"~\"Fertilizer Consumption\",                      \n                             \"x7_large_dams_world_accumulative\"~\"Large Dams\",                    \n                             \"x8_water_use_world\"~\"Water Use\",                   \n                             \"x9_paper_production_world\"~\"Paper Production\",                  \n                             \"x10_transportation_world\"~\"Transportation\",                 \n                             \"x11_telecommunications_world\"~\"Tele- communications\",                \n                             \"x12_international_tourism_world\"~\"International Tourism\",               \n                             \"x1_carbon_dioxide_carbon_dioxide_ppm\"~\"Carbon Dioxide\",              \n                             \"x2_nitrous_oxide_nitrous_oxide_ppb\"~\"Nitrous Oxide\",             \n                             \"x3_methane_methane_ppb\"~\"Methane\",            \n                             \"x4_ozone_ozone_percent_loss\"~\"Stratospheric Ozone\",           \n                             \"x5_temperature_temperature_anomaly_deg_c\"~\"Surface Temperature\",          \n                             \"x6_ocean_acidification_mean_hydrogen_ion_concentraion_h_nmol_kg\"~\"Ocean Acidification\",     \n                             \"x7_marine_fish_marine_fish_capture_million_tonnes\"~\"Marine Fish Capture\",    \n                             \"x8_shrimp_aqu_shrimp_aquaculture_million_tonnes\"~\"Shrimp Aquaculture\",   \n                             \"x9_nitrogen_nitrogen_flux_mtons_yr_1\"~\"Coastal Nitrogen\",  \n                             \"x10_tropical_forest_tropical_forset_loss_percent\"~\"Tropical Forest Loss\", \n                             \"x11_dom_land_domesticated_land_percent\"~\"Domesticated Land\",\n                             \"x12_terrestrial_biosph_degradati_percent_decr_mean_species_abundance\"~\"Terrestrial Biosphere Degradation\",\n                             .default = indicator),\n         \n         # write up the labels - FT style for millions/billions - https://aboutus.ft.com/press_release/ft-makes-change-to-style-guide\n         \n         yAxisLabels = case_match(indicator,\n                                  \"x1_population_world\"~\"bn\",                                                 \n                                  \"x2_real_gdp_world\"~\"tn US$\",                                                   \n                                  \"x3_fdi_world\"~\"tn US$\",                                                       \n                                  \"x4_urban_population_world\"~\"bn\",                                         \n                                  \"x5_primary_energy_use_exajoule_ej\"~\"Exajoule - EJ\",                                 \n                                  \"x6_fertilizer_consumption_world_incl_historic\"~\"mn tonnes\",                      \n                                  \"x7_large_dams_world_accumulative\"~\"k dams\",                    \n                                  \"x8_water_use_world\"~\"k km¬≥\",                   \n                                  \"x9_paper_production_world\"~\"mn tonnes\",                  \n                                  \"x10_transportation_world\"~\"mn motor vehicles\",                 \n                                  \"x11_telecommunications_world\"~\"bn phone subscriptions\",                \n                                  \"x12_international_tourism_world\"~\"mn arrivals\",               \n                                  \"x1_carbon_dioxide_carbon_dioxide_ppm\"~\"atmos. conc. ppm\",              \n                                  \"x2_nitrous_oxide_nitrous_oxide_ppb\"~\"atmos. conc. ppb\",             \n                                  \"x3_methane_methane_ppb\"~\"atmos. conc. ppb\",            \n                                  \"x4_ozone_ozone_percent_loss\"~\"% loss\",           \n                                  \"x5_temperature_temperature_anomaly_deg_c\"~\"temp. anomaly ¬∞C\",          \n                                  \"x6_ocean_acidification_mean_hydrogen_ion_concentraion_h_nmol_kg\"~\"hydrogen ion, nmol kg‚Åª¬π\",     \n                                  \"x7_marine_fish_marine_fish_capture_million_tonnes\"~\"mn tonnes\",    \n                                  \"x8_shrimp_aqu_shrimp_aquaculture_million_tonnes\"~\"mn tonnes\",   \n                                  \"x9_nitrogen_nitrogen_flux_mtons_yr_1\"~\"Human N flux mtons yr‚Åª¬π\",  \n                                  \"x10_tropical_forest_tropical_forset_loss_percent\"~\"% loss area\", \n                                  \"x11_dom_land_domesticated_land_percent\"~\"% of total land area\",\n                                  \"x12_terrestrial_biosph_degradati_percent_decr_mean_species_abundance\"~\"% dec. mean species abundance\",\n                                  .default = indicator)) %>% \n  \n  # Convert to readable % values i.e. 40 rather than 0.4. This is because the y axis label will tell you if its % or not. No need for extra code to adjust each axis value for each of 24 vars\n  \n  mutate(values = if_else (indicator == \"x11_dom_land_domesticated_land_percent\", values*100, values)) %>%  \n  \n  # this is to consistently shade values under the line for surface temp facet + \n  # leave a little room underneath so the line isn't flush with bottom of multiple. \n  # Observable plot area mark defaults to shading within constraint of y = 0, but also offers y1 + y2 values\n  \n  group_by(indicator) %>% \n  mutate(min = if_else (indicator == \"x5_temperature_temperature_anomaly_deg_c\", min(values) - 0.1, NA)) %>% \n  ungroup() %>% \n  \n  # for the conditional bit of code that will fix differences between surface temp and other small multiples\n  \n  mutate(values_temp = values,\n         values_all_else = if_else (indicator == \"x5_temperature_temperature_anomaly_deg_c\", NA, values)) \n\n\nHere is what my final cleaned up dataset looked like:\n\nglimpse(igbp_combined_df)\n\nRows: 2,950\nColumns: 9\n$ year            <dbl> 1750, 1760, 1770, 1780, 1790, 1800, 1810, 1820, 1830, ‚Ä¶\n$ indicator       <chr> \"x1_population_world\", \"x1_population_world\", \"x1_popu‚Ä¶\n$ values          <dbl> 0.7388971, 0.7770402, 0.8198269, 0.8960748, 0.9404165,‚Ä¶\n$ trend_type      <chr> \"Socio-economic Trends\", \"Socio-economic Trends\", \"Soc‚Ä¶\n$ labels          <chr> \"World Population\", \"World Population\", \"World Populat‚Ä¶\n$ yAxisLabels     <chr> \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", ‚Ä¶\n$ min             <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ values_temp     <dbl> 0.7388971, 0.7770402, 0.8198269, 0.8960748, 0.9404165,‚Ä¶\n$ values_all_else <dbl> 0.7388971, 0.7770402, 0.8198269, 0.8960748, 0.9404165,‚Ä¶\n\n\nLet‚Äôs export that as a CSV too:\n\nwrite.csv(igbp_combined_df, \"igbp_combined_df.csv\", row.names=FALSE)\n\nDone!"
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#the-chart",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#the-chart",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "The chart",
    "text": "The chart\nNow. If you‚Äôve stuck it out this far, you might be thinking ‚ÄúYusuf. I really didn‚Äôt want to know all of that. I thought this was about making charts, and I was thinking you‚Äôd discuss fun things like colour choice and font. Can you just explain the chart a bit? And make it quick‚Äù. If you‚Äôre having this sort of reaction, I say‚Ä¶fair enough‚Ä¶but no promises. Let‚Äôs get to the chart."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#historical-materialism",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#historical-materialism",
    "title": "The Hockey Sticks of Prosperity and Doom",
    "section": "Historical materialism",
    "text": "Historical materialism\nBut then again, it seems possible to subsume some of this phenomena into a broader framing, and attribute primacy to certain causes over others. Many of these narratives seem epiphenomenal.\nI am not going to defend historical materialism in this post (maybe another time). But I just wanted to\nTake one debate that generates more heat than light these days - degrowth.\nDegrowth\nHistorical materialism\nIn this post, I wanted to run through the steps for making one of the charts. This is for a few reasons. Maybe its helpful for others. I haven‚Äôt seen a cleaned up version of this dataset.\nHere it is:\n\nKNOWN ERRORS"
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#rough-plots-with-ggplot2",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#rough-plots-with-ggplot2",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "Rough plots with ggplot2",
    "text": "Rough plots with ggplot2\nLet‚Äôs take a look at the data just to check that our cleaning worked out. For speed, we can use ggplot2. 6 short lines of code gets us this:\n\nigbp_combined_df %>% \nggplot(aes(year, values)) +\ngeom_line() +\nfacet_wrap(~labels, scales = \"free_y\", ncol = 4) +\nscale_x_continuous(breaks = c(1750, 2010)) +\ntheme_minimal() \n\n\n\n\nThat is not bad at all. Its a rough but meaningful chart that took basically no time. It seems to resemble the Great Acceleration charts. I did a few more spot checks, and after spotting two issues (noted above), one remaining mistake or difference from other charts I‚Äôve seen, is the nitrogen data. I think whats happening is that there are observations sitting outside the 1750-2010 range, and other visualisations have been created by interpolating between these dates for the nitrogen data. I could be wrong about the source of the discrepancy. Let me know what you reckon.\nAnyway, Its insane that we have tools like ggplot2. While we‚Äôre here, let‚Äôs see if we can make a quick version of that New Scientist Great Acceleration chart:\n\nigbp_combined_df %>% \n  \n# scale the values   \ngroup_by(indicator) %>% \nmutate(scaled_values = scale(values),\n# multiply Earth system trends by -1 to flip\n       scaled_values = if_else(trend_type == \"Earth System Trends\", scaled_values * -1, scaled_values)) %>% \nungroup() %>%\n\n# Plot it  \n\nggplot(aes(year, scaled_values, group = labels, colour = labels)) +\ngeom_line() +\nscale_x_continuous(breaks = c(1750, 1950, 2010)) +\ntheme_minimal() +\ntheme(legend.position = \"none\",\n      axis.title.x = element_blank(),\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid = element_blank())\n\n\n\n\nHmmm. Its a bit off, but the rough ideas are there. I won‚Äôt take this one much further. Now that the data seems like its in the right shape, let‚Äôs try and make the actual chart."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#what-tool-to-use",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#what-tool-to-use",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "What tool to use?",
    "text": "What tool to use?\nSo what tool to use? We could continue with ggplot2. Its great. Its fast, intuitive, and I‚Äôm already working in R. So that seems like the natural choice. But It can be tough to refine the styling precisely. Not impossible though. I‚Äôve seen people make extraordinary things with it.\nBut given that this is 24 small multiples and I want it to fit on a phone, I want a bit more control. It would also be good if the chart could be rendered at the highest possible quality, and then any exports would be scaled down versions of that.\nWhat tool would give us this? Its tempting to go for the other end of the spectrum from the high level of abstraction of ggplot2. This would be D3. D3 is incredible low level, but you can basically make whatever you want. The trade-off is that the code would be very long‚Ä¶and I am not where I want to be with my D3 skills.\nSo we‚Äôve reached an impasse. Ease and high level abstraction - ggplot2. Unparalleled flexibility and detail - D3. If only I could have both. Well‚Ä¶that‚Äôs what I take Observable Plot to be. Its built on D3 and by the creators of D3, but it uses the grammar of graphics that ggplot2 relies on for its wonderful capacity for abstraction. It is, as the great scholar H. Montana tells us, the best of both worlds. You can write less code to see more charts‚Ä¶but its extensible with D3 if you need it. Great for somebody like me who was sitting in between ggplot2 and D3 (and finding D3 tough)."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#finally-making-the-chart-with-observable-plot",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#finally-making-the-chart-with-observable-plot",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "Finally making the chart with Observable Plot",
    "text": "Finally making the chart with Observable Plot\nFirst let‚Äôs just move our dataset object in R to work with Observable Plot.\n\nojs_define(igbp_combined_df_convert = igbp_combined_df)\n\nNext, let‚Äôs import the latest version of Plot, change the data structure to work with it, and make sure it is reading all the dates correctly.\n\nnewPlot = import(\"https://esm.sh/@observablehq/plot@0.6.9\");\n\n\n// Transpose\ndata = transpose(igbp_combined_df_convert);\n\n//Parse time\nparseTime = d3.utcParse(\"%Y\");\n\ndataEdit = data // have to define this otherwise it throws an error\n  .forEach((d) => {\n  d.year = parseTime(d.year);\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs try and make this chart. Its important to note that Observable Plot is still being worked on, and new features are being added all the time. So one thing it can‚Äôt do easily is make small multiple charts where all the Y axes have different scales/units (scales = ‚Äúfree_y‚Äù for the ggplot2 users). It can do other standard small multiple charts, and I think they‚Äôre pretty close to proper solution for free scales. But for now, we have to use a small workaround. This workaround draws on excellent solutions suggested by Toph Tucker and Mike Bostock for this.\nRoughly how does this solution work? Well it takes the dataset, uses a bit of D3 to iterate through each group (the 24 Great Acceleration indicators), and makes a chart for each one using Observable Plot, and fits it into a space. I‚Äôve left the space as flexible so the chart will move about depending on device. If you‚Äôre on your phone, the charts per row will scale down to 1x24. So its a bit of a scroll. But if you open it on a laptop, it might be 4x6.\nHere it is:\n\n//https://observablehq.com/@tophtucker/facet-wrap + https://observablehq.com/@observablehq/plot-facet-wrap\n// Credit to Toph Tucker and Mike Bostock for these solutions. I mashed them together to make the Great Acceleration chart.\n\nhtl.html`<div id=facetChart style=\"display: flex; flex-wrap: wrap; line-height: 0px;\">\n  ${d3.groups(data, (d) => d.labels)\n    // iterate through the data\n    .map(([facet, data]) =>\n      htl.html`<figure>\n\n        ${newPlot.plot({\n          width: 199,\n          height: 170,\n          \n          marks: [\n      \n          newPlot.line(data, {\n          x: \"year\",\n          y: \"values\",\n          z: \"trend_type\",\n          // change the colour for the type of trend its showing\n          stroke: (d) => (d.trend_type === \"Socio-economic Trends\" ? \"#3f6bee\" : \"#e61f00\")})\n          \n          ]\n\n        })}\n\n      </figure>`\n  )}\n</div>`\n\n\n\n\n\n\nThat is basically the backbone of this chart. I really don‚Äôt want to ‚Äúrest of the owl‚Äù my explanation, but this post is dragging on a bit. So I‚Äôll do two things. First, there is a live version of the chart code that you can explore here. Second, the remaining steps to fix up the labelling and aesthetics are under the button below‚Ä¶\n\n\n\n\n\n\nFurther edits\n\n\n\n\n\n\nFix up the spacing between charts and axes\nMake the tick marks as minimal as possible (e.g.¬†x axis - 1750, 1950, 2010)\nLarger font\nOnly grid lines for Y\nRemove the tick marks that stick out from each axis\nWrite a bit of code to filter through data and get correct label for each y axis\nPut the legend in the title\nAdd shading under the line to better view trends and draw attention to acceleration. This will need to be conditional to ensure that variables increasing from non-0 are shaded from underneath and not just between 0 and line\nWhere the Y axis is being cut, deemphasise a hard floor by removing Y rule. For consistency, make floor visible for surface temperature as it is increasing, but from a previous temperature anomaly that was negative\nPut the label of each indicator in the frame and make it bold so you can see what each multiple is referring to\nAdd a dashed line to mark 1950 - where most indicators surge\nPlace a slightly thicker white line behind the line so it appears clearer against the shading and background\n\n\n\n\nAnd I‚Äôve folded the code away next to the chart.\n\nThe hockey sticks of prosperity and doom\n\n\nSocio-economic trends vs Earth system trends between 1750 and 2010\n\n\n\nCode\nhtl.html`<div id=facetChart style=\"display: flex; flex-wrap: wrap; line-height: 0px;\">\n  ${d3.groups(data, (d) => d.labels)\n    .map(([facet, data]) =>\n      htl.html`<figure>\n\n        ${newPlot.plot({\n        \n        // Dimensions\n          width: 199,\n          height: 170,\n          marginBottom: 20,\n          marginLeft: 35,\n          marginTop: 24,\n          \n        // Font size\n          style: {fontSize: \"12px\"},\n        \n        // x axis features  \n          x: {\n          label: null,\n          nice: false,\n          ticks: [new Date(\"1750-01-01\"), new Date(\"1950-01-01\"), new Date(\"2010-01-01\")],\n          domain: [new Date(\"1750-01-01\"), new Date(\"2010-01-01\")]\n          },\n          \n        // y axis features\n          y: {\n          nice: true,\n          ticks: 4,\n          grid: true,\n          tickSize: 0,\n          domain: d3.extent(data, (d) => d.values),\n          labelArrow: \"none\",\n          label: `${d3.groups(data, (d) => d.yAxisLabels).map(([yAxisLabels, data]) => yAxisLabels)}` //genuinely can't believe this worked\n          },\n          \n          \n        // remove legend\n          color: {legend: false},\n          \n        // actual components of shaded area chart\n          marks: [\n          \n        // Add gradient - Credit Mike Bostock again -  https://observablehq.com/@observablehq/plot-area-chart-with-gradient\n          \n          () => htl.svg`<defs>\n                <linearGradient id=\"gradientBlue\" gradientTransform=\"rotate(0)\">\n                <stop offset=\"25%\" stop-color=\"white\" stop-opacity=\"0\" />\n                <stop offset=\"100%\" stop-color=\"#3f6bee\" stop-opacity=\"0.7\" />\n                </linearGradient>\n                <linearGradient id=\"gradientRed\" gradientTransform=\"rotate(0)\">\n                <stop offset=\"25%\" stop-color=\"white\" stop-opacity=\"0\" />\n                <stop offset=\"100%\" stop-color=\"#e61f00\" stop-opacity=\"0.7\" />\n                </linearGradient>\n          </defs>`,\n        \n        // add a y rule for surface temp 0 value\n          \n          newPlot.ruleY(data, {\n            y: 0, \n            stroke: \"black\", \n            strokeWidth: (d) => (d.labels === \"Surface Temperature\" ? 0.5 : 0), // x position and clipping is different for surface temp data\n            clip: \"frame\"\n            }), \n      \n          \n        // add dashed line for 1950\n          \n          newPlot.ruleX([new Date(\"1950-01-01\")], {stroke: \"black\", strokeDasharray: \"5,3\", strokeOpacity: 0.8}),\n        \n          \n        // add area chart\n          \n          newPlot.areaY(data, {\n          x: \"year\", \n          y: \"values_all_else\", \n          fill: (d) => (d.trend_type === \"Socio-economic Trends\" ? \"url(#gradientBlue)\" : \"url(#gradientRed)\"), //\"#3f6bee\" : \"#e61f00\"\n          clip: true,\n          curve: \"natural\"\n        }),\n        \n      // add area chart for surface temp (different because goes below 0)\n        \n        newPlot.areaY(data, {\n          x: \"year\", \n          y1: \"min\",\n          y2: \"values_temp\", \n          fill: (d) => (d.trend_type === \"Socio-economic Trends\" ? \"url(#gradientBlue)\" : \"url(#gradientRed)\"),\n          clip: true,\n          curve: \"natural\"\n        }),\n        \n      // add white line behind line to make it clearer\n        \n          newPlot.line(data, {\n          x: \"year\",\n          y: \"values\",\n          z: \"trend_type\",\n          stroke: \"#FFFFFF\",\n          clip: true,\n          strokeWidth:3,\n          curve: \"natural\"\n      }),\n      \n      // add line to emphasise area chart\n      \n          newPlot.line(data, {\n          x: \"year\",\n          y: \"values\",\n          z: \"trend_type\",\n          stroke: (d) => (d.trend_type === \"Socio-economic Trends\" ? \"#3f6bee\" : \"#e61f00\"),\n          clip: true,\n          strokeWidth:1.5,\n          curve: \"natural\"\n      }),\n      \n    // add a y rule for when starting y axis at 0\n      \n      newPlot.ruleY(data, {\n            y: 0, \n            stroke: \"black\", \n            strokeWidth: (d) => (d.labels === \"Surface Temperature\" ? 0 : 2.1), // put a hard axis rule for all starting at 0\n            clip: \"frame\"\n            }), // FIX THIS - conditional width?\n            \n    // add the y axis part of frame just for some separation between plots\n      \n      newPlot.frame({anchor: \"left\"}),\n      \n    // add text labels on each small multiple so we know what indicator it is\n    \n      newPlot.text([`${d3.groups(data, (d) => d.labels).map(([labels, data]) => labels)}`], \n          {frameAnchor: \"top-left\",\n          dx: 2.5, \n          dy: 4,\n          lineWidth: 5,\n          fontSize: \"16.5px\",\n          fontWeight: \"bold\",\n          fill: \"black\",\n          stroke: \"#FFFFFF\"\n          })\n          ]\n\n        })}\n\n      </figure>`\n  )}\n</div>`\n\n\n\n\n\n\n\nI think that should do it."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#conclusion",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#conclusion",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "Conclusion",
    "text": "Conclusion\nThat ran on for some time. I‚Äôd like to stress that all the tools I‚Äôve used to make these charts are free and open source (including the one to make this website), I have a page of resources with some handy links, and many people (along with a lot of luck) have helped me get to this point. I‚Äôm trying to make everything as open as I can in case its useful for others, but the real constraint for people is time. I can‚Äôt put that on GitHub. One suggestion might be to carefully use LLMs to help out with bits of code. The key word here is carefully.\nThis was an interesting challenge, and Anthea Roberts‚Äô and Nicolas Lamp‚Äôs piece gave me a lot to reflect on and look into. I‚Äôd encourage everybody to have a read of The Polycrisis newsletter. Kate Mackenzie and Tim Sahay have been putting together exceptional content on intersecting crises, the political economy of climate change, and global North/South dynamics."
  },
  {
    "objectID": "posts/hockey-sticks-of-prosperity-and-doom/index.html#postscript",
    "href": "posts/hockey-sticks-of-prosperity-and-doom/index.html#postscript",
    "title": "Making the Hockey Sticks of Prosperity and Doom üèí",
    "section": "Postscript",
    "text": "Postscript\nSometimes charts can help us understand things. But we should know their limits."
  },
  {
    "objectID": "posts/uk-sycophantic-stooge-us-imperialism/index.html",
    "href": "posts/uk-sycophantic-stooge-us-imperialism/index.html",
    "title": "The sycophantic stooge of US imperialism",
    "section": "",
    "text": "In light of the UK‚Äôs recent abstentions at the UN on a ceasefire in Gaza, I do some simple analysis on UK/US voting patterns at the UN General Assembly.\nOn Friday, the UK abstained on a UN Security Council vote for a ceasefire in Gaza. The US vetoed it. On Tuesday, the UK abstained again on a UN General Assembly vote for a ceasefire in Gaza. It passed. The Security Council vote would have been legally binding, the latter General Assembly vote is non-binding and symbolic - reflecting opinion amongst member states.\nWe cannot be naive about who the UN serves and how, but it is not trivial that 82% of the General Assembly voted for the ceasefire (153 for, 10 against, 23 abstentions) compared to 68% previously for the truce (121 for, 14 against, 44 abstentions).\nThe Palestinian people have refused to give up on their long struggle for liberation and freedom. The latest General Assembly result reflects the organisation and pressure of people worldwide in support of that struggle. Though the power does not reside with the General Assembly, maintaining and increasing pressure on member states to call for a ceasefire is critical to save lives, and to influence what happens next.\nAs is well known1, the power sits with the US - the only member of the Security Council to veto the resolution. Of all the times the US has used its veto in the Security Council, half have been to protect Israel. The US arms (as do we), protects, and supports Israel as the face of US imperialism in the Middle East. The UK may have inaugurated settler colonialism in Palestine, but in its decline as a global power, UK foreign policy ambitions were subordinated to the US.\nUnder this arrangement, it doesn‚Äôt matter whether the UK is referred to as the ‚Äúlieutenant‚Äù of American foreign policy, or how many times the term ‚ÄúSpecial Relationship‚Äù is publicly employed by UK officials to feign a relationship among equals. This arrangement is vastly conditioned by US economic, political, and military interests. Previous failures to comply by the UK have been rebuked by the US.\nAll this is to say, a stooge is a stooge, and each UK abstention from voting for a ceasefire in Gaza is complicity with genocide."
  },
  {
    "objectID": "posts/uk-sycophantic-stooge-us-imperialism/index.html#what-we-are-complicit-in",
    "href": "posts/uk-sycophantic-stooge-us-imperialism/index.html#what-we-are-complicit-in",
    "title": "The sycophantic stooge of US imperialism",
    "section": "What we are complicit in",
    "text": "What we are complicit in\nLet us first be clear about what the UK has helped create (historically and at present with our support), and what we are complicit in. I am not attempting this for completeness, novelty, or to lay claim to expertise that I don‚Äôt possess. Instead, my aim is to summarise some of what can easily be learnt about the present situation. You may wish to skip this section if you are up to date. Please correct me if I am mistaken. I would encourage looking at the following sources:\n\nDecolonize Palestine and the reading list they have put together\nAl Jazeera - Israel-Gaza war in maps and charts: Live tracker\nFinancial Times - The Israel-Hamas war in maps: latest updates\nGaza: An Inquest into Its Martyrdom - Norman Finkelstein\nFrom the River to the Sea: Essays for a Free Palestine - Edited by Sai Englert, Michal Schatz and Rosie Warren\n\nWe are witnessing an ongoing Nakba. We are witnessing a genocide of the Palestinian people. Since October the 7th, in Gaza and the West Bank at least 18,894 Palestinian people have been murdered by Israel - at least 7,794 of them children. By now this figure is likely out of date (figures used from - 14/12/23 10:00AM local time, 6:00AM GMT). It cannot be repeated enough that these are people not numbers. It cannot be repeated enough that this did not begin on October 7th.\nIsrael‚Äôs bombing of Gaza has been catastrophic, and has drawn comparison to the Allied bombing campaign of Germany for the extent of its devastation. The total yield of bombs has exceeded the atomic bombings of Hiroshima and Nagasaki. Hospitals, homes, schools, bakeries, shops, universities, and the parliament have all been bombed. Even where people attempt to reach ‚Äúsafe zones‚Äù in the south - the Israeli army bombs them. There are no ‚Äúsafe zones‚Äù. Flesh rots under the rubble. Just as it has when this has happened before.\nThe devastation wrought by the bombing has meant that 85% of the population has been displaced. Almost all of the 2.3 million people have crowded into the south of Gaza - bearing in mind, that the whole Gaza strip already had one of the highest population densities in the world. As a result, Palestinian people are living on the streets. The arrival of winter has exacerbated their suffering as rains, winds, storms, and now floods, hit their tents and they have no winter clothing.\nThe remaining hospitals are overwhelmed - with only 11 out of 36 ‚Äúpartially functional‚Äù. They are running out of medication and supplies, hospital procedures have taken place without anesthetic, and humanitarian aid is bottle-necked at a single checkpoint (but Israel has said it will open a second checkpoint). People are starving. Sanitation conditions are dire. All of this combined with the extreme population density has meant that disease is spreading fast.\nWhat happens when Palestinians speak out? The courageous and outspoken intellectual, Professor Refaat Alareer2, was assassinated by air strike along with six members of his family. In a powerful and poignant interview, in the midst of bombardment, Professor Alareer said the following:\n\nI am an academic. Probably the toughest thing I have at home is an Expo marker. But if the Israelis invade, if they barge at us, charge at us open door-to-door to massacre us, I am going to use that marker to throw it at the Israeli soldiers, even if that is the last thing that I would be able to do. And this is the feeling of everybody. We are helpless. We have nothing to lose.\n‚Äî Dr.¬†Refaat Alareer\n\nThe Israeli army did not charge at Professor Refaat Alareer in person. They ordered an airstrike. Think about this. The US, (arguably) the most powerful entity on Earth, arms, finances, protects, and supports Israel - itself a nuclear power. And yet, the Israeli army could not face Professor Alareer in person. Instead they resorted to an airstrike. What might that say about Israel? What might that say about Professor Refaat Alareer? Professor Alareer‚Äôs final published poem has been circulating and has been translated into multiple languages across the world.\nNothing can excuse these atrocities, and people understand that, even if our politicians act within the confines of expedience, and some would-be philosophers engage in all manner of ridiculous obfuscation in servility to power."
  },
  {
    "objectID": "posts/uk-sycophantic-stooge-us-imperialism/index.html#analysing-un-general-assembly-votes",
    "href": "posts/uk-sycophantic-stooge-us-imperialism/index.html#analysing-un-general-assembly-votes",
    "title": "The sycophantic stooge of US imperialism",
    "section": "Analysing UN General Assembly votes",
    "text": "Analysing UN General Assembly votes\nIn this post, my main aim was to do some simple analysis of UN General Assembly votes in light of the UK‚Äôs recent abstention. Simply put, I wanted to look at how often the UK follows the US at the UN General Assembly, and see if I could shed light on some obvious patterns of subordination.\nThe data I am using come from here - Erik Voeten - Data and Analyses of Voting in the UN General Assembly - UNVotes.rda - updated 21/09/2023\nWhat follows, of course, comes with caveats. Click below to see them:\n\n\n\n\n\n\nCaveats\n\n\n\n\n\n\nThe data I am using is only available between 01/01/1946 - 30/06/2023. So it does not include the most recent votes.\nThe issue classifications are based on ‚Äúsearches in descriptions‚Äù, have been subjected to a ‚Äúrudimentary visual check‚Äù, and ‚Äúmay not be 100% accurate‚Äù\nThe breakdown of the dataset coverage is as follows (I have copy and pasted this from the codebook):\n\nME: Votes relating to the Palestinian conflict (19%)\nNU: Votes relating to nuclear weapons and nuclear material (13%)\nDI: Votes relating to arms control and disarmament (16%)\nCO: Votes relating to colonialism (18%)\nHR: Votes relating to human rights (17%)\nEC: Votes relating to (economic) development (9%)\n\nThese classifications cover 92% of resolutions in the dataset. I have left out the 8% which have no classification (I think some of these cover e.g.¬†votes about the UN itself).\nWhilst member states may choose to abstain for many reasons, I have chosen to interpret them alongside ‚Äúagree‚Äù with the following disjunction ‚Äúabstained or voted with X‚Äù. This is because I am interpreting abstentions as functionally equivalent to leaving a resolution unopposed - as the UK is now doing.\nThe variable ‚Äúimportant‚Äù refers to votes marked by the US state department as important in their yearly report on voting practices in the US. NOTE - IT IS NOT AVAILABLE FOR ALL YEARS. I have tried to make this clear."
  },
  {
    "objectID": "posts/uk-sycophantic-stooge-us-imperialism/index.html#uk-vs-the-us-at-the-un-general-assembly",
    "href": "posts/uk-sycophantic-stooge-us-imperialism/index.html#uk-vs-the-us-at-the-un-general-assembly",
    "title": "The sycophantic stooge of US imperialism",
    "section": "UK vs the US at the UN General Assembly",
    "text": "UK vs the US at the UN General Assembly\nIn a nutshell, I‚Äôve taken the UN General Assembly data, and counted how many times the UK has either voted against the US, OR abstained/voted with the US. I call this variable ‚Äúsycophancy‚Äù, and it should reveal the sort of patterns we‚Äôre seeing now - where the UK does not directly disagree with the US, but does not oppose it.\nTwo interesting variables in this dataset. First, there is a classification for what issues are being voted on (e.g.¬†Human Rights/Palestine) - so we can break votes down specifically to see UK votes on Palestine. Second, there is a variable called ‚Äúimportant votes‚Äù. These are votes marked by the US state department as important in their yearly report on UNGA voting practices. This classification has been available since 1983, and was created to keep an eye on countries who were receiving aid from the US. I had a source for this claim - I need to find it. For now, put in their own terms, the US State Department say:\n\nImportant issues are defined in the U.S. Department of State‚Äôs annual U.S. Congressional Report on ‚ÄúVoting Practices in the United Nations‚Äù and by Public Law 101-246 which calls for, with respect to plenary votes for the UN General Assembly, a listing of ‚Äúvotes on issues which directly affected important United States interests and on which the United States lobbied extensively.‚Äù An essential basis for identifying ‚Äúimportant‚Äù issues is their consistency with the State Department‚Äôs Strategic Goals.\nSource\n\nWe can reappropriate this classification for our own ends. Specifically, it would help track how often the UK goes along with key US interests - thus helping us track its obvious role as sycophantic stooge a bit better. We must be careful about interpreting this, because of course different parts of US government can disagree (e.g.¬†State Department vs.¬†Pentagon). But I still think this is a helpful and interesting classification to work with.\nHere is the cleaned up data:\n\n\nCode\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(readr)\n\nload(\"UNVotes.rda\")\n\n# vote ‚Äì Vote choice\n# 1 ‚Äì Yes\n# 2 ‚Äì Abstain\n# 3 ‚Äì No\n# 8 ‚Äì Absent\n# 9 ‚Äì Not a member\n\n# Work out UK vs USA at the UN\n\nuk_vs_us_votes <- completeVotes %>% \nclean_names() %>%\n  \n# select relevant vars\nselect(\n  country, \n  countryname, \n  date, \n  unres, \n  vote, \n  descr, \n  importantvote, \n  me, \n  nu, \n  di, \n  hr, \n  co, \n  ec) %>% \n  \n  # filter for relevant countries (noting name change some years and no codes)\n  \n  filter(country%in%c(\"UNITED STATES\", \"UNITED KINGDOM\") |\n         countryname%in%c(\"United States of America\", \"United Kingdom of Great Britain and Northern Ireland\") |\n         country%in%c(\"USA\", \"GBR\")) %>% \n  \n  filter(!vote==8) %>% # remove absences \n  \n  # make variable to flag identical votes \n  \n  group_by(unres, date) %>% \n  mutate(flag = n_distinct(vote) == 1) %>%\n  ungroup() %>% \n  \n  # make variable for sycophance - defined as abstaining or voting with X\n  \n  mutate(sycophancy = case_when(\n        vote == 2 ~ \"Agreed\", # 2 = abstain.\n        flag == TRUE ~ \"Agreed\",\n        flag == FALSE ~ \"Disagreed\"\n        )) %>% \n  \n  # filter only for UK\n  \n  filter(country==\"UNITED KINGDOM\" |\n         countryname==\"United Kingdom of Great Britain and Northern Ireland\" |\n         country==\"GBR\",\n         date>=\"1983-01-01\" # from 1983 as \"important\" classification available then\n         ) %>% \n  pivot_longer(8:13, names_to = \"issue_code\", values_to = \"issue_flag\") %>%\n  filter(issue_flag==1 \n         ) %>% \n  mutate(year = year(date),\n         sum=1)\n\n\n# Prep for faceted diverging bar chart\n\ndiverging_bars_uk <- uk_vs_us_votes %>% \n  select(year, issue_code, importantvote, sycophancy) %>% \n  mutate(importantvote = case_match(\n        importantvote,\n        1 ~ \"Important\",\n        0 ~ \"Not important\",\n        NA ~ \"Unavailable\"\n        ),\n        issue_code = case_match(\n        issue_code,\n        \"me\" ~ \"Palestine\",\n        \"nu\" ~ \"Nuclear weapons & material\",\n        \"di\" ~ \"Arms control & disarmament\",\n        \"co\" ~ \"Colonialism\",\n        \"hr\" ~ \"Human rights\",\n        \"ec\" ~ \"Economic development\"\n       )) %>% \n  mutate(count = 1) %>% \n  group_by(year, issue_code, importantvote, sycophancy) %>% \n  summarise(sum = sum(count)) %>% \n  mutate(sum = if_else(sycophancy==\"Disagreed\", sum*-1, sum)) \n\nglimpse(diverging_bars_uk)\n\n\nRows: 778\nColumns: 5\nGroups: year, issue_code, importantvote [498]\n$ year          <dbl> 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 19‚Ä¶\n$ issue_code    <chr> \"Arms control & disarmament\", \"Arms control & disarmamen‚Ä¶\n$ importantvote <chr> \"Important\", \"Not important\", \"Not important\", \"Not impo‚Ä¶\n$ sycophancy    <chr> \"Agreed\", \"Agreed\", \"Agreed\", \"Disagreed\", \"Agreed\", \"Ag‚Ä¶\n$ sum           <dbl> 1, 36, 13, -4, 2, 20, -3, 2, 16, -1, 1, 30, -1, 1, 20, -‚Ä¶\n\n\n\n\n\nAnd this is what it looks like if we take some time to lay it out:\n\n\n\nBetween 1983-2023*, the UK barely opposed the US on UN resolutions - except for Palestine. But, where it‚Äôs possible to check votes marked ‚Äúimportant‚Äù by the US State Department, the UK falls in line\n\n\nnewPlot = import(\"https://esm.sh/@observablehq/plot@0.6.9\");\n\nparseTimeYear = d3.utcParse(\"%Y\");\n\ndiv_bars_uk = transpose(diverging_bars_uk_convert)\n\ndiv_bars_ukEdit = div_bars_uk // have to define this otherwise it throws an error\n  .forEach((d) => {\n  d.year = parseTimeYear(d.year);\n});\n\nannotation = {\n  return [\n      newPlot.text(div_bars_uk, {\n        text: \"issue_code\",\n        fx: \"issue_code\",\n        frameAnchor: \"top\",\n        dy: -20,\n        lineWidth: 5,\n        fontSize: \"10px\",\n        fontWeight: \"bold\",\n        fill: \"black\",\n        stroke: \"white\"\n    }),\n    newPlot.text(div_bars_uk, {\n        text: [`Abstained or voted with the US`],\n        fx: [\"Economic development\"],\n        frameAnchor: \"top\",\n        dx: 17,\n        dy: 45,\n        lineWidth: 5,\n        fontSize: \"12px\",\n        fill: \"black\",\n        stroke: \"white\",\n        fontStyle: \"italic\"\n    }),\n    newPlot.text(div_bars_uk, {\n        text: [`Opposed US`],\n        fx: [\"Economic development\"],\n        frameAnchor: \"top\",\n        dy: 210,\n        dx: 15,\n        lineWidth: 5,\n        fontSize: \"12px\",\n        fill: \"black\",\n        stroke: \"white\",\n        fontStyle: \"italic\"\n    }),\n    newPlot.text(div_bars_uk, {\n        text: [`US \"important\" classification unavailable`],\n        fx: [\"Colonialism\"],\n        frameAnchor: \"top\",\n        dy: 25,\n        dx: -3,\n        lineWidth: 5,\n        fontSize: \"12px\",\n        fill: \"black\",\n        stroke: \"white\",\n        fontStyle: \"italic\"\n    }),\n    newPlot.arrow(div_bars_uk, {\n      fx: [\"Colonialism\"],\n      x1: new Date(\"2004-01-01\"),\n      y1: 21,\n      x2: new Date(\"2020-01-01\"),\n      y2: 15,\n      bend: -22.5\n    }),\n    newPlot.arrow(div_bars_uk, {\n      fx: [\"Economic development\"],\n      x1: new Date(\"1994-01-01\"),\n      y1: 9,\n      x2: new Date(\"1994-01-01\"),\n      y2: 35\n    }),\n    newPlot.arrow(div_bars_uk, {\n      fx: [\"Economic development\"],\n      x1: new Date(\"1994-01-01\"),\n      y1: -4,\n      x2: new Date(\"1994-01-01\"),\n      y2: -19\n    }),\n    newPlot.frame({fx: \"Palestine\"})\n  ];\n}\n\nxAxis = ({\n  tickRotate: 90,\n  ticks: [new Date(\"1983-01-01\"), new Date(\"2023-01-01\")],\n  label: null,\n  nice: true,\n  interval: \"year\"\n})\n\nyAxis = ({\n    grid: true,\n    nice: true,\n    tickSize: 0,\n    //ticks: 8,\n    label: \"Votes\",\n    labelArrow: \"none\",\n    tickFormat: Math.abs\n})\n\nfacetStyle = ({\nlabel: null, padding: 0.2, axis: null\n})\n\nbars = [\n    newPlot.barY( \n      div_bars_uk, \n        newPlot.stackY({\n        fx: \"issue_code\", \n        x: \"year\",\n        y: \"sum\", \n        z: \"importantvote\",\n        fill: \"importantvote\",\n        order: \"importantvote\",\n        stroke: \"importantvote\",\n        inset: 0, \n        clip: true\n      })) \n]\n\nnewPlot.plot({\n  height: 300,\n  marginTop: 25,\n  marginBottom: 40,\n  fx: facetStyle,\n  color: {legend: false, range: [\"red\", \"#707173\", \"lightgrey\"]},\n  x: xAxis,\n  y: yAxis,\n  marks: [\n      bars,\n      annotation,\n      newPlot.ruleY([0])\n  ].flat()\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*5/11/1983 - 30/6 /2023. See codebook for further caveats.  Source: UNGA; Erik Voeten - Data and Analyses of Voting in the UN General Assembly Graphic: Yusuf Imaad Khan / @yusuf_i_k\n\n\nThe patterns are quite striking - even if this is quite a dense chart. Hopefully the title and annotations aid interpretation. Do let me know if you have better ideas. I have been somewhat stubborn in insisting that all this information could go on the same chart."
  },
  {
    "objectID": "posts/uk-sycophantic-stooge-us-imperialism/index.html#us-vs-the-world-at-the-un-general-assembly",
    "href": "posts/uk-sycophantic-stooge-us-imperialism/index.html#us-vs-the-world-at-the-un-general-assembly",
    "title": "The sycophantic stooge of US imperialism",
    "section": "US vs the World at the UN General Assembly",
    "text": "US vs the World at the UN General Assembly\nHaving taken things this far, we may as well go a little further. What does this look like for the whole world vs the US at the UN General Assembly? This analysis has been done elsewhere, but maybe not broken down for important votes and the the specific six topics.\nOnce again, here is the cleaned up data - counting how many times the world has either voted against the US, OR abstained/voted with the US.\n\n\nCode\n# Repeat the process but for the whole World vs USA at UN\n\nworld_vs_us_votes <- completeVotes %>% \nclean_names() %>% \nselect(country, countryname, date, unres, vote, descr, importantvote, me, nu, di, hr, co, ec) %>% \n  filter(!vote==8,\n         !vote==9) # 9 = not a member \n\nusa_votes <- world_vs_us_votes %>%\n  filter(country==\"UNITED STATES\" |\n         countryname==\"United States of America\" |\n         country==\"USA\") %>% \n  select(unres, date, vote) %>% \n  rename(us_vote = vote)\n\nworld_vs_us_votes <- world_vs_us_votes %>%\n  filter(!country==\"UNITED STATES\" |\n         !countryname==\"United States of America\" |\n         !country==\"USA\") %>% \n  left_join(usa_votes , by = c(\"unres\", \"date\")) %>% \n  mutate(flag = us_vote == vote) %>% \n  mutate(sycophancy = case_when(\n        vote == 2 ~ \"Agreed\", # 2 = abstain.\n        flag == TRUE ~ \"Agreed\",\n        flag == FALSE ~ \"Disagreed\"\n        )) %>% \n  filter(date>=\"1983-01-01\") %>% \n  pivot_longer(8:13, names_to = \"issue_code\", values_to = \"issue_flag\") %>%\n  filter(issue_flag==1\n         ) %>% \n  mutate(year = year(date),\n         sum=1) %>% \n  distinct()\n\ndiverging_bars_world <- world_vs_us_votes %>% \n  select(year, issue_code, importantvote, sycophancy) %>% \n  mutate(importantvote = case_match(\n        importantvote,\n        1 ~ \"Important\",\n        0 ~ \"Not important\",\n        NA ~ \"Unavailable\"\n        ),\n        issue_code = case_match(\n        issue_code,\n        \"me\" ~ \"Palestine\",\n        \"nu\" ~ \"Nuclear weapons & material\",\n        \"di\" ~ \"Arms control & disarmament\",\n        \"co\" ~ \"Colonialism\",\n        \"hr\" ~ \"Human rights\",\n        \"ec\" ~ \"Economic development\"\n       )) %>% \n  mutate(count = 1) %>% \n  group_by(year, issue_code, importantvote, sycophancy) %>% \n  summarise(sum = sum(count, na.rm=T)) %>% \n  mutate(sum = if_else(sycophancy==\"Disagreed\", sum*-1, sum)) \n\nglimpse(diverging_bars_world)\n\n\nRows: 999\nColumns: 5\nGroups: year, issue_code, importantvote [498]\n$ year          <dbl> 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 19‚Ä¶\n$ issue_code    <chr> \"Arms control & disarmament\", \"Arms control & disarmamen‚Ä¶\n$ importantvote <chr> \"Important\", \"Important\", \"Not important\", \"Not importan‚Ä¶\n$ sycophancy    <chr> \"Agreed\", \"Disagreed\", \"Agreed\", \"Disagreed\", \"Agreed\", ‚Ä¶\n$ sum           <dbl> 127, -20, 1480, -3761, 270, -2233, 115, -165, 1173, -213‚Ä¶\n\n\n\n\n\nAnd here it is visualised:\n\n\n\nBetween 1983-2023*, the world strongly opposed the US on UN resolutions across a range of subjects. Where it‚Äôs possible to check votes marked ‚Äúimportant‚Äù by the US State Department, dissent narrows\n\n\ndiv_bars_world = transpose(div_bars_world_convert)\n\ndiv_bars_worldEdit = div_bars_world// have to define this otherwise it throws an error\n  .forEach((d) => {\n  d.year = parseTimeYear(d.year);\n});\n\n\nannotation2 = {\n  return [\n      newPlot.text(div_bars_world, {\n        text: \"issue_code\",\n        fx: \"issue_code\",\n        frameAnchor: \"top\",\n        dy: -20,\n        lineWidth: 5,\n        fontSize: \"10px\",\n        fontWeight: \"bold\",\n        fill: \"black\",\n        stroke: \"white\"\n    }),\n    newPlot.text(div_bars_world, {\n        text: [`Abstained or voted with the US`],\n        fx: [\"Economic development\"],\n        frameAnchor: \"top\",\n        dx: 17,\n        dy: 10,\n        lineWidth: 5,\n        fontSize: \"12px\",\n        fill: \"black\",\n        stroke: \"white\",\n        fontStyle: \"italic\"\n    }),\n    newPlot.text(div_bars_world, {\n        text: [`Opposed US`],\n        fx: [\"Economic development\"],\n        frameAnchor: \"top\",\n        dy: 185,\n        dx: 15,\n        lineWidth: 5,\n        fontSize: \"12px\",\n        fill: \"black\",\n        stroke: \"white\",\n        fontStyle: \"italic\"\n    }),\n    newPlot.text(div_bars_world, {\n        text: [`US \"important\" classification unavailable`],\n        fx: [\"Colonialism\"],\n        frameAnchor: \"top\",\n        dy: 0,\n        dx: -3,\n        lineWidth: 5,\n        fontSize: \"12px\",\n        fill: \"black\",\n        stroke: \"white\",\n        fontStyle: \"italic\"\n    }),\n    newPlot.arrow(div_bars_world, {\n      fx: [\"Colonialism\"],\n      x1: new Date(\"2004-01-01\"),\n      y1: 2050,\n      x2: new Date(\"2020-01-01\"),\n      y2: 1250,\n      bend: -22.5\n    }),\n    newPlot.arrow(div_bars_world, {\n      fx: [\"Economic development\"],\n      x1: new Date(\"1994-01-01\"),\n      y1: 500,\n      x2: new Date(\"1994-01-01\"),\n      y2: 3600\n    }),\n    newPlot.arrow(div_bars_world, {\n      fx: [\"Economic development\"],\n      x1: new Date(\"1994-01-01\"),\n      y1: -1200,\n      x2: new Date(\"1994-01-01\"),\n      y2: -4800\n    }),\n    newPlot.frame({fx: \"Palestine\"})\n  ];\n}\n\nxAxis2 = ({\n  tickRotate: 90,\n  ticks: [new Date(\"1983-01-01\"), new Date(\"2023-01-01\")],\n  label: null,\n  nice: true,\n  interval: \"year\"\n})\n\nyAxis2 = ({\n    grid: true,\n    nice: true,\n    tickSize: 0,\n    label: \"Votes\",\n    labelArrow: \"none\",\n    tickFormat: Math.abs\n})\n\nfacetStyle2 = ({\nlabel: null, padding: 0.2, axis: null\n})\n\nbars2 = [\n    newPlot.barY( \n      div_bars_world, \n        newPlot.stackY({\n        fx: \"issue_code\", \n        x: \"year\",\n        y: \"sum\", \n        z: \"importantvote\",\n        fill: \"importantvote\",\n        order: \"importantvote\",\n        stroke: \"importantvote\",\n        inset: 0, \n        clip: true\n      })) \n].flat()\n\nnewPlot.plot({\n  height: 300,\n  marginTop: 25,\n  marginBottom: 40,\n  fx: facetStyle2,\n  color: {legend: false, range: [\"red\", \"#707173\", \"lightgrey\"]},\n  x: xAxis2,\n  y: yAxis2,\n  marks: [\n      bars2,\n      annotation2,\n      newPlot.ruleY([0])\n  ].flat()\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*5/11/1983 - 30/6/2023. See codebook for further caveats.  Source: UNGA; Erik Voeten - Data and Analyses of Voting in the UN General Assembly Graphic: Yusuf Imaad Khan / @yusuf_i_k\n\n\nAs you might have expected, when compared to the UK, the pattern flips for the whole world vs the US. Far more direct disagreement across all six topics - but once again a narrowing of dissent on key US priorities. Once again, do let me know if you have feedback, any ideas/suggestions, and if you‚Äôve spotted any issues in my code (which is all open)."
  },
  {
    "objectID": "posts/uk-sycophantic-stooge-us-imperialism/index.html#conclusion",
    "href": "posts/uk-sycophantic-stooge-us-imperialism/index.html#conclusion",
    "title": "The sycophantic stooge of US imperialism",
    "section": "Conclusion",
    "text": "Conclusion\nThese charts may strike you as obvious or trivial. But I think being armed with the facts and having a clear picture of what is going on are too important to dismiss. For those who believe the situation is hopeless, and that the UK vote at the UN doesn‚Äôt matter, consider asking why the UK didn‚Äôt just support the General Assembly resolution for the ceasefire symbolically."
  },
  {
    "objectID": "posts/uk-sycophantic-stooge-us-imperialism/index.html#postscript",
    "href": "posts/uk-sycophantic-stooge-us-imperialism/index.html#postscript",
    "title": "The sycophantic stooge of US imperialism",
    "section": "Postscript",
    "text": "Postscript\nI sat on these charts for a little while because I thought they were obvious, trivial, and idiotic. On further reflection, I decided they might not be, and that I should write this up. This was for a few reasons:\n\nThis is my blog and I‚Äôll do what I like\nI was inspired by:\n\nMona Chalabi‚Äôs visualisations on UN Security Council vetos\n@comrade_sweezy‚Äôs blog and efforts\nJohn-Baptiste Oduor‚Äôs review of Someone Else‚Äôs Empire, by Tom Stevensen (which is now on my reading list)\nThe general lunacy of the UK\nThis invocation by Asem al-Nabih, a friend of Refaat Alareer, to speak up‚Ä¶"
  },
  {
    "objectID": "posts/in-our-thousands/index.html",
    "href": "posts/in-our-thousands/index.html",
    "title": "The",
    "section": "",
    "text": "Particle Text Effect"
  },
  {
    "objectID": "posts/in-our-thousands/index.html#idea",
    "href": "posts/in-our-thousands/index.html#idea",
    "title": "In our thousands, in our millions..",
    "section": "Idea",
    "text": "Idea"
  },
  {
    "objectID": "posts/in-our-thousands/index.html#attempt",
    "href": "posts/in-our-thousands/index.html#attempt",
    "title": "In our thousands, in our millions..",
    "section": "Attempt",
    "text": "Attempt"
  },
  {
    "objectID": "posts/in-our-thousands/index.html#introduction",
    "href": "posts/in-our-thousands/index.html#introduction",
    "title": "In our thousands, in our millions..",
    "section": "Introduction",
    "text": "Introduction"
  }
]